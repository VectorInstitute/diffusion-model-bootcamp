{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 29 14:13:43 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    27W /  70W |   4549MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     16804      C   ...z7DAirMd-py3.9/bin/python     4546MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "# TABSYN: Tabular Data Synthesis with Diffusion Models\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two challenges regarding the extention of diffusion models to tabular data are:\n",
    "1. **Diverse data types:** a single table can have different columns each containing data of different types, including numerical, categorical, text, etc.\n",
    "2. **Varied distributions:** the distribution of data under different columns in a single table varry widely from column to column.\n",
    "\n",
    "**TabSyn** addresses these challenges by introducing a latent space where tabular data of all columns are jointly represented. It then proceedes to train a diffusion model on the latent representations.\n",
    "This tactic allows TabSyn to:\n",
    "1. Train a single diffusion model for all data types in the dataset (i.e. Generality).\n",
    "2. Optimize the distribution of latent embeddings to facilitate training of the subsequent diffusion model, thus generating higher quality synthetic data (i.e. Quality).\n",
    "3. Require much fewer reverse steps during training of the diffusion model, and synthesize data faster (i.e. Speed).\n",
    "\n",
    "In this notebook, we review and implement the TabSyn model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we import all necessary libraries and modules required for setting up the environment.\n",
    "Most of the libraries we need to implement TabSyn are the same as TabDDPM.\n",
    "We also specify `NAME_URL_DICT_UCI`, `DATA_NAME`, `DATA_DIR` and other paths as in TabDDPM's implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import src\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scripts.download_dataset import download_from_uci\n",
    "from scripts.process_dataset import process_data\n",
    "\n",
    "from src.data import preprocess, TabularDataset\n",
    "from src.baselines.tabsyn.pipeline import TabSyn\n",
    "\n",
    "\n",
    "NAME_URL_DICT_UCI = {\n",
    "    \"adult\": \"https://archive.ics.uci.edu/static/public/2/adult.zip\",\n",
    "    \"default\": \"https://archive.ics.uci.edu/static/public/350/default+of+credit+card+clients.zip\",\n",
    "    \"magic\": \"https://archive.ics.uci.edu/static/public/159/magic+gamma+telescope.zip\",\n",
    "    \"shoppers\": \"https://archive.ics.uci.edu/static/public/468/online+shoppers+purchasing+intention+dataset.zip\",\n",
    "    \"beijing\": \"https://archive.ics.uci.edu/static/public/381/beijing+pm2+5+data.zip\",\n",
    "    \"news\": \"https://archive.ics.uci.edu/static/public/332/online+news+popularity.zip\",\n",
    "}\n",
    "\n",
    "DATA_DIR = \"/projects/aieng/diffusion_bootcamp/data/tabular_copy\"\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "SYNTH_DATA_DIR = os.path.join(DATA_DIR, \"synthetic_data\")\n",
    "DATA_NAME = \"default\"\n",
    "\n",
    "MODEL_PATH = \"/projects/aieng/diffusion_bootcamp/models/tabular/tabsyn_copy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Dataset\n",
    "\n",
    "In this section, we will download the **Default of Credit Card Clients** dataset from the UCI repository and load it into a pandas DataFrame. This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. We will use this dataset to demonstrate the TabSyn method.\n",
    "For more explanation of different steps in this section, please refer to TabDDPM's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing dataset default from UCI.\n",
      "Aready downloaded.\n",
      "default (27000, 24) (3000, 24) (30000, 24)\n",
      "Numerical (27000, 14)\n",
      "Categorical (27000, 9)\n",
      "Processing and Saving default Successfully!\n",
      "default\n",
      "Total 30000\n",
      "Train 27000\n",
      "Test 3000\n",
      "Num 14\n",
      "Cat 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>24.0</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>18457.0</td>\n",
       "      <td>21381.0</td>\n",
       "      <td>18914.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>39.0</td>\n",
       "      <td>payment delay for four months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>...</td>\n",
       "      <td>125357.0</td>\n",
       "      <td>121853.0</td>\n",
       "      <td>124731.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6216.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4552.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>single</td>\n",
       "      <td>23.0</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>...</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>12525.0</td>\n",
       "      <td>12219.0</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>14019.0</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>12525.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>married</td>\n",
       "      <td>35.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>married</td>\n",
       "      <td>39.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12140.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL     SEX        EDUCATION MARRIAGE   AGE  \\\n",
       "0    20000.0  female       university  married  24.0   \n",
       "1   200000.0  female       university  married  39.0   \n",
       "2   230000.0  female       university   single  23.0   \n",
       "3    50000.0  female  graduate school  married  35.0   \n",
       "4   160000.0    male  graduate school  married  39.0   \n",
       "\n",
       "                            PAY_0                           PAY_2  \\\n",
       "0  payment delay for three months  payment delay for three months   \n",
       "1   payment delay for four months  payment delay for three months   \n",
       "2                        pay duly                        pay duly   \n",
       "3     payment delay for one month     payment delay for one month   \n",
       "4                         unknown                         unknown   \n",
       "\n",
       "                            PAY_3                           PAY_4  \\\n",
       "0     payment delay for one month     payment delay for one month   \n",
       "1  payment delay for three months  payment delay for three months   \n",
       "2                        pay duly                        pay duly   \n",
       "3     payment delay for one month                         unknown   \n",
       "4                         unknown                         unknown   \n",
       "\n",
       "                            PAY_5  ... BILL_AMT4  BILL_AMT5  BILL_AMT6  \\\n",
       "0     payment delay for one month  ...   18457.0    21381.0    18914.0   \n",
       "1  payment delay for three months  ...  125357.0   121853.0   124731.0   \n",
       "2                        pay duly  ...    1045.0    12525.0    12219.0   \n",
       "3                         unknown  ...       0.0        0.0        0.0   \n",
       "4                         unknown  ...       0.0     2920.0        0.0   \n",
       "\n",
       "   PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n",
       "0       0.0    1500.0    1600.0    1646.0     678.0    1000.0   \n",
       "1       0.0    6216.0   10000.0       0.0    5000.0    4552.0   \n",
       "2    1444.0   14019.0    1045.0   12525.0     244.0     725.0   \n",
       "3    2400.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4      35.0       0.0       0.0    2920.0       0.0   12140.0   \n",
       "\n",
       "   default payment next month  \n",
       "0                           1  \n",
       "1                           1  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download data\n",
    "download_from_uci(DATA_NAME, RAW_DATA_DIR, NAME_URL_DICT_UCI)\n",
    "\n",
    "# process data\n",
    "INFO_DIR = \"data_info\"\n",
    "process_data(DATA_NAME, INFO_DIR, DATA_DIR)\n",
    "\n",
    "# review data\n",
    "df = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"train.csv\"))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ? does not exist in the DataFrame.\n",
      "{'cat_col_idx': [1, 2, 3, 5, 6, 7, 8, 9, 10],\n",
      " 'column_info': {'0': {},\n",
      "                 '1': {},\n",
      "                 '10': {},\n",
      "                 '11': {},\n",
      "                 '12': {},\n",
      "                 '13': {},\n",
      "                 '14': {},\n",
      "                 '15': {},\n",
      "                 '16': {},\n",
      "                 '17': {},\n",
      "                 '18': {},\n",
      "                 '19': {},\n",
      "                 '2': {},\n",
      "                 '20': {},\n",
      "                 '21': {},\n",
      "                 '22': {},\n",
      "                 '23': {},\n",
      "                 '3': {},\n",
      "                 '4': {},\n",
      "                 '5': {},\n",
      "                 '6': {},\n",
      "                 '7': {},\n",
      "                 '8': {},\n",
      "                 '9': {},\n",
      "                 'categorizes': [0, 1],\n",
      "                 'max': 528666.0,\n",
      "                 'min': 0.0,\n",
      "                 'type': 'categorical'},\n",
      " 'column_names': ['LIMIT_BAL',\n",
      "                  'SEX',\n",
      "                  'EDUCATION',\n",
      "                  'MARRIAGE',\n",
      "                  'AGE',\n",
      "                  'PAY_0',\n",
      "                  'PAY_2',\n",
      "                  'PAY_3',\n",
      "                  'PAY_4',\n",
      "                  'PAY_5',\n",
      "                  'PAY_6',\n",
      "                  'BILL_AMT1',\n",
      "                  'BILL_AMT2',\n",
      "                  'BILL_AMT3',\n",
      "                  'BILL_AMT4',\n",
      "                  'BILL_AMT5',\n",
      "                  'BILL_AMT6',\n",
      "                  'PAY_AMT1',\n",
      "                  'PAY_AMT2',\n",
      "                  'PAY_AMT3',\n",
      "                  'PAY_AMT4',\n",
      "                  'PAY_AMT5',\n",
      "                  'PAY_AMT6',\n",
      "                  'default payment next month'],\n",
      " 'data_path': '/projects/aieng/diffusion_bootcamp/data/tabular/raw_data/default/default '\n",
      "              'of credit card clients.xls',\n",
      " 'file_type': 'xls',\n",
      " 'header': 'infer',\n",
      " 'idx_mapping': {'0': 0,\n",
      "                 '1': 14,\n",
      "                 '10': 22,\n",
      "                 '11': 2,\n",
      "                 '12': 3,\n",
      "                 '13': 4,\n",
      "                 '14': 5,\n",
      "                 '15': 6,\n",
      "                 '16': 7,\n",
      "                 '17': 8,\n",
      "                 '18': 9,\n",
      "                 '19': 10,\n",
      "                 '2': 15,\n",
      "                 '20': 11,\n",
      "                 '21': 12,\n",
      "                 '22': 13,\n",
      "                 '23': 23,\n",
      "                 '3': 16,\n",
      "                 '4': 1,\n",
      "                 '5': 17,\n",
      "                 '6': 18,\n",
      "                 '7': 19,\n",
      "                 '8': 20,\n",
      "                 '9': 21},\n",
      " 'idx_name_mapping': {'0': 'LIMIT_BAL',\n",
      "                      '1': 'SEX',\n",
      "                      '10': 'PAY_6',\n",
      "                      '11': 'BILL_AMT1',\n",
      "                      '12': 'BILL_AMT2',\n",
      "                      '13': 'BILL_AMT3',\n",
      "                      '14': 'BILL_AMT4',\n",
      "                      '15': 'BILL_AMT5',\n",
      "                      '16': 'BILL_AMT6',\n",
      "                      '17': 'PAY_AMT1',\n",
      "                      '18': 'PAY_AMT2',\n",
      "                      '19': 'PAY_AMT3',\n",
      "                      '2': 'EDUCATION',\n",
      "                      '20': 'PAY_AMT4',\n",
      "                      '21': 'PAY_AMT5',\n",
      "                      '22': 'PAY_AMT6',\n",
      "                      '23': 'default payment next month',\n",
      "                      '3': 'MARRIAGE',\n",
      "                      '4': 'AGE',\n",
      "                      '5': 'PAY_0',\n",
      "                      '6': 'PAY_2',\n",
      "                      '7': 'PAY_3',\n",
      "                      '8': 'PAY_4',\n",
      "                      '9': 'PAY_5'},\n",
      " 'inverse_idx_mapping': {'0': 0,\n",
      "                         '1': 4,\n",
      "                         '10': 19,\n",
      "                         '11': 20,\n",
      "                         '12': 21,\n",
      "                         '13': 22,\n",
      "                         '14': 1,\n",
      "                         '15': 2,\n",
      "                         '16': 3,\n",
      "                         '17': 5,\n",
      "                         '18': 6,\n",
      "                         '19': 7,\n",
      "                         '2': 11,\n",
      "                         '20': 8,\n",
      "                         '21': 9,\n",
      "                         '22': 10,\n",
      "                         '23': 23,\n",
      "                         '3': 12,\n",
      "                         '4': 13,\n",
      "                         '5': 14,\n",
      "                         '6': 15,\n",
      "                         '7': 16,\n",
      "                         '8': 17,\n",
      "                         '9': 18},\n",
      " 'metadata': {'columns': {'0': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '1': {'sdtype': 'categorical'},\n",
      "                          '10': {'sdtype': 'categorical'},\n",
      "                          '11': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '12': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '13': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '14': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '15': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '16': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '17': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '18': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '19': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '2': {'sdtype': 'categorical'},\n",
      "                          '20': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '21': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '22': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '23': {'sdtype': 'categorical'},\n",
      "                          '3': {'sdtype': 'categorical'},\n",
      "                          '4': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '5': {'sdtype': 'categorical'},\n",
      "                          '6': {'sdtype': 'categorical'},\n",
      "                          '7': {'sdtype': 'categorical'},\n",
      "                          '8': {'sdtype': 'categorical'},\n",
      "                          '9': {'sdtype': 'categorical'}}},\n",
      " 'name': 'default',\n",
      " 'num_col_idx': [0, 4, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
      " 'target_col_idx': [23],\n",
      " 'task_type': 'binclass',\n",
      " 'test_num': 3000,\n",
      " 'test_path': None,\n",
      " 'train_num': 27000}\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "value = \" ?\"\n",
    "if value in df.values:\n",
    "    print(f\"{value} exists in the DataFrame.\")\n",
    "else:\n",
    "    print(f\"{value} does not exist in the DataFrame.\")\n",
    "\n",
    "# review json file and its contents\n",
    "with open(f\"{PROCESSED_DATA_DIR}/{DATA_NAME}/info.json\", \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "pprint(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabSyn Algorithem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will describe the design of TabSyn as well as its main hyperparameters loaded through config, which affect the model’s effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TabSyn** consists of two parts:\n",
    "1. A *variational auto-encoder (VAE)* which learns a joint representation space for the given tabular data.\n",
    "2. A *Diffusion model* which learns the distribution of data in the joint representation space.\n",
    "\n",
    "The figure below shows a diagram of the TabSyn model.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn.jpg\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VAE**\n",
    "\n",
    "The left-side of the figure shows the VAE which operates in the original data space. The VAE itself consists of two parts: an encoder and a decoder. It also contains the corresponding tokenizer and detokenizer.\n",
    "Each row of the input tabular data ($\\pmb{x}$) is tokenized, then embedded by a transformer. Another transformer decodes the embeddings and a detokenizer reconstructs the table ($\\pmb{\\tilde{x}}$). The VAE is trained by minimizing the reconstruction loss between $\\pmb{x}$ and $\\pmb{\\tilde{x}}$.\n",
    "\n",
    "After the VAE is fully trained, the whole data ($\\pmb{x}$) is tokenized and embedded. The embedding of each row is flattened to form a 1-dimensional vector $\\pmb{z}$.\n",
    "These 1-dimensional embeddings for all rows are stored on disk, and will later be used to train the diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diffusion**\n",
    "\n",
    "The right-side of the figure shows the diffusion model which operates in the latent representation space; in other words, it only *sees* the embeddings obtained by the VAE, not the original tabular data.\n",
    "The diffusion model can be similarly divided into two parts: a forward process, and a reverse process.\n",
    "\n",
    "The forward process receives the embedded data points. A single data point is denoted by $\\pmb{z_0}$ in the figure. Gaussian noise is incrementally added to the embeddings in numerous incremental steps during the forward process. The number of the steps is denoted by $T$ in the figure. $T$ should be high enough that the distribution of embeddings at step $t=T$ is essentially a standard Gaussian distribution; in other words, the signal-to-noise ratio is practically zero.\n",
    "\n",
    "The reverse process, on the other hand, learns to *predict* an earlier-step embedding (e.g. $\\pmb{z_{t-\\Delta t}}$) from a later-step embedding (e.g. $\\pmb{z_t}$) via a neural network.\n",
    "\n",
    "After the diffusion model is fully trained, the reverse process can estimate the data distribution at step $t=0$ if it receives a standard Gaussian distribution at step $t=T$. New data points can be synthesized by sampling from this estimated distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load the configuration file that contains the hyperparameters for the TabSyn model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_params': {'lambd': 0.7, 'max_beta': 0.01, 'min_beta': 1e-05},\n",
      " 'model_params': {'d_token': 4, 'factor': 32, 'n_head': 1, 'num_layers': 2},\n",
      " 'task_type': 'binclass',\n",
      " 'train': {'diffusion': {'batch_size': 4096,\n",
      "                         'num_dataset_workers': 4,\n",
      "                         'num_epochs': 9},\n",
      "           'optim': {'diffusion': {'factor': 0.9,\n",
      "                                   'lr': 0.001,\n",
      "                                   'patience': 20,\n",
      "                                   'weight_decay': 0},\n",
      "                     'vae': {'factor': 0.95,\n",
      "                             'lr': 0.001,\n",
      "                             'patience': 10,\n",
      "                             'weight_decay': 0}},\n",
      "           'vae': {'batch_size': 4096,\n",
      "                   'num_dataset_workers': 4,\n",
      "                   'num_epochs': 10}},\n",
      " 'transforms': {'cat_encoding': None,\n",
      "                'cat_min_frequency': None,\n",
      "                'cat_nan_policy': None,\n",
      "                'normalization': 'quantile',\n",
      "                'num_nan_policy': 'mean',\n",
      "                'y_policy': 'default'}}\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join(\"src/baselines/tabsyn/configs\", f\"{DATA_NAME}.toml\")\n",
    "raw_config = src.load_config(config_path)\n",
    "\n",
    "pprint(raw_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is a TOML file that contains the following hyperparameters:\n",
    "\n",
    "1. **model_params:** specifies the structure of the transformers (both encoder and decoder) in the VAE model, including number of transformer layers, number of self-attnetion heads and token dimension.\n",
    "\n",
    "2. **transforms:** specifies the transformations and preprocessing of the data before tokenization, such as cleaning, normalization, and encoding.\n",
    "    - For preprocessing numerical features, we use the gaussian quantile transformation and replace the NaN values with mean of each row.\n",
    "    - For categorical features, we use the one-hot encoding method. NaN values are left unchanged, but we have the option to replace them. We have the option to drop the values that appear with less than a given minimum frequency under each column. Furthermore, we have the option to add an extra encoding step for categorical features during tokenization.\n",
    "\n",
    "3. **train.vae:** specifies training parameters of the VAE, including batch size, number of epochs, and number of dataset workers.\n",
    "\n",
    "4. **train.diffusion:** specifies the same training parameters as above for the diffusion model.\n",
    "\n",
    "5. **train.optim.vae:** specifies the parameters of the *Adam* optimizer and the `ReduceLROnPlateau` learning rate scheduler used to train the VAE. Optimizer parameters include initial learning rate and weight decay. LR scheduler parameters includer `factor` and `patience`.\n",
    "\n",
    "6. **train.optim.diffusion:** specifies the same parameters as above for the diffusion model.\n",
    "\n",
    "7. **loss_params:** specifies parameters of the loss function used to train the VAE including `max_beta`, `min_beta` and `lambd`.\n",
    "\n",
    "$\\beta$ is the coefficient of the KL divergence term in the VAE loss formula,\n",
    "\n",
    "$\\mathcal{L}_{vae} = \\mathcal{L}_{mse} + \\mathcal{L}_{ce} + \\beta \\mathcal{L}_{kl}$\n",
    ".\n",
    "\n",
    "Parameters `max_beta` and `min_beta` determine the range of $\\beta$. $\\beta$ is first set to `max_beta`. If the loss stops decreasing for a certain number of epochs (e.g. $10$ epochs), then at the end of each epoch after that (e.g. epoch $11$, $12$, etc.) $\\beta$ is decreased by a factor of `lambd`,\n",
    "$\\beta_{new} = \\lambda \\beta_{curr}$,\n",
    "until it reaches `beta_min`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we pre-process the data and make a dataset object.\n",
    "\n",
    "First, we determine transformations needed for the dataset, such as normalization and cleaning, in `transforms`. Next, using `preprocess` function we load the data from disk in arrays that contain both training and test data (`X_num` and `X_cat`), as well as the number of categories for each categorical feature (`categories`) and the number of numerical features (`d_numerical`).\n",
    "\n",
    "We then separate the train and test data in different arrays and convert them to Pytorch tensors.\n",
    "We create a dataset object (`TabularDataset`) with the train data. `TabularDataset` is a simple module which returns the tokens of a single row at a time. Each row constiutes a single data sample in TabSyn. Afterwards, we create a Dataloader for the train data using the `batch_size` and `num_workers` specified in config.\n",
    "\n",
    "In contrast, we keep the test data as tensors (`X_test_num` and `X_test_cat`). If a GPU is available, we move these tensors to GPU so that they can be accessed by the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path /projects/aieng/diffusion_bootcamp/data/tabular_copy/processed_data/default\n",
      "No NaNs in numerical features, skipping\n"
     ]
    }
   ],
   "source": [
    "# preprocess data\n",
    "X_num, X_cat, categories, d_numerical = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                   transforms = raw_config[\"transforms\"],\n",
    "                                                   task_type = raw_config[\"task_type\"])\n",
    "\n",
    "# separate train and test data\n",
    "X_train_num, X_test_num = X_num\n",
    "X_train_cat, X_test_cat = X_cat\n",
    "\n",
    "# convert to float tensor\n",
    "X_train_num, X_test_num = torch.tensor(X_train_num).float(), torch.tensor(X_test_num).float()\n",
    "X_train_cat, X_test_cat =  torch.tensor(X_train_cat), torch.tensor(X_test_cat)\n",
    "\n",
    "# create dataset module\n",
    "train_data = TabularDataset(X_train_num.float(), X_train_cat)\n",
    "\n",
    "# move test data to gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test_num = X_test_num.float().to(device)\n",
    "X_test_cat = X_test_cat.to(device)\n",
    "\n",
    "# create train dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = raw_config[\"train\"][\"vae\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"vae\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset num shape:  torch.Size([27000, 14])\n",
      "test dataset num shape:  torch.Size([3000, 14])\n",
      "train dataset cat shape:  torch.Size([27000, 10])\n",
      "test dataset cat shape:  torch.Size([3000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"train dataset num shape: \", train_data.X_num.shape)\n",
    "print(\"test dataset num shape: \", X_test_num.shape)\n",
    "\n",
    "print(\"train dataset cat shape: \", train_data.X_cat.shape)\n",
    "print(\"test dataset cat shape: \", X_test_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the model using the `TabSyn` class. `TabSyn` class takes the following arguments:\n",
    "\n",
    "1. `train_loader`: dataloader for train data.\n",
    "2. `X_test_num`: numerical features of the test data.\n",
    "3. `X_test_cat`: categorical features of the train data.\n",
    "4. `num_numerical_features`: number of numerical features in the dataset.\n",
    "5. `num_classes`: number of classes (i.e. categories) of each categorical feature in the dataset.\n",
    "6. `device`: the device on which the model and data exist, either \"cpu\" or \"cuda\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabsyn = TabSyn(train_loader,\n",
    "                X_test_num, X_test_cat,\n",
    "                num_numerical_features = d_numerical,\n",
    "                num_classes = categories,\n",
    "                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TabSyn` class has the tools to instantiate VAE and diffusion models, train both, and sample from the trained diffusion model.\n",
    "We will demonstrate how to use these tools in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE and the diffusion model are trained independently. The following subsections explain each training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A. Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate the VAE using the `instantiate_vae` method. This method takes the VAE model hyperparameters, optimizer and lr scheduler parameters from config, and instantiates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "Successfully instantiated VAE model.\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model for training\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = raw_config[\"train\"][\"optim\"][\"vae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have instantiated the VAE, we can train it using the `train_vae` function.\n",
    "This function receives the loss hyperparameters and number of epochs from the config.\n",
    "Moreover, it recieves `save_path` which is the directory where trained model checkpoints will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 3/3 [00:00<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, beta = 0.010000, Train MSE: 13.663756, Train CE:2.154334, Train KL:0.945002, Val MSE:12.495003, Val CE:2.104598, Train ACC:0.280852, Val ACC:0.286902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 3/3 [00:00<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, beta = 0.010000, Train MSE: 12.019743, Train CE:2.083766, Train KL:1.020754, Val MSE:11.045834, Val CE:2.061779, Train ACC:0.296256, Val ACC:0.301399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3/3 [00:00<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, beta = 0.010000, Train MSE: 10.634787, Train CE:2.044676, Train KL:1.146477, Val MSE:9.823824, Val CE:2.036949, Train ACC:0.304690, Val ACC:0.300689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 3/3 [00:00<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, beta = 0.010000, Train MSE: 9.484139, Train CE:2.034360, Train KL:1.300207, Val MSE:8.833112, Val CE:2.014092, Train ACC:0.311833, Val ACC:0.318329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 3/3 [00:00<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, beta = 0.010000, Train MSE: 8.459760, Train CE:2.017638, Train KL:1.464170, Val MSE:7.862492, Val CE:1.994941, Train ACC:0.337866, Val ACC:0.348642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 3/3 [00:00<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, beta = 0.010000, Train MSE: 7.611962, Train CE:1.999599, Train KL:1.626225, Val MSE:7.053391, Val CE:1.986582, Train ACC:0.383262, Val ACC:0.402778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 3/3 [00:00<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, beta = 0.010000, Train MSE: 6.869578, Train CE:1.979303, Train KL:1.785650, Val MSE:6.403675, Val CE:1.964470, Train ACC:0.437565, Val ACC:0.456103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, beta = 0.010000, Train MSE: 6.204691, Train CE:1.968276, Train KL:1.947868, Val MSE:5.838765, Val CE:1.956694, Train ACC:0.476162, Val ACC:0.486922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 3/3 [00:00<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, beta = 0.010000, Train MSE: 5.646989, Train CE:1.966729, Train KL:2.115103, Val MSE:5.310214, Val CE:1.970527, Train ACC:0.499656, Val ACC:0.497972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, beta = 0.010000, Train MSE: 5.151427, Train CE:1.979931, Train KL:2.282740, Val MSE:4.881950, Val CE:1.979485, Train ACC:0.497676, Val ACC:0.498581\n",
      "Training time: 0.1025 mins\n",
      "Successfully trained and saved the VAE model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tabsyn.train_vae(**raw_config[\"loss_params\"],\n",
    "                 num_epochs = raw_config[\"train\"][\"vae\"][\"num_epochs\"],\n",
    "                 save_path = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the VAE, we embed the training data with the trained encoder and store the embeddings in a direcotry specified by `vae_ckpt_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved pretrained embeddings on disk!\n"
     ]
    }
   ],
   "source": [
    "# embed all inputs in the latent space\n",
    "tabsyn.save_vae_embeddings(X_train_num, X_train_cat,\n",
    "                           vae_ckpt_dir = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Train Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have stored the training data embeddings, we need to load and prepare them for the diffusion model.\n",
    "We load the embeddings using `load_vae_embeddings`. We normalize the embeddings by subtracting the mean and dividing by the standard deviation. Then, we create a Dataloader with the specified `batch_size` and `num_workers` from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latent space embeddings\n",
    "train_z, _ = tabsyn.load_latent_embeddings(os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))  # train_z dim: B x in_dim\n",
    "\n",
    "# normalize embeddings\n",
    "mean, std = train_z.mean(0), train_z.std(0)\n",
    "train_z = (train_z - mean) / std\n",
    "latent_train_data = train_z\n",
    "\n",
    "# create data loader\n",
    "latent_train_loader = DataLoader(\n",
    "    latent_train_data,\n",
    "    batch_size = raw_config[\"train\"][\"diffusion\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"diffusion\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is ready, we instantiate the diffusion model with `instantiate_diffusion`. The input dimension and hidden dimention of the diffusion model is determined by the dimension of the embeddings. \n",
    "Moreover, we instantiate the optimizer and lr scheduler using hyperparameters from config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPDiffusion(\n",
      "  (proj): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=1024, out_features=96, bias=True)\n",
      "  )\n",
      "  (map_noise): PositionalEmbedding()\n",
      "  (time_embed): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "The number of parameters: 10690656\n",
      "Successfully instantiated diffusion model.\n"
     ]
    }
   ],
   "source": [
    "# instantiate diffusion model for training\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = raw_config[\"train\"][\"optim\"][\"diffusion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the diffusion model with `train_diffusion` function.\n",
    "This function takes the following arguements:\n",
    "1. `latent_train_loader`: dataloader for the latent representations which are used to train the diffusion model.\n",
    "2. `num_epochs`: number of training epochs.\n",
    "3. `ckpt_path`: directory where the model checkpoints will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/9: 100%|██████████| 3/3 [00:00<00:00,  4.81it/s, Loss=2.03]\n",
      "Epoch 2/9: 100%|██████████| 3/3 [00:00<00:00,  5.66it/s, Loss=1.73]\n",
      "Epoch 3/9: 100%|██████████| 3/3 [00:00<00:00,  5.37it/s, Loss=1.57]\n",
      "Epoch 4/9: 100%|██████████| 3/3 [00:00<00:00,  5.68it/s, Loss=1.46]\n",
      "Epoch 5/9: 100%|██████████| 3/3 [00:00<00:00,  5.47it/s, Loss=1.36]\n",
      "Epoch 6/9: 100%|██████████| 3/3 [00:00<00:00,  5.58it/s, Loss=1.26]\n",
      "Epoch 7/9: 100%|██████████| 3/3 [00:00<00:00,  5.65it/s, Loss=1.2] \n",
      "Epoch 8/9: 100%|██████████| 3/3 [00:00<00:00,  5.62it/s, Loss=1.19]\n",
      "Epoch 9/9: 100%|██████████| 3/3 [00:00<00:00,  5.53it/s, Loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  5.782466650009155\n"
     ]
    }
   ],
   "source": [
    "# train diffusion model\n",
    "tabsyn.train_diffusion(latent_train_loader,\n",
    "                       num_epochs = raw_config[\"train\"][\"diffusion\"][\"num_epochs\"],\n",
    "                       ckpt_path = os.path.join(MODEL_PATH, DATA_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training model from scratch, we can also load weights of a pre-trained model from a given checkpoint with `load_model_state` function.\n",
    "If we haven't instantiated the VAE and diffusion model beforehand, we need to instantiate them first using `instantiate_vae` and `instantiate_diffusion` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import src.baselines.tabsyn.pipeline as pipeline\n",
    "# importlib.reload(pipeline)\n",
    "# TabSyn = getattr(pipeline, \"TabSyn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "Successfully instantiated VAE model.\n",
      "MLPDiffusion(\n",
      "  (proj): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=1024, out_features=96, bias=True)\n",
      "  )\n",
      "  (map_noise): PositionalEmbedding()\n",
      "  (time_embed): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "The number of parameters: 10690656\n",
      "Successfully instantiated diffusion model.\n",
      "Loaded model state from /projects/aieng/diffusion_bootcamp/models/tabular/tabsyn_copy/default\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = None)\n",
    "\n",
    "# load latent embeddings of input data\n",
    "train_z, token_dim = tabsyn.load_latent_embeddings(os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))\n",
    "\n",
    "# instantiate diffusion model\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = None)\n",
    "\n",
    "# load state from checkpoint\n",
    "tabsyn.load_model_state(ckpt_dir = os.path.join(MODEL_PATH, DATA_NAME),\n",
    "                        dif_ckpt_name = \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained the model effectively, using `sample` function we can generate synthetic data starting from compelete noise. The input of this function is as follows:\n",
    "\n",
    "1. `train_z`: latent embeddings of the training data.\n",
    "2. `info`: info about the data from the json file we reviewed at the beginning of this notebook.\n",
    "3. `num_inverse`: detokenizer for numerical features.\n",
    "4. `cat_inverse`: detokenizer for categorical features.\n",
    "5. `save_path`: file-path where the synthetic table will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path /projects/aieng/diffusion_bootcamp/data/tabular_copy/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(27000, 10)\n",
      "Time: 14.919931173324585\n",
      "Saving sampled data to /projects/aieng/diffusion_bootcamp/data/tabular_copy/synthetic_data/default/tabsyn.csv\n"
     ]
    }
   ],
   "source": [
    "# load data info file\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"info.json\"), \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "data_info[\"token_dim\"] = token_dim\n",
    "\n",
    "# get inverse tokenizers\n",
    "_, _, categories, d_numerical, num_inverse, cat_inverse = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                                     transforms = raw_config[\"transforms\"],\n",
    "                                                                     task_type = raw_config[\"task_type\"],\n",
    "                                                                     inverse = True)\n",
    "\n",
    "# sample data\n",
    "num_samples = train_z.shape[0]\n",
    "in_dim = train_z.shape[1] \n",
    "mean_input_emb = train_z.mean(0)\n",
    "tabsyn.sample(num_samples,\n",
    "              in_dim,\n",
    "              mean_input_emb,\n",
    "              info = data_info,\n",
    "              num_inverse = num_inverse,\n",
    "              cat_inverse = cat_inverse,\n",
    "              save_path = os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here, we review the synthesized data. In the following `evaluate_synthetic_data.ipynb` notebook, we will evaluate this synthesized data with respect to various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>280000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>married</td>\n",
       "      <td>51.0</td>\n",
       "      <td>payment delay for four months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>...</td>\n",
       "      <td>27263.635</td>\n",
       "      <td>7197.2793</td>\n",
       "      <td>241922.400000</td>\n",
       "      <td>390.0000</td>\n",
       "      <td>824.0794</td>\n",
       "      <td>8439.32200</td>\n",
       "      <td>1600.1881</td>\n",
       "      <td>4497.3160</td>\n",
       "      <td>26750.3790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>unknown_2</td>\n",
       "      <td>single</td>\n",
       "      <td>35.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for six months</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>...</td>\n",
       "      <td>104424.300</td>\n",
       "      <td>23847.3280</td>\n",
       "      <td>111397.680000</td>\n",
       "      <td>2414.6257</td>\n",
       "      <td>12931.7950</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11677.5020</td>\n",
       "      <td>2500.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>single</td>\n",
       "      <td>38.0</td>\n",
       "      <td>payment delay for six months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>167001.2800</td>\n",
       "      <td>-6.496786</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4736.5596</td>\n",
       "      <td>948.36914</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25601.6660</td>\n",
       "      <td>782.2601</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>280000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>unknown_2</td>\n",
       "      <td>others</td>\n",
       "      <td>42.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for two months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for six months</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>...</td>\n",
       "      <td>42026.207</td>\n",
       "      <td>19727.7730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2000.0000</td>\n",
       "      <td>2414.6865</td>\n",
       "      <td>714.34033</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>4950.8240</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>unknown_2</td>\n",
       "      <td>married</td>\n",
       "      <td>39.0</td>\n",
       "      <td>payment delay for five months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>...</td>\n",
       "      <td>188255.390</td>\n",
       "      <td>30975.2300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2000.0000</td>\n",
       "      <td>146717.4000</td>\n",
       "      <td>2040.16080</td>\n",
       "      <td>4400.6290</td>\n",
       "      <td>3643.9773</td>\n",
       "      <td>12991.9550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL     SEX        EDUCATION MARRIAGE   AGE  \\\n",
       "0   280000.0    male  graduate school  married  51.0   \n",
       "1    50000.0  female        unknown_2   single  35.0   \n",
       "2   500000.0  female  graduate school   single  38.0   \n",
       "3   280000.0    male        unknown_2   others  42.0   \n",
       "4   230000.0  female        unknown_2  married  39.0   \n",
       "\n",
       "                           PAY_0                           PAY_2  \\\n",
       "0  payment delay for four months  payment delay for three months   \n",
       "1    payment delay for one month  payment delay for three months   \n",
       "2   payment delay for six months  payment delay for three months   \n",
       "3    payment delay for one month    payment delay for two months   \n",
       "4  payment delay for five months  payment delay for three months   \n",
       "\n",
       "                            PAY_3                         PAY_4  \\\n",
       "0     payment delay for one month   payment delay for one month   \n",
       "1     payment delay for one month  payment delay for six months   \n",
       "2  payment delay for three months                      pay duly   \n",
       "3  payment delay for three months  payment delay for six months   \n",
       "4     payment delay for one month   payment delay for one month   \n",
       "\n",
       "                            PAY_5  ...   BILL_AMT4    BILL_AMT5  \\\n",
       "0  payment delay for three months  ...   27263.635    7197.2793   \n",
       "1                        pay duly  ...  104424.300   23847.3280   \n",
       "2                         unknown  ...       0.000  167001.2800   \n",
       "3                        pay duly  ...   42026.207   19727.7730   \n",
       "4                        pay duly  ...  188255.390   30975.2300   \n",
       "\n",
       "       BILL_AMT6   PAY_AMT1     PAY_AMT2    PAY_AMT3    PAY_AMT4    PAY_AMT5  \\\n",
       "0  241922.400000   390.0000     824.0794  8439.32200   1600.1881   4497.3160   \n",
       "1  111397.680000  2414.6257   12931.7950     0.00000  11677.5020   2500.0000   \n",
       "2      -6.496786     0.0000    4736.5596   948.36914      0.0000  25601.6660   \n",
       "3       0.000000  2000.0000    2414.6865   714.34033   1000.0000   4950.8240   \n",
       "4       0.000000  2000.0000  146717.4000  2040.16080   4400.6290   3643.9773   \n",
       "\n",
       "     PAY_AMT6  default payment next month  \n",
       "0  26750.3790                           0  \n",
       "1      0.0000                           0  \n",
       "2    782.2601                           0  \n",
       "3      0.0000                           1  \n",
       "4  12991.9550                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zhang, Hengrui, et al.** \"Mixed-type tabular data synthesis with score-based diffusion in latent space.\" *International Conference on Learning Representations (ICLR)* (2023).\n",
    "\n",
    "**GitHub Repository:** [Amazon Science - Tabsyn](https://github.com/amazon-science/tabsyn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_boot_shared",
   "language": "python",
   "name": "diffusion_boot_shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
