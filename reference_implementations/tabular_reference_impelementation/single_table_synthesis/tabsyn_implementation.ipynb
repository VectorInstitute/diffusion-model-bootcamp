{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "# TABSYN: Tabular Data Synthesis with Diffusion Models\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two challenges regarding the extention of diffusion models to tabular data are:\n",
    "1. **Diverse data types:** a single table can have different columns each containing data of different types, including numerical, categorical, text, etc.\n",
    "2. **Varied distributions:** the distribution of data under different columns in a single table varry widely from column to column.\n",
    "\n",
    "**TabSyn** addresses these challenges by introducing a latent space where tabular data of all columns are jointly represented. It then proceedes to train a diffusion model on the latent representations.\n",
    "This tactic allows TabSyn to:\n",
    "1. Train a single diffusion model for all data types in the dataset (i.e. Generality).\n",
    "2. Optimize the distribution of latent embeddings to facilitate training of the subsequent diffusion model, thus generating higher quality synthetic data (i.e. Quality).\n",
    "3. Require much fewer reverse steps during training of the diffusion model, and synthesize data faster (i.e. Speed).\n",
    "\n",
    "In this notebook, we review and implement the TabSyn model. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Default Dataset]()\n",
    "    \n",
    "    \n",
    "3. [TabSyn Algorithem]()\n",
    "    \n",
    "    3.1. [Load Config]()\n",
    "    \n",
    "    3.2. [Make Dataset]()\n",
    "    \n",
    "    3.3. [Instantiate Model]()\n",
    "    \n",
    "    3.4. [Train Model]()\n",
    "        \n",
    "    3.5. [Load Pretrained Model]()\n",
    "    \n",
    "    3.6. [Sample Data]()\n",
    "    \n",
    "    3.7. [Review Synthetic Data]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we import all necessary libraries and modules required for setting up the environment.\n",
    "Most of the libraries we need to implement TabSyn are the same as TabDDPM.\n",
    "We also specify `NAME_URL_DICT_UCI`, `DATA_NAME`, `DATA_DIR` and other paths as in TabDDPM's implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import src\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scripts.download_dataset import download_from_uci\n",
    "from scripts.process_dataset import process_data\n",
    "\n",
    "from src.data import preprocess, TabularDataset\n",
    "from src.baselines.tabsyn.pipeline import TabSyn\n",
    "\n",
    "\n",
    "NAME_URL_DICT_UCI = {\n",
    "    \"adult\": \"https://archive.ics.uci.edu/static/public/2/adult.zip\",\n",
    "    \"default\": \"https://archive.ics.uci.edu/static/public/350/default+of+credit+card+clients.zip\",\n",
    "    \"magic\": \"https://archive.ics.uci.edu/static/public/159/magic+gamma+telescope.zip\",\n",
    "    \"shoppers\": \"https://archive.ics.uci.edu/static/public/468/online+shoppers+purchasing+intention+dataset.zip\",\n",
    "    \"beijing\": \"https://archive.ics.uci.edu/static/public/381/beijing+pm2+5+data.zip\",\n",
    "    \"news\": \"https://archive.ics.uci.edu/static/public/332/online+news+popularity.zip\",\n",
    "}\n",
    "\n",
    "# For shared directory you can change it to \"/projects/aieng/diffusion_bootcamp/data/tabular\"\n",
    "DATA_DIR = \"data/\"\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "SYNTH_DATA_DIR = os.path.join(DATA_DIR, \"synthetic_data\")\n",
    "DATA_NAME = \"default\"\n",
    "\n",
    "MODEL_PATH = \"models/tabsyn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Dataset\n",
    "\n",
    "In this section, we will download the **Default of Credit Card Clients** dataset from the UCI repository and load it into a pandas DataFrame. This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. We will use this dataset to demonstrate the TabSyn method.\n",
    "For more explanation of different steps in this section, please refer to TabDDPM's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing dataset default from UCI.\n",
      "Aready downloaded.\n",
      "default (27000, 24) (3000, 24) (30000, 24)\n",
      "Numerical (27000, 14)\n",
      "Categorical (27000, 9)\n",
      "Processing and Saving default Successfully!\n",
      "default\n",
      "Total 30000\n",
      "Train 27000\n",
      "Test 3000\n",
      "Num 14\n",
      "Cat 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>24.0</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>18457.0</td>\n",
       "      <td>21381.0</td>\n",
       "      <td>18914.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>39.0</td>\n",
       "      <td>payment delay for four months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>...</td>\n",
       "      <td>125357.0</td>\n",
       "      <td>121853.0</td>\n",
       "      <td>124731.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6216.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4552.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>single</td>\n",
       "      <td>23.0</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>...</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>12525.0</td>\n",
       "      <td>12219.0</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>14019.0</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>12525.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>married</td>\n",
       "      <td>35.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>married</td>\n",
       "      <td>39.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12140.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>high school</td>\n",
       "      <td>others</td>\n",
       "      <td>59.0</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>18055.0</td>\n",
       "      <td>18755.0</td>\n",
       "      <td>20299.0</td>\n",
       "      <td>1596.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>university</td>\n",
       "      <td>single</td>\n",
       "      <td>42.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>17029.0</td>\n",
       "      <td>10575.0</td>\n",
       "      <td>9478.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>single</td>\n",
       "      <td>26.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>-884.0</td>\n",
       "      <td>-6332.0</td>\n",
       "      <td>-9333.0</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>6730.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>5448.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>40.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>196467.0</td>\n",
       "      <td>201425.0</td>\n",
       "      <td>-8276.0</td>\n",
       "      <td>7028.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>10013.0</td>\n",
       "      <td>9505.0</td>\n",
       "      <td>175074.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>27.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for four months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>...</td>\n",
       "      <td>18472.0</td>\n",
       "      <td>18037.0</td>\n",
       "      <td>18487.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL     SEX        EDUCATION MARRIAGE   AGE  \\\n",
       "0    20000.0  female       university  married  24.0   \n",
       "1   200000.0  female       university  married  39.0   \n",
       "2   230000.0  female       university   single  23.0   \n",
       "3    50000.0  female  graduate school  married  35.0   \n",
       "4   160000.0    male  graduate school  married  39.0   \n",
       "5    20000.0    male      high school   others  59.0   \n",
       "6    50000.0    male       university   single  42.0   \n",
       "7   130000.0  female  graduate school   single  26.0   \n",
       "8   300000.0  female       university  married  40.0   \n",
       "9    20000.0    male       university  married  27.0   \n",
       "\n",
       "                            PAY_0                           PAY_2  \\\n",
       "0  payment delay for three months  payment delay for three months   \n",
       "1   payment delay for four months  payment delay for three months   \n",
       "2                        pay duly                        pay duly   \n",
       "3     payment delay for one month     payment delay for one month   \n",
       "4                         unknown                         unknown   \n",
       "5  payment delay for three months     payment delay for one month   \n",
       "6     payment delay for one month     payment delay for one month   \n",
       "7     payment delay for one month                        pay duly   \n",
       "8     payment delay for one month     payment delay for one month   \n",
       "9     payment delay for one month     payment delay for one month   \n",
       "\n",
       "                            PAY_3                           PAY_4  \\\n",
       "0     payment delay for one month     payment delay for one month   \n",
       "1  payment delay for three months  payment delay for three months   \n",
       "2                        pay duly                        pay duly   \n",
       "3     payment delay for one month                         unknown   \n",
       "4                         unknown                         unknown   \n",
       "5     payment delay for one month     payment delay for one month   \n",
       "6     payment delay for one month     payment delay for one month   \n",
       "7                        pay duly                        pay duly   \n",
       "8     payment delay for one month     payment delay for one month   \n",
       "9     payment delay for one month   payment delay for four months   \n",
       "\n",
       "                            PAY_5  ... BILL_AMT4  BILL_AMT5  BILL_AMT6  \\\n",
       "0     payment delay for one month  ...   18457.0    21381.0    18914.0   \n",
       "1  payment delay for three months  ...  125357.0   121853.0   124731.0   \n",
       "2                        pay duly  ...    1045.0    12525.0    12219.0   \n",
       "3                         unknown  ...       0.0        0.0        0.0   \n",
       "4                         unknown  ...       0.0     2920.0        0.0   \n",
       "5     payment delay for one month  ...   18055.0    18755.0    20299.0   \n",
       "6     payment delay for one month  ...   17029.0    10575.0     9478.0   \n",
       "7                         unknown  ...    -884.0    -6332.0    -9333.0   \n",
       "8     payment delay for one month  ...  196467.0   201425.0    -8276.0   \n",
       "9  payment delay for three months  ...   18472.0    18037.0    18487.0   \n",
       "\n",
       "   PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n",
       "0       0.0    1500.0    1600.0    1646.0     678.0    1000.0   \n",
       "1       0.0    6216.0   10000.0       0.0    5000.0    4552.0   \n",
       "2    1444.0   14019.0    1045.0   12525.0     244.0     725.0   \n",
       "3    2400.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4      35.0       0.0       0.0    2920.0       0.0   12140.0   \n",
       "5    1596.0    1600.0    1300.0    1000.0    2000.0       0.0   \n",
       "6    2500.0    2000.0    2500.0     500.0     500.0     500.0   \n",
       "7    1298.0    6730.0     900.0    5448.0       0.0   25000.0   \n",
       "8    7028.0    6000.0   50000.0   10013.0    9505.0  175074.0   \n",
       "9    3000.0   10000.0       0.0       0.0     900.0    2000.0   \n",
       "\n",
       "   default payment next month  \n",
       "0                           1  \n",
       "1                           1  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  \n",
       "5                           1  \n",
       "6                           0  \n",
       "7                           0  \n",
       "8                           0  \n",
       "9                           0  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download data\n",
    "download_from_uci(DATA_NAME, RAW_DATA_DIR, NAME_URL_DICT_UCI)\n",
    "\n",
    "# process data\n",
    "INFO_DIR = \"data_info\"\n",
    "process_data(DATA_NAME, INFO_DIR, DATA_DIR)\n",
    "\n",
    "# review data\n",
    "df = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"train.csv\"))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat_col_idx': [1, 2, 3, 5, 6, 7, 8, 9, 10],\n",
      " 'column_info': {'0': {},\n",
      "                 '1': {},\n",
      "                 '10': {},\n",
      "                 '11': {},\n",
      "                 '12': {},\n",
      "                 '13': {},\n",
      "                 '14': {},\n",
      "                 '15': {},\n",
      "                 '16': {},\n",
      "                 '17': {},\n",
      "                 '18': {},\n",
      "                 '19': {},\n",
      "                 '2': {},\n",
      "                 '20': {},\n",
      "                 '21': {},\n",
      "                 '22': {},\n",
      "                 '23': {},\n",
      "                 '3': {},\n",
      "                 '4': {},\n",
      "                 '5': {},\n",
      "                 '6': {},\n",
      "                 '7': {},\n",
      "                 '8': {},\n",
      "                 '9': {},\n",
      "                 'categorizes': [0, 1],\n",
      "                 'max': 528666.0,\n",
      "                 'min': 0.0,\n",
      "                 'type': 'categorical'},\n",
      " 'column_names': ['LIMIT_BAL',\n",
      "                  'SEX',\n",
      "                  'EDUCATION',\n",
      "                  'MARRIAGE',\n",
      "                  'AGE',\n",
      "                  'PAY_0',\n",
      "                  'PAY_2',\n",
      "                  'PAY_3',\n",
      "                  'PAY_4',\n",
      "                  'PAY_5',\n",
      "                  'PAY_6',\n",
      "                  'BILL_AMT1',\n",
      "                  'BILL_AMT2',\n",
      "                  'BILL_AMT3',\n",
      "                  'BILL_AMT4',\n",
      "                  'BILL_AMT5',\n",
      "                  'BILL_AMT6',\n",
      "                  'PAY_AMT1',\n",
      "                  'PAY_AMT2',\n",
      "                  'PAY_AMT3',\n",
      "                  'PAY_AMT4',\n",
      "                  'PAY_AMT5',\n",
      "                  'PAY_AMT6',\n",
      "                  'default payment next month'],\n",
      " 'data_path': '/projects/aieng/diffusion_bootcamp/data/tabular/raw_data/default/default '\n",
      "              'of credit card clients.xls',\n",
      " 'file_type': 'xls',\n",
      " 'header': 'infer',\n",
      " 'idx_mapping': {'0': 0,\n",
      "                 '1': 14,\n",
      "                 '10': 22,\n",
      "                 '11': 2,\n",
      "                 '12': 3,\n",
      "                 '13': 4,\n",
      "                 '14': 5,\n",
      "                 '15': 6,\n",
      "                 '16': 7,\n",
      "                 '17': 8,\n",
      "                 '18': 9,\n",
      "                 '19': 10,\n",
      "                 '2': 15,\n",
      "                 '20': 11,\n",
      "                 '21': 12,\n",
      "                 '22': 13,\n",
      "                 '23': 23,\n",
      "                 '3': 16,\n",
      "                 '4': 1,\n",
      "                 '5': 17,\n",
      "                 '6': 18,\n",
      "                 '7': 19,\n",
      "                 '8': 20,\n",
      "                 '9': 21},\n",
      " 'idx_name_mapping': {'0': 'LIMIT_BAL',\n",
      "                      '1': 'SEX',\n",
      "                      '10': 'PAY_6',\n",
      "                      '11': 'BILL_AMT1',\n",
      "                      '12': 'BILL_AMT2',\n",
      "                      '13': 'BILL_AMT3',\n",
      "                      '14': 'BILL_AMT4',\n",
      "                      '15': 'BILL_AMT5',\n",
      "                      '16': 'BILL_AMT6',\n",
      "                      '17': 'PAY_AMT1',\n",
      "                      '18': 'PAY_AMT2',\n",
      "                      '19': 'PAY_AMT3',\n",
      "                      '2': 'EDUCATION',\n",
      "                      '20': 'PAY_AMT4',\n",
      "                      '21': 'PAY_AMT5',\n",
      "                      '22': 'PAY_AMT6',\n",
      "                      '23': 'default payment next month',\n",
      "                      '3': 'MARRIAGE',\n",
      "                      '4': 'AGE',\n",
      "                      '5': 'PAY_0',\n",
      "                      '6': 'PAY_2',\n",
      "                      '7': 'PAY_3',\n",
      "                      '8': 'PAY_4',\n",
      "                      '9': 'PAY_5'},\n",
      " 'inverse_idx_mapping': {'0': 0,\n",
      "                         '1': 4,\n",
      "                         '10': 19,\n",
      "                         '11': 20,\n",
      "                         '12': 21,\n",
      "                         '13': 22,\n",
      "                         '14': 1,\n",
      "                         '15': 2,\n",
      "                         '16': 3,\n",
      "                         '17': 5,\n",
      "                         '18': 6,\n",
      "                         '19': 7,\n",
      "                         '2': 11,\n",
      "                         '20': 8,\n",
      "                         '21': 9,\n",
      "                         '22': 10,\n",
      "                         '23': 23,\n",
      "                         '3': 12,\n",
      "                         '4': 13,\n",
      "                         '5': 14,\n",
      "                         '6': 15,\n",
      "                         '7': 16,\n",
      "                         '8': 17,\n",
      "                         '9': 18},\n",
      " 'metadata': {'columns': {'0': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '1': {'sdtype': 'categorical'},\n",
      "                          '10': {'sdtype': 'categorical'},\n",
      "                          '11': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '12': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '13': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '14': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '15': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '16': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '17': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '18': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '19': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '2': {'sdtype': 'categorical'},\n",
      "                          '20': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '21': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '22': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '23': {'sdtype': 'categorical'},\n",
      "                          '3': {'sdtype': 'categorical'},\n",
      "                          '4': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '5': {'sdtype': 'categorical'},\n",
      "                          '6': {'sdtype': 'categorical'},\n",
      "                          '7': {'sdtype': 'categorical'},\n",
      "                          '8': {'sdtype': 'categorical'},\n",
      "                          '9': {'sdtype': 'categorical'}}},\n",
      " 'name': 'default',\n",
      " 'num_col_idx': [0, 4, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
      " 'target_col_idx': [23],\n",
      " 'task_type': 'binclass',\n",
      " 'test_num': 3000,\n",
      " 'test_path': None,\n",
      " 'train_num': 27000}\n"
     ]
    }
   ],
   "source": [
    "# review json file and its contents\n",
    "with open(f\"{PROCESSED_DATA_DIR}/{DATA_NAME}/info.json\", \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "pprint(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabSyn Algorithem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will describe the design of TabSyn as well as its main hyperparameters loaded through config, which affect the model’s effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TabSyn** consists of two parts:\n",
    "1. A *variational auto-encoder (VAE)* which learns a joint representation space for the given tabular data.\n",
    "2. A *Diffusion model* which learns the distribution of data in the joint representation space.\n",
    "\n",
    "The figure below shows a diagram of the TabSyn model.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn.jpg\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VAE**\n",
    "\n",
    "The left-side of the figure shows the VAE which operates in the original data space. The VAE itself consists of two parts: an encoder and a decoder. It also contains the corresponding tokenizer and detokenizer.\n",
    "Each row of the input tabular data ($\\pmb{x}$) is tokenized, then embedded by a transformer. Another transformer decodes the embeddings and a detokenizer reconstructs the table ($\\pmb{\\tilde{x}}$). The VAE is trained by minimizing the reconstruction loss between $\\pmb{x}$ and $\\pmb{\\tilde{x}}$.\n",
    "\n",
    "After the VAE is fully trained, the whole data ($\\pmb{x}$) is tokenized and embedded. The embedding of each row is flattened to form a 1-dimensional vector $\\pmb{z}$.\n",
    "These 1-dimensional embeddings for all rows are stored on disk, and will later be used to train the diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diffusion**\n",
    "\n",
    "The right-side of the figure shows the diffusion model which operates in the latent representation space; in other words, it only *sees* the embeddings obtained by the VAE, not the original tabular data.\n",
    "The diffusion model can be similarly divided into two parts: a forward process, and a reverse process.\n",
    "\n",
    "The forward process receives the embedded data points. A single data point is denoted by $\\pmb{z_0}$ in the figure. Gaussian noise is incrementally added to the embeddings in numerous incremental steps during the forward process. The number of the steps is denoted by $T$ in the figure. $T$ should be high enough that the distribution of embeddings at step $t=T$ is essentially a standard Gaussian distribution; in other words, the signal-to-noise ratio is practically zero.\n",
    "\n",
    "The reverse process, on the other hand, learns to *predict* an earlier-step embedding (e.g. $\\pmb{z_{t-\\Delta t}}$) from a later-step embedding (e.g. $\\pmb{z_t}$) via a neural network.\n",
    "\n",
    "After the diffusion model is fully trained, the reverse process can estimate the data distribution at step $t=0$ if it receives a standard Gaussian distribution at step $t=T$. New data points can be synthesized by sampling from this estimated distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load the configuration file that contains the hyperparameters for the TabSyn model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_params': {'lambd': 0.7, 'max_beta': 0.01, 'min_beta': 1e-05},\n",
      " 'model_params': {'d_token': 4, 'factor': 32, 'n_head': 1, 'num_layers': 2},\n",
      " 'task_type': 'binclass',\n",
      " 'train': {'diffusion': {'batch_size': 4096,\n",
      "                         'num_dataset_workers': 4,\n",
      "                         'num_epochs': 9},\n",
      "           'optim': {'diffusion': {'factor': 0.9,\n",
      "                                   'lr': 0.001,\n",
      "                                   'patience': 20,\n",
      "                                   'weight_decay': 0},\n",
      "                     'vae': {'factor': 0.95,\n",
      "                             'lr': 0.001,\n",
      "                             'patience': 10,\n",
      "                             'weight_decay': 0}},\n",
      "           'vae': {'batch_size': 4096,\n",
      "                   'num_dataset_workers': 4,\n",
      "                   'num_epochs': 10}},\n",
      " 'transforms': {'cat_encoding': None,\n",
      "                'cat_min_frequency': None,\n",
      "                'cat_nan_policy': None,\n",
      "                'normalization': 'quantile',\n",
      "                'num_nan_policy': 'mean',\n",
      "                'y_policy': 'default'}}\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join(\"src/baselines/tabsyn/configs\", f\"{DATA_NAME}.toml\")\n",
    "raw_config = src.load_config(config_path)\n",
    "\n",
    "pprint(raw_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is a TOML file that contains the following hyperparameters:\n",
    "\n",
    "1. **model_params:** specifies the structure of the transformers (both encoder and decoder) in the VAE model, including number of transformer layers, number of self-attnetion heads and token dimension.\n",
    "\n",
    "2. **transforms:** specifies the transformations and preprocessing of the data before tokenization, such as cleaning, normalization, and encoding.\n",
    "    - For preprocessing numerical features, we use the gaussian quantile transformation and replace the NaN values with mean of each row.\n",
    "    - For categorical features, we use the one-hot encoding method. NaN values are left unchanged, but we have the option to replace them. We have the option to drop the values that appear with less than a given minimum frequency under each column. Furthermore, we have the option to add an extra encoding step for categorical features during tokenization.\n",
    "\n",
    "3. **train.vae:** specifies training parameters of the VAE, including batch size, number of epochs, and number of dataset workers.\n",
    "\n",
    "4. **train.diffusion:** specifies the same training parameters as above for the diffusion model.\n",
    "\n",
    "5. **train.optim.vae:** specifies the parameters of the *Adam* optimizer and the `ReduceLROnPlateau` learning rate scheduler used to train the VAE. Optimizer parameters include initial learning rate and weight decay. LR scheduler parameters includer `factor` and `patience`.\n",
    "\n",
    "6. **train.optim.diffusion:** specifies the same parameters as above for the diffusion model.\n",
    "\n",
    "7. **loss_params:** specifies parameters of the loss function used to train the VAE including `max_beta`, `min_beta` and `lambd`.\n",
    "\n",
    "$\\beta$ is the coefficient of the KL divergence term in the VAE loss formula,\n",
    "\n",
    "$\\mathcal{L}_{vae} = \\mathcal{L}_{mse} + \\mathcal{L}_{ce} + \\beta \\mathcal{L}_{kl}$\n",
    ".\n",
    "\n",
    "Parameters `max_beta` and `min_beta` determine the range of $\\beta$. $\\beta$ is first set to `max_beta`. If the loss stops decreasing for a certain number of epochs (e.g. $10$ epochs), then at the end of each epoch after that (e.g. epoch $11$, $12$, etc.) $\\beta$ is decreased by a factor of `lambd`,\n",
    "$\\beta_{new} = \\lambda \\beta_{curr}$,\n",
    "until it reaches `beta_min`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we pre-process the data and make a dataset object.\n",
    "\n",
    "First, we determine transformations needed for the dataset, such as normalization and cleaning, in `transforms`. Next, using `preprocess` function we load the data from disk in arrays that contain both training and test data (`X_num` and `X_cat`), as well as the number of categories for each categorical feature (`categories`) and the number of numerical features (`d_numerical`).\n",
    "\n",
    "We then separate the train and test data in different arrays and convert them to Pytorch tensors.\n",
    "We create a dataset object (`TabularDataset`) with the train data. `TabularDataset` is a simple module which returns the tokens of a single row at a time. Each row constiutes a single data sample in TabSyn. Afterwards, we create a Dataloader for the train data using the `batch_size` and `num_workers` specified in config.\n",
    "\n",
    "In contrast, we keep the test data as tensors (`X_test_num` and `X_test_cat`). If a GPU is available, we move these tensors to GPU so that they can be accessed by the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path data/processed_data/default\n",
      "No NaNs in numerical features, skipping\n"
     ]
    }
   ],
   "source": [
    "# preprocess data\n",
    "X_num, X_cat, categories, d_numerical = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                   transforms = raw_config[\"transforms\"],\n",
    "                                                   task_type = raw_config[\"task_type\"])\n",
    "\n",
    "# separate train and test data\n",
    "X_train_num, X_test_num = X_num\n",
    "X_train_cat, X_test_cat = X_cat\n",
    "\n",
    "# convert to float tensor\n",
    "X_train_num, X_test_num = torch.tensor(X_train_num).float(), torch.tensor(X_test_num).float()\n",
    "X_train_cat, X_test_cat =  torch.tensor(X_train_cat), torch.tensor(X_test_cat)\n",
    "\n",
    "# create dataset module\n",
    "train_data = TabularDataset(X_train_num.float(), X_train_cat)\n",
    "\n",
    "# move test data to gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test_num = X_test_num.float().to(device)\n",
    "X_test_cat = X_test_cat.to(device)\n",
    "\n",
    "# create train dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = raw_config[\"train\"][\"vae\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"vae\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset num shape:  torch.Size([27000, 14])\n",
      "test dataset num shape:  torch.Size([3000, 14])\n",
      "train dataset cat shape:  torch.Size([27000, 10])\n",
      "test dataset cat shape:  torch.Size([3000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"train dataset num shape: \", train_data.X_num.shape)\n",
    "print(\"test dataset num shape: \", X_test_num.shape)\n",
    "\n",
    "print(\"train dataset cat shape: \", train_data.X_cat.shape)\n",
    "print(\"test dataset cat shape: \", X_test_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the model using the `TabSyn` class. `TabSyn` class takes the following arguments:\n",
    "\n",
    "1. `train_loader`: dataloader for train data.\n",
    "2. `X_test_num`: numerical features of the test data.\n",
    "3. `X_test_cat`: categorical features of the train data.\n",
    "4. `num_numerical_features`: number of numerical features in the dataset.\n",
    "5. `num_classes`: number of classes (i.e. categories) of each categorical feature in the dataset.\n",
    "6. `device`: the device on which the model and data exist, either \"cpu\" or \"cuda\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabsyn = TabSyn(train_loader,\n",
    "                X_test_num, X_test_cat,\n",
    "                num_numerical_features = d_numerical,\n",
    "                num_classes = categories,\n",
    "                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TabSyn` class has the tools to instantiate VAE and diffusion models, train both, and sample from the trained diffusion model.\n",
    "We will demonstrate how to use these tools in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE and the diffusion model are trained independently. The following subsections explain each training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A. Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate the VAE using the `instantiate_vae` method. This method takes the VAE model hyperparameters, optimizer and lr scheduler parameters from config, and instantiates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "Successfully instantiated VAE model.\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model for training\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = raw_config[\"train\"][\"optim\"][\"vae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have instantiated the VAE, we can train it using the `train_vae` function.\n",
    "This function receives the loss hyperparameters and number of epochs from the config.\n",
    "Moreover, it recieves `save_path` which is the directory where trained model checkpoints will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 3/3 [00:00<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, beta = 0.010000, Train MSE: 13.663756, Train CE:2.154334, Train KL:0.945002, Val MSE:12.495003, Val CE:2.104598, Train ACC:0.280852, Val ACC:0.286902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 3/3 [00:00<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, beta = 0.010000, Train MSE: 12.019743, Train CE:2.083766, Train KL:1.020754, Val MSE:11.045834, Val CE:2.061779, Train ACC:0.296256, Val ACC:0.301399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3/3 [00:00<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, beta = 0.010000, Train MSE: 10.634787, Train CE:2.044676, Train KL:1.146477, Val MSE:9.823824, Val CE:2.036949, Train ACC:0.304690, Val ACC:0.300689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 3/3 [00:00<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, beta = 0.010000, Train MSE: 9.484139, Train CE:2.034360, Train KL:1.300207, Val MSE:8.833112, Val CE:2.014092, Train ACC:0.311833, Val ACC:0.318329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 3/3 [00:00<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, beta = 0.010000, Train MSE: 8.459760, Train CE:2.017638, Train KL:1.464170, Val MSE:7.862492, Val CE:1.994941, Train ACC:0.337866, Val ACC:0.348642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 3/3 [00:00<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, beta = 0.010000, Train MSE: 7.611962, Train CE:1.999599, Train KL:1.626225, Val MSE:7.053391, Val CE:1.986582, Train ACC:0.383262, Val ACC:0.402778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 3/3 [00:00<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, beta = 0.010000, Train MSE: 6.869578, Train CE:1.979303, Train KL:1.785650, Val MSE:6.403675, Val CE:1.964470, Train ACC:0.437565, Val ACC:0.456103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, beta = 0.010000, Train MSE: 6.204691, Train CE:1.968276, Train KL:1.947868, Val MSE:5.838765, Val CE:1.956694, Train ACC:0.476162, Val ACC:0.486922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 3/3 [00:00<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, beta = 0.010000, Train MSE: 5.646989, Train CE:1.966729, Train KL:2.115103, Val MSE:5.310214, Val CE:1.970527, Train ACC:0.499656, Val ACC:0.497972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, beta = 0.010000, Train MSE: 5.151427, Train CE:1.979931, Train KL:2.282740, Val MSE:4.881950, Val CE:1.979485, Train ACC:0.497676, Val ACC:0.498581\n",
      "Training time: 0.1025 mins\n",
      "Successfully trained and saved the VAE model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tabsyn.train_vae(**raw_config[\"loss_params\"],\n",
    "                 num_epochs = raw_config[\"train\"][\"vae\"][\"num_epochs\"],\n",
    "                 save_path = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the VAE, we embed the training data with the trained encoder and store the embeddings in a direcotry specified by `vae_ckpt_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved pretrained embeddings on disk!\n"
     ]
    }
   ],
   "source": [
    "# embed all inputs in the latent space\n",
    "tabsyn.save_vae_embeddings(X_train_num, X_train_cat,\n",
    "                           vae_ckpt_dir = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Train Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have stored the training data embeddings, we need to load and prepare them for the diffusion model.\n",
    "We load the embeddings using `load_vae_embeddings`. We normalize the embeddings by subtracting the mean and dividing by the standard deviation. Then, we create a Dataloader with the specified `batch_size` and `num_workers` from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latent space embeddings\n",
    "train_z, _ = tabsyn.load_latent_embeddings(os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))  # train_z dim: B x in_dim\n",
    "\n",
    "# normalize embeddings\n",
    "mean, std = train_z.mean(0), train_z.std(0)\n",
    "train_z = (train_z - mean) / std\n",
    "latent_train_data = train_z\n",
    "\n",
    "# create data loader\n",
    "latent_train_loader = DataLoader(\n",
    "    latent_train_data,\n",
    "    batch_size = raw_config[\"train\"][\"diffusion\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"diffusion\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is ready, we instantiate the diffusion model with `instantiate_diffusion`. The input dimension and hidden dimention of the diffusion model is determined by the dimension of the embeddings. \n",
    "Moreover, we instantiate the optimizer and lr scheduler using hyperparameters from config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPDiffusion(\n",
      "  (proj): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=1024, out_features=96, bias=True)\n",
      "  )\n",
      "  (map_noise): PositionalEmbedding()\n",
      "  (time_embed): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "The number of parameters: 10690656\n",
      "Successfully instantiated diffusion model.\n"
     ]
    }
   ],
   "source": [
    "# instantiate diffusion model for training\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = raw_config[\"train\"][\"optim\"][\"diffusion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the diffusion model with `train_diffusion` function.\n",
    "This function takes the following arguements:\n",
    "1. `latent_train_loader`: dataloader for the latent representations which are used to train the diffusion model.\n",
    "2. `num_epochs`: number of training epochs.\n",
    "3. `ckpt_path`: directory where the model checkpoints will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/9: 100%|██████████| 3/3 [00:00<00:00,  4.81it/s, Loss=2.03]\n",
      "Epoch 2/9: 100%|██████████| 3/3 [00:00<00:00,  5.66it/s, Loss=1.73]\n",
      "Epoch 3/9: 100%|██████████| 3/3 [00:00<00:00,  5.37it/s, Loss=1.57]\n",
      "Epoch 4/9: 100%|██████████| 3/3 [00:00<00:00,  5.68it/s, Loss=1.46]\n",
      "Epoch 5/9: 100%|██████████| 3/3 [00:00<00:00,  5.47it/s, Loss=1.36]\n",
      "Epoch 6/9: 100%|██████████| 3/3 [00:00<00:00,  5.58it/s, Loss=1.26]\n",
      "Epoch 7/9: 100%|██████████| 3/3 [00:00<00:00,  5.65it/s, Loss=1.2] \n",
      "Epoch 8/9: 100%|██████████| 3/3 [00:00<00:00,  5.62it/s, Loss=1.19]\n",
      "Epoch 9/9: 100%|██████████| 3/3 [00:00<00:00,  5.53it/s, Loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  5.782466650009155\n"
     ]
    }
   ],
   "source": [
    "# train diffusion model\n",
    "tabsyn.train_diffusion(latent_train_loader,\n",
    "                       num_epochs = raw_config[\"train\"][\"diffusion\"][\"num_epochs\"],\n",
    "                       ckpt_path = os.path.join(MODEL_PATH, DATA_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training model from scratch, we can also load weights of a pre-trained model from a given checkpoint with `load_model_state` function.\n",
    "If we haven't instantiated the VAE and diffusion model beforehand, we need to instantiate them first using `instantiate_vae` and `instantiate_diffusion` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "Successfully instantiated VAE model.\n",
      "MLPDiffusion(\n",
      "  (proj): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=1024, out_features=96, bias=True)\n",
      "  )\n",
      "  (map_noise): PositionalEmbedding()\n",
      "  (time_embed): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "The number of parameters: 10690656\n",
      "Successfully instantiated diffusion model.\n",
      "Loaded model state from /projects/aieng/diffusion_bootcamp/models/tabular/tabsyn/default\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = None)\n",
    "\n",
    "latent_embeddings_path = \"/projects/aieng/diffusion_bootcamp/models/tabular/tabsyn/default/vae\"\n",
    "# load latent embeddings of input data\n",
    "train_z, token_dim = tabsyn.load_latent_embeddings(latent_embeddings_path)\n",
    "\n",
    "# instantiate diffusion model\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = None)\n",
    "\n",
    "pretrained_model_path = \"/projects/aieng/diffusion_bootcamp/models/tabular/tabsyn/default\"\n",
    "# load state from checkpoint\n",
    "tabsyn.load_model_state(ckpt_dir = pretrained_model_path,\n",
    "                        dif_ckpt_name = \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained the model effectively, using `sample` function we can generate synthetic data starting from compelete noise. The input of this function is as follows:\n",
    "\n",
    "1. `train_z`: latent embeddings of the training data.\n",
    "2. `info`: info about the data from the json file we reviewed at the beginning of this notebook.\n",
    "3. `num_inverse`: detokenizer for numerical features.\n",
    "4. `cat_inverse`: detokenizer for categorical features.\n",
    "5. `save_path`: file-path where the synthetic table will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path data/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(27000, 10)\n",
      "Time: 15.525609970092773\n",
      "Saving sampled data to data/synthetic_data/default/tabsyn.csv\n"
     ]
    }
   ],
   "source": [
    "# load data info file\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"info.json\"), \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "data_info[\"token_dim\"] = token_dim\n",
    "\n",
    "# get inverse tokenizers\n",
    "_, _, categories, d_numerical, num_inverse, cat_inverse = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                                     transforms = raw_config[\"transforms\"],\n",
    "                                                                     task_type = raw_config[\"task_type\"],\n",
    "                                                                     inverse = True)\n",
    "\n",
    "# sample data\n",
    "num_samples = train_z.shape[0]\n",
    "in_dim = train_z.shape[1] \n",
    "mean_input_emb = train_z.mean(0)\n",
    "tabsyn.sample(num_samples,\n",
    "              in_dim,\n",
    "              mean_input_emb,\n",
    "              info = data_info,\n",
    "              num_inverse = num_inverse,\n",
    "              cat_inverse = cat_inverse,\n",
    "              save_path = os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here, we review the synthesized data. In the following `evaluate_synthetic_data.ipynb` notebook, we will evaluate this synthesized data with respect to various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501312.34</td>\n",
       "      <td>female</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>married</td>\n",
       "      <td>78.57147</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>465448.700</td>\n",
       "      <td>927171.000</td>\n",
       "      <td>961664.000</td>\n",
       "      <td>100726.0500</td>\n",
       "      <td>73776.375</td>\n",
       "      <td>877114.90000</td>\n",
       "      <td>620114.7500</td>\n",
       "      <td>421979.0600</td>\n",
       "      <td>187410.3400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>999821.44</td>\n",
       "      <td>female</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>single</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>77179.070</td>\n",
       "      <td>33111.530</td>\n",
       "      <td>173841.720</td>\n",
       "      <td>70362.1250</td>\n",
       "      <td>13021.870</td>\n",
       "      <td>880473.60000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>422858.7800</td>\n",
       "      <td>528666.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000.00</td>\n",
       "      <td>female</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>single</td>\n",
       "      <td>43.00000</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for five months</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>-169473.300</td>\n",
       "      <td>-15443.804</td>\n",
       "      <td>-224676.810</td>\n",
       "      <td>1799.6123</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150000.00</td>\n",
       "      <td>male</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>64.54518</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>266024.560</td>\n",
       "      <td>904401.300</td>\n",
       "      <td>960747.500</td>\n",
       "      <td>62932.4060</td>\n",
       "      <td>30830.547</td>\n",
       "      <td>834951.10000</td>\n",
       "      <td>4480.7330</td>\n",
       "      <td>422027.2500</td>\n",
       "      <td>372800.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80000.00</td>\n",
       "      <td>male</td>\n",
       "      <td>high school</td>\n",
       "      <td>single</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>181319.670</td>\n",
       "      <td>382370.700</td>\n",
       "      <td>354707.440</td>\n",
       "      <td>3000.0000</td>\n",
       "      <td>14168.706</td>\n",
       "      <td>101073.74000</td>\n",
       "      <td>621000.0000</td>\n",
       "      <td>4000.0000</td>\n",
       "      <td>2801.1248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500000.00</td>\n",
       "      <td>male</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>married</td>\n",
       "      <td>44.00000</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for five months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>...</td>\n",
       "      <td>240717.280</td>\n",
       "      <td>927171.000</td>\n",
       "      <td>961664.000</td>\n",
       "      <td>62564.4960</td>\n",
       "      <td>47341.360</td>\n",
       "      <td>677374.94000</td>\n",
       "      <td>2001.0645</td>\n",
       "      <td>380413.0300</td>\n",
       "      <td>17833.5210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000.00</td>\n",
       "      <td>male</td>\n",
       "      <td>university</td>\n",
       "      <td>single</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for four months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for five months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>...</td>\n",
       "      <td>98312.480</td>\n",
       "      <td>88146.734</td>\n",
       "      <td>206940.620</td>\n",
       "      <td>2131.2730</td>\n",
       "      <td>1277.121</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3000.0000</td>\n",
       "      <td>3000.0000</td>\n",
       "      <td>3301.5688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130000.00</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>190170.450</td>\n",
       "      <td>924844.000</td>\n",
       "      <td>960859.700</td>\n",
       "      <td>81164.6100</td>\n",
       "      <td>42518.610</td>\n",
       "      <td>788682.40000</td>\n",
       "      <td>25178.9530</td>\n",
       "      <td>344590.4400</td>\n",
       "      <td>27608.4880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10000.00</td>\n",
       "      <td>male</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>single</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>21569.852</td>\n",
       "      <td>8925.561</td>\n",
       "      <td>19056.596</td>\n",
       "      <td>1600.1823</td>\n",
       "      <td>20000.000</td>\n",
       "      <td>8566.44800</td>\n",
       "      <td>414.3890</td>\n",
       "      <td>3692.6938</td>\n",
       "      <td>15087.8560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10000.00</td>\n",
       "      <td>male</td>\n",
       "      <td>high school</td>\n",
       "      <td>married</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>75176.760</td>\n",
       "      <td>52177.234</td>\n",
       "      <td>-169602.620</td>\n",
       "      <td>79965.3200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>731.92474</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL     SEX        EDUCATION MARRIAGE       AGE  \\\n",
       "0  501312.34  female  graduate school  married  78.57147   \n",
       "1  999821.44  female  graduate school   single  34.00000   \n",
       "2   20000.00  female  graduate school   single  43.00000   \n",
       "3  150000.00    male       university  married  64.54518   \n",
       "4   80000.00    male      high school   single  32.00000   \n",
       "5  500000.00    male  graduate school  married  44.00000   \n",
       "6   10000.00    male       university   single  29.00000   \n",
       "7  130000.00  female       university  married  24.00000   \n",
       "8   10000.00    male  graduate school   single  30.00000   \n",
       "9   10000.00    male      high school  married  29.00000   \n",
       "\n",
       "                            PAY_0                           PAY_2  \\\n",
       "0  payment delay for three months     payment delay for one month   \n",
       "1     payment delay for one month     payment delay for one month   \n",
       "2                        pay duly                        pay duly   \n",
       "3     payment delay for one month     payment delay for one month   \n",
       "4     payment delay for one month     payment delay for one month   \n",
       "5  payment delay for three months  payment delay for three months   \n",
       "6  payment delay for three months   payment delay for four months   \n",
       "7  payment delay for three months     payment delay for one month   \n",
       "8     payment delay for one month     payment delay for one month   \n",
       "9     payment delay for one month     payment delay for one month   \n",
       "\n",
       "                            PAY_3                          PAY_4  \\\n",
       "0     payment delay for one month    payment delay for one month   \n",
       "1                        pay duly    payment delay for one month   \n",
       "2  payment delay for three months  payment delay for five months   \n",
       "3     payment delay for one month    payment delay for one month   \n",
       "4     payment delay for one month    payment delay for one month   \n",
       "5  payment delay for three months  payment delay for five months   \n",
       "6  payment delay for three months  payment delay for five months   \n",
       "7     payment delay for one month    payment delay for one month   \n",
       "8     payment delay for one month    payment delay for one month   \n",
       "9  payment delay for three months    payment delay for one month   \n",
       "\n",
       "                            PAY_5  ...   BILL_AMT4   BILL_AMT5   BILL_AMT6  \\\n",
       "0     payment delay for one month  ...  465448.700  927171.000  961664.000   \n",
       "1     payment delay for one month  ...   77179.070   33111.530  173841.720   \n",
       "2                         unknown  ... -169473.300  -15443.804 -224676.810   \n",
       "3     payment delay for one month  ...  266024.560  904401.300  960747.500   \n",
       "4     payment delay for one month  ...  181319.670  382370.700  354707.440   \n",
       "5  payment delay for three months  ...  240717.280  927171.000  961664.000   \n",
       "6  payment delay for three months  ...   98312.480   88146.734  206940.620   \n",
       "7     payment delay for one month  ...  190170.450  924844.000  960859.700   \n",
       "8     payment delay for one month  ...   21569.852    8925.561   19056.596   \n",
       "9     payment delay for one month  ...   75176.760   52177.234 -169602.620   \n",
       "\n",
       "      PAY_AMT1   PAY_AMT2      PAY_AMT3     PAY_AMT4     PAY_AMT5  \\\n",
       "0  100726.0500  73776.375  877114.90000  620114.7500  421979.0600   \n",
       "1   70362.1250  13021.870  880473.60000       0.0000  422858.7800   \n",
       "2    1799.6123      0.000       0.00000       0.0000       0.0000   \n",
       "3   62932.4060  30830.547  834951.10000    4480.7330  422027.2500   \n",
       "4    3000.0000  14168.706  101073.74000  621000.0000    4000.0000   \n",
       "5   62564.4960  47341.360  677374.94000    2001.0645  380413.0300   \n",
       "6    2131.2730   1277.121       0.00000    3000.0000    3000.0000   \n",
       "7   81164.6100  42518.610  788682.40000   25178.9530  344590.4400   \n",
       "8    1600.1823  20000.000    8566.44800     414.3890    3692.6938   \n",
       "9   79965.3200      0.000     731.92474     500.0000       0.0000   \n",
       "\n",
       "      PAY_AMT6  default payment next month  \n",
       "0  187410.3400                           0  \n",
       "1  528666.0000                           0  \n",
       "2       0.0000                           0  \n",
       "3  372800.0000                           0  \n",
       "4    2801.1248                           0  \n",
       "5   17833.5210                           1  \n",
       "6    3301.5688                           1  \n",
       "7   27608.4880                           1  \n",
       "8   15087.8560                           0  \n",
       "9       0.0000                           0  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zhang, Hengrui, et al.** \"Mixed-type tabular data synthesis with score-based diffusion in latent space.\" *International Conference on Learning Representations (ICLR)* (2023).\n",
    "\n",
    "**GitHub Repository:** [Amazon Science - Tabsyn](https://github.com/amazon-science/tabsyn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_models",
   "language": "python",
   "name": "diffusion_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
