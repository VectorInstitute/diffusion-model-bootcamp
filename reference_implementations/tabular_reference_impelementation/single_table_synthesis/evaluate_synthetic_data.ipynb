{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "royal-phase",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "# Synthetic Data Evaluation\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-annotation",
   "metadata": {},
   "source": [
    "In this notebook, we will evaluate the generated synthetic data using both the TabDDPM and TabSyn algorithms based on various metrics. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Density Estimation of Single Column and Pair-wise Correlation]()\n",
    "    \n",
    "    \n",
    "3. [$\\alpha$-Precision and $\\beta$-Recall ]()\n",
    "\n",
    "    \n",
    "4. [Machine Learning Efficiency]()\n",
    "\n",
    "\n",
    "5. [Privacy Protection: Distance to Closest Record (DCR)]()\n",
    "\n",
    "\n",
    "6. [Detection: Classifier Two Sample Tests (C2ST)]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-cornwall",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-speaker",
   "metadata": {},
   "source": [
    "First, we will import functions to evaluate the data using various metrics. Then, we will define the paths to the real train and test data, as well as the TabDDPM and TabSyn generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stretch-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from scripts.eval.eval_density import eval_density\n",
    "from scripts.eval.eval_quality import eval_quality\n",
    "from scripts.eval.eval_mle import eval_mle\n",
    "from scripts.eval.eval_dcr import eval_dcr\n",
    "from scripts.eval.eval_detection import eval_detection\n",
    "\n",
    "\n",
    "dataname = \"default\"\n",
    "\n",
    "DATA_DIR = \"data/tabular\"\n",
    "\n",
    "TRAIN_DATA_PATH = f\"{DATA_DIR}/processed_data/{dataname}/train.csv\"\n",
    "TEST_DATA_PATH = f\"{DATA_DIR}/processed_data/{dataname}/test.csv\"\n",
    "TABDDPM_DATA_PATH = f\"{DATA_DIR}/synthetic_data/{dataname}/tabddpm.csv\"\n",
    "TABSYN_DATA_PATH = f\"{DATA_DIR}/synthetic_data/{dataname}/tabsyn.csv\"\n",
    "INFO_PATH = f\"{DATA_DIR}/processed_data/{dataname}/info.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-virus",
   "metadata": {},
   "source": [
    "# Density Estimation of Single Column and Pair-wise Correlation\n",
    "\n",
    "Two metrics are computed in this section: column shapes and column pair trends.\n",
    "We explain each in the following subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a383a5c",
   "metadata": {},
   "source": [
    "## Single Column Similarity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81863e",
   "metadata": {},
   "source": [
    "Essentially, each column in a table represents a single feature which accepts various values from a certain type; e.g. numerical, categorical, datetime or boolean. This feature can be described as a random variable where the values listed under the column are its samples. As a result, the density distribution of each column can be computed and compared between the real data and the synthetic data.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_column_shapes_2.png\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "The better the distributions match each other, the better the quality of synthetic data.\n",
    "\n",
    "The similarity of the two distriutions can be measured via different metrics for different data types; for example, **KSComplement (Kolmogorov-Smirnov Complement)** is used for numerical features and **TVComplement (Total Variation Distance Complement)** for categorical features. The overall Column Shape Score for the whole table is the average of column shape similarity scores of all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d521918f",
   "metadata": {},
   "source": [
    "**KST (Kolmogorov-Smirnov Test)**\n",
    "computes the cumulative distribution function (CDF) of a numerical random variable for real and synthetic data. Then, finds the maximum different between the two CDFs $M$. Finally, the *KSComplement* score is defined as $1 - M$ so that the higher the score, the more similar the distributions.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_kst.png\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "**TVD (Total Variation Distance)**\n",
    "computes the frequency of each category's appearance under a certain column and defines it as the said categorie's probability. Then, it computes the sum of differences of probabilities between real and synthetic data as \n",
    "$\\delta(R, S) = \\frac{1}{2} \\sum_{\\omega \\in \\Omega}  | R_\\omega - S_\\omega |$\n",
    ".\n",
    "\n",
    "Here, $\\omega$ describes all the possible categories in a column, $\\Omega$. Meanwhile, $R$ and $S$ refer to the real and synthetic frequencies for those categories. The *TVComplement* returns $1-TVD$ so that a higher score means higher quality.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_tvd.png\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb4e44d",
   "metadata": {},
   "source": [
    "## Pair-wise Correlation Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3abc8c0",
   "metadata": {},
   "source": [
    "The correlation between two random variables describes how they vary in relation to each other. The higher the score, the more the trends are alike.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_column_pair_trends.png\" width=\"900\"/>\n",
    "</p>\n",
    "\n",
    "Here, we use different metrics to compute the correlation between different pairs of data types:\n",
    "\n",
    "| Column Type | Metric |\n",
    "| ----------- | ------ |\n",
    "| numerical & numerical | [correlation similarity](https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/correlationsimilarity) |\n",
    "| categorical & categorical | [contingency similarity](https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/contingencysimilarity) |\n",
    "| numerical & categorical | discretize the numerical columns into bins, then apply contingency similarity. |\n",
    "\n",
    "This yields a score between every pair of columns. The **Column Pair Trends** score is the average of all the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "charitable-panel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syn_path='data/tabular/synthetic_data/default/tabddpm.csv'\n",
      "Generating report ...\n",
      "(1/2) Evaluating Column Shapes: : 100%|██████████| 24/24 [00:00<00:00, 51.17it/s]\n",
      "(2/2) Evaluating Column Pair Trends: : 100%|██████████| 276/276 [00:12<00:00, 21.36it/s]\n",
      "\n",
      "Overall Score: 90.23%\n",
      "\n",
      "Properties:\n",
      "- Column Shapes: 93.51%\n",
      "- Column Pair Trends: 86.96%\n",
      "Shape: 0.9350632716049384\n",
      "Trend: 0.8696141571360197\n"
     ]
    }
   ],
   "source": [
    "shape, trend = eval_density(TABDDPM_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH)\n",
    "print(\"Shape:\", shape)\n",
    "print(\"Trend:\", trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-mention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "(1/2) Evaluating Column Shapes: : 100%|██████████| 24/24 [00:00<00:00, 42.47it/s]\n",
      "(2/2) Evaluating Column Pair Trends: : 100%|██████████| 276/276 [00:14<00:00, 18.62it/s]\n",
      "\n",
      "Overall Score: 97.1%\n",
      "\n",
      "Properties:\n",
      "- Column Shapes: 98.68%\n",
      "- Column Pair Trends: 95.52%\n",
      "Shape: 0.9868132716049383\n",
      "Trend: 0.9551999968185801\n"
     ]
    }
   ],
   "source": [
    "shape, trend = eval_density(TABSYN_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH)\n",
    "print(\"Shape:\", shape)\n",
    "print(\"Trend:\", trend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-genome",
   "metadata": {},
   "source": [
    "# $\\alpha$-Precision and $\\beta$-Recall "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9f731",
   "metadata": {},
   "source": [
    "$\\alpha$-Preicison and $\\beta$-Recall are generalizations of Precision and Recall metrics proposed by [Sajjadi et al.](https://proceedings.neurips.cc/paper/2018/hash/f7696a9b362ac5a51c3dc8f098b73923-Abstract.html) in 2018. These metrics can range between $[0, 1]$ and the closest they are to $1$ the better.\n",
    "\n",
    "\n",
    "Preicison measures the fidelity or quality of sythetic data. In more clear terms, it computes the proporation of synthetic datapoints that are *close* to real datapoints. \n",
    "Recall, on the other hand, measures the diversity of synthetic data; i.e. the extent to which these samples cover the full variability of real samples. More clearly, recall computes the proportion of real datapoints that are *close* to synthetic datapoints.\n",
    "\n",
    "\n",
    "If we denote the distibution of real datapoints by $P(X)$ and the distribution of sythetic datapoints by $Q(Y)$, precision is the portion of $Q(Y)$ that can be generated by $P(X)$, while recall is the portion of $P(X)$ that can be generated by $Q(Y)$.\n",
    "\n",
    "To better understand these concepts, let's assume that the real/generated dataponits are samples from an underlying real/generated manifold.\n",
    "Precision measures the proportation of generated datapoints that fall on the real manifold, while recall measures the proportion of real datapoints that fall on the generated manifold.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_precision.png\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_recall.png\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "The Precision-Recall metrics are very sensitive to outliers since even a few outliers can greatly change the shape of the underlying manifold.\n",
    "To address this limitation, $\\pmb{\\alpha}$**-Precision** and $\\pmb{\\beta}$**-Recall** are defined by assuming that a fraction $1−\\alpha$ (or $1−\\beta$) of the real (and synthetic) data are “outliers”, and $\\alpha$ (or $\\beta$) are “typical”. \n",
    "$\\alpha$-Precision is the fraction of synthetic samples that resemble the “most typical” fraction $\\alpha$ of real samples, whereas $\\beta$-Recall is the fraction of real samples covered by the most typical fraction $\\beta$ of synthetic samples.\n",
    "The two metrics are evaluated for all $\\alpha, \\beta \\in [0, 1]$, providing entire precision and recall curves instead of single numbers.\n",
    "\n",
    "\n",
    "To illustrate, consider the below image. Blue and red points are real and generated datapoints, respectively. The large blue and red spheres show the underlying manifold that was estimated from real and generated datapoints.\n",
    "Good quality generated datapoints should fall within the blue sphere like image *(c)*. They should not lie far from the blue sphere like *(a)*. Moreover, they should not be placed too close (or *copied*) to a real datapoint like *(b)*.\n",
    "Image *(d)* shows an outlier in the real datapoints which is cut outside of the manifold due to the application on $\\alpha$ and $\\beta$.\n",
    "If we used vanilla Precision and Recall, the blue sphere's radius should have increased to include the outlier which would also lead it to include noisy synthetic datapoints like *(a)*.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_alpha_precision_beta_recall.png\" width=\"500\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_precision_all, beta_recall_all = eval_quality(\n",
    "    TABDDPM_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH\n",
    ")\n",
    "print(\"Alpha precision:\", alpha_precision_all)\n",
    "print(\"Beta recall:\", beta_recall_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faced-accuracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/tabular/synthetic_data/default/tabsyn.csv\n",
      "=========== All Features ===========\n",
      "Data shape:  (27000, 93)\n",
      "Alpha precision: 0.9901046402724564\n",
      "Beta recall: 0.4702987654320988\n"
     ]
    }
   ],
   "source": [
    "alpha_precision_all, beta_recall_all = eval_quality(\n",
    "    TABSYN_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH\n",
    ")\n",
    "print(\"Alpha precision:\", alpha_precision_all)\n",
    "print(\"Beta recall:\", beta_recall_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-duncan",
   "metadata": {},
   "source": [
    "# Machine Learning Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92c0e04",
   "metadata": {},
   "source": [
    "This method trains a machine learning model (in our case, an XGBoost model) on the synthetic data and evaluates it on the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "classified-editing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:17<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABSYN - Overall score:\n",
      "{'best_acc_scores': {'XGBClassifier': {'accuracy': 0.81,\n",
      "                                       'binary_f1': 0.45297504798464494,\n",
      "                                       'roc_auc': 0.7877443582482191,\n",
      "                                       'weighted_f1': 0.554220929899447}},\n",
      " 'best_auroc_scores': {'XGBClassifier': {'accuracy': 0.814,\n",
      "                                         'binary_f1': 0.4571984435797665,\n",
      "                                         'roc_auc': 0.7888965197353714,\n",
      "                                         'weighted_f1': 0.5580960679415622}},\n",
      " 'best_avg_scores': {'XGBClassifier': {'accuracy': 0.81,\n",
      "                                       'binary_f1': 0.45297504798464494,\n",
      "                                       'roc_auc': 0.7877443582482191,\n",
      "                                       'weighted_f1': 0.554220929899447}},\n",
      " 'best_f1_scores': {'XGBClassifier': {'accuracy': 0.81,\n",
      "                                      'binary_f1': 0.45297504798464494,\n",
      "                                      'roc_auc': 0.7877443582482191,\n",
      "                                      'weighted_f1': 0.554220929899447}},\n",
      " 'best_weighted_scores': {'XGBClassifier': {'accuracy': 0.81,\n",
      "                                            'binary_f1': 0.45297504798464494,\n",
      "                                            'roc_auc': 0.7877443582482191,\n",
      "                                            'weighted_f1': 0.554220929899447}}}\n"
     ]
    }
   ],
   "source": [
    "## does a grid search over given params and reports all scores for each best of them\n",
    "# tabular dataload and tabular transformer look extra\n",
    "overall_score = eval_mle(TABSYN_DATA_PATH, TEST_DATA_PATH, INFO_PATH)\n",
    "print(\"TABSYN - Overall score:\")\n",
    "pprint(overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294ae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:35<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABDDPM - Overall score:\n",
      "{'best_acc_scores': {'XGBClassifier': {'accuracy': 0.8146666666666667,\n",
      "                                       'binary_f1': 0.46124031007751937,\n",
      "                                       'roc_auc': 0.7850053040919847,\n",
      "                                       'weighted_f1': 0.5612639528642225}},\n",
      " 'best_auroc_scores': {'XGBClassifier': {'accuracy': 0.8116666666666666,\n",
      "                                         'binary_f1': 0.4684854186265287,\n",
      "                                         'roc_auc': 0.7883066601188635,\n",
      "                                         'weighted_f1': 0.5662194341712794}},\n",
      " 'best_avg_scores': {'XGBClassifier': {'accuracy': 0.8133333333333334,\n",
      "                                       'binary_f1': 0.47368421052631576,\n",
      "                                       'roc_auc': 0.7745027065422088,\n",
      "                                       'weighted_f1': 0.5704319144701299}},\n",
      " 'best_f1_scores': {'XGBClassifier': {'accuracy': 0.8133333333333334,\n",
      "                                      'binary_f1': 0.47368421052631576,\n",
      "                                      'roc_auc': 0.7745027065422088,\n",
      "                                      'weighted_f1': 0.5704319144701299}},\n",
      " 'best_weighted_scores': {'XGBClassifier': {'accuracy': 0.8133333333333334,\n",
      "                                            'binary_f1': 0.47368421052631576,\n",
      "                                            'roc_auc': 0.7745027065422088,\n",
      "                                            'weighted_f1': 0.5704319144701299}}}\n"
     ]
    }
   ],
   "source": [
    "overall_score = eval_mle(TABDDPM_DATA_PATH, TEST_DATA_PATH, INFO_PATH)\n",
    "print(\"TABDDPM - Overall score:\")\n",
    "pprint(overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803cae50",
   "metadata": {},
   "source": [
    "As baseline, we also evaluate a similar ML model (i.e. XGBoost) on the real training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfce5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:31<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE - Overall score:\n",
      "{'best_acc_scores': {'XGBClassifier': {'accuracy': 0.81,\n",
      "                                       'binary_f1': 0.46629213483146065,\n",
      "                                       'roc_auc': 0.7825501876094182,\n",
      "                                       'weighted_f1': 0.5642753583567985}},\n",
      " 'best_auroc_scores': {'XGBClassifier': {'accuracy': 0.814,\n",
      "                                         'binary_f1': 0.46449136276391556,\n",
      "                                         'roc_auc': 0.7882964420782628,\n",
      "                                         'weighted_f1': 0.5636057524278798}},\n",
      " 'best_avg_scores': {'XGBClassifier': {'accuracy': 0.81,\n",
      "                                       'binary_f1': 0.46629213483146065,\n",
      "                                       'roc_auc': 0.7825501876094182,\n",
      "                                       'weighted_f1': 0.5642753583567985}},\n",
      " 'best_f1_scores': {'XGBClassifier': {'accuracy': 0.81,\n",
      "                                      'binary_f1': 0.46629213483146065,\n",
      "                                      'roc_auc': 0.7825501876094182,\n",
      "                                      'weighted_f1': 0.5642753583567985}},\n",
      " 'best_weighted_scores': {'XGBClassifier': {'accuracy': 0.81,\n",
      "                                            'binary_f1': 0.46629213483146065,\n",
      "                                            'roc_auc': 0.7825501876094182,\n",
      "                                            'weighted_f1': 0.5642753583567985}}}\n"
     ]
    }
   ],
   "source": [
    "overall_score = eval_mle(TRAIN_DATA_PATH, TEST_DATA_PATH, INFO_PATH)\n",
    "print(\"BASELINE - Overall score:\")\n",
    "pprint(overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd38e9",
   "metadata": {},
   "source": [
    "# Privacy Protection: Distance to Closest Record (DCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476c5c1",
   "metadata": {},
   "source": [
    "One of the applications of synthetically generated data is protecting sensitive information while creating similar substitute data that could be used to train machine learning models or published on public platforms.\n",
    "For this purpose, we must ensure that the synthetic datapoints are far enough from any real datapoints to prevent leaking of real sensitive information.\n",
    "\n",
    "One metric that is used for this purpose is **Distance to Closest Record (DCR)**.\n",
    "DCR is the Euclidean distance between a synthetic datapoint and its nearest real datapoint.\n",
    "DCR equal to zero means that the synthetic datapoint will leak the real information, while higher DCR values mean less risk of privacy leakage.\n",
    "\n",
    "`eval_dcr` computes the DCR of each synthetic datapoint to real datapoints in two different sets: training and test. Then, it returns the proportion of synthetic datapoints that are closer to the training dataset than the test dataset.\n",
    "If the size of the training and test datasets are equal, this score should ideally be $0.5$ indicating that the model has not overfit to training data and the synthetic datapoints are not memorized copies of training data.\n",
    "If the size of the training and test datasets are different, the ideal value for this score is #Train / (#Train + #Test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3b0b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat_col_idx': [1, 2, 3, 5, 6, 7, 8, 9, 10],\n",
      " 'column_info': {'0': {},\n",
      "                 '1': {},\n",
      "                 '10': {},\n",
      "                 '11': {},\n",
      "                 '12': {},\n",
      "                 '13': {},\n",
      "                 '14': {},\n",
      "                 '15': {},\n",
      "                 '16': {},\n",
      "                 '17': {},\n",
      "                 '18': {},\n",
      "                 '19': {},\n",
      "                 '2': {},\n",
      "                 '20': {},\n",
      "                 '21': {},\n",
      "                 '22': {},\n",
      "                 '23': {},\n",
      "                 '3': {},\n",
      "                 '4': {},\n",
      "                 '5': {},\n",
      "                 '6': {},\n",
      "                 '7': {},\n",
      "                 '8': {},\n",
      "                 '9': {},\n",
      "                 'categorizes': [0, 1],\n",
      "                 'max': 528666.0,\n",
      "                 'min': 0.0,\n",
      "                 'type': 'categorical'},\n",
      " 'column_names': ['LIMIT_BAL',\n",
      "                  'SEX',\n",
      "                  'EDUCATION',\n",
      "                  'MARRIAGE',\n",
      "                  'AGE',\n",
      "                  'PAY_0',\n",
      "                  'PAY_2',\n",
      "                  'PAY_3',\n",
      "                  'PAY_4',\n",
      "                  'PAY_5',\n",
      "                  'PAY_6',\n",
      "                  'BILL_AMT1',\n",
      "                  'BILL_AMT2',\n",
      "                  'BILL_AMT3',\n",
      "                  'BILL_AMT4',\n",
      "                  'BILL_AMT5',\n",
      "                  'BILL_AMT6',\n",
      "                  'PAY_AMT1',\n",
      "                  'PAY_AMT2',\n",
      "                  'PAY_AMT3',\n",
      "                  'PAY_AMT4',\n",
      "                  'PAY_AMT5',\n",
      "                  'PAY_AMT6',\n",
      "                  'default payment next month'],\n",
      " 'data_path': '/projects/aieng/diffusion_bootcamp/data/tabular/raw_data/default/default '\n",
      "              'of credit card clients.xls',\n",
      " 'file_type': 'xls',\n",
      " 'header': 'infer',\n",
      " 'idx_mapping': {'0': 0,\n",
      "                 '1': 14,\n",
      "                 '10': 22,\n",
      "                 '11': 2,\n",
      "                 '12': 3,\n",
      "                 '13': 4,\n",
      "                 '14': 5,\n",
      "                 '15': 6,\n",
      "                 '16': 7,\n",
      "                 '17': 8,\n",
      "                 '18': 9,\n",
      "                 '19': 10,\n",
      "                 '2': 15,\n",
      "                 '20': 11,\n",
      "                 '21': 12,\n",
      "                 '22': 13,\n",
      "                 '23': 23,\n",
      "                 '3': 16,\n",
      "                 '4': 1,\n",
      "                 '5': 17,\n",
      "                 '6': 18,\n",
      "                 '7': 19,\n",
      "                 '8': 20,\n",
      "                 '9': 21},\n",
      " 'idx_name_mapping': {'0': 'LIMIT_BAL',\n",
      "                      '1': 'SEX',\n",
      "                      '10': 'PAY_6',\n",
      "                      '11': 'BILL_AMT1',\n",
      "                      '12': 'BILL_AMT2',\n",
      "                      '13': 'BILL_AMT3',\n",
      "                      '14': 'BILL_AMT4',\n",
      "                      '15': 'BILL_AMT5',\n",
      "                      '16': 'BILL_AMT6',\n",
      "                      '17': 'PAY_AMT1',\n",
      "                      '18': 'PAY_AMT2',\n",
      "                      '19': 'PAY_AMT3',\n",
      "                      '2': 'EDUCATION',\n",
      "                      '20': 'PAY_AMT4',\n",
      "                      '21': 'PAY_AMT5',\n",
      "                      '22': 'PAY_AMT6',\n",
      "                      '23': 'default payment next month',\n",
      "                      '3': 'MARRIAGE',\n",
      "                      '4': 'AGE',\n",
      "                      '5': 'PAY_0',\n",
      "                      '6': 'PAY_2',\n",
      "                      '7': 'PAY_3',\n",
      "                      '8': 'PAY_4',\n",
      "                      '9': 'PAY_5'},\n",
      " 'inverse_idx_mapping': {'0': 0,\n",
      "                         '1': 4,\n",
      "                         '10': 19,\n",
      "                         '11': 20,\n",
      "                         '12': 21,\n",
      "                         '13': 22,\n",
      "                         '14': 1,\n",
      "                         '15': 2,\n",
      "                         '16': 3,\n",
      "                         '17': 5,\n",
      "                         '18': 6,\n",
      "                         '19': 7,\n",
      "                         '2': 11,\n",
      "                         '20': 8,\n",
      "                         '21': 9,\n",
      "                         '22': 10,\n",
      "                         '23': 23,\n",
      "                         '3': 12,\n",
      "                         '4': 13,\n",
      "                         '5': 14,\n",
      "                         '6': 15,\n",
      "                         '7': 16,\n",
      "                         '8': 17,\n",
      "                         '9': 18},\n",
      " 'metadata': {'columns': {'0': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '1': {'sdtype': 'categorical'},\n",
      "                          '10': {'sdtype': 'categorical'},\n",
      "                          '11': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '12': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '13': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '14': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '15': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '16': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '17': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '18': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '19': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '2': {'sdtype': 'categorical'},\n",
      "                          '20': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '21': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '22': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '23': {'sdtype': 'categorical'},\n",
      "                          '3': {'sdtype': 'categorical'},\n",
      "                          '4': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '5': {'sdtype': 'categorical'},\n",
      "                          '6': {'sdtype': 'categorical'},\n",
      "                          '7': {'sdtype': 'categorical'},\n",
      "                          '8': {'sdtype': 'categorical'},\n",
      "                          '9': {'sdtype': 'categorical'}}},\n",
      " 'name': 'default',\n",
      " 'num_col_idx': [0, 4, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
      " 'target_col_idx': [23],\n",
      " 'task_type': 'binclass',\n",
      " 'test_num': 3000,\n",
      " 'test_path': None,\n",
      " 'train_num': 27000}\n"
     ]
    }
   ],
   "source": [
    "# review json file and its contents\n",
    "with open(INFO_PATH, \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "pprint(data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pursuant-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCR Score, a value closer to 0.9 is better\n",
      "Distance to Closest Record: 0.9066666666666666\n"
     ]
    }
   ],
   "source": [
    "ideal_dcr = data_info[\"train_num\"] / (data_info[\"train_num\"] + data_info[\"test_num\"])\n",
    "\n",
    "dcr_score = eval_dcr(TABDDPM_DATA_PATH, TRAIN_DATA_PATH, TEST_DATA_PATH, INFO_PATH)\n",
    "print(f\"DCR Score, a value closer to {ideal_dcr} is better\")\n",
    "print(\"Distance to Closest Record:\", dcr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d31ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCR Score, a value closer to 0.9 is better\n",
      "Distance to Closest Record: 0.8962222222222223\n"
     ]
    }
   ],
   "source": [
    "ideal_dcr = data_info[\"train_num\"] / (data_info[\"train_num\"] + data_info[\"test_num\"])\n",
    "\n",
    "dcr_score = eval_dcr(TABSYN_DATA_PATH, TRAIN_DATA_PATH, TEST_DATA_PATH, INFO_PATH)\n",
    "print(f\"DCR Score, a value closer to {ideal_dcr} is better\")\n",
    "print(\"Distance to Closest Record:\", dcr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf7d99",
   "metadata": {},
   "source": [
    "# Detection: Classifier Two Sample Tests (C2ST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff5955",
   "metadata": {},
   "source": [
    "This metric evaluates if the synthetic data can be detected from the real data via a machine learning model, hence measuring how difficult it is to distinguish synthetic from real data. A logistic regression model is used in `eval_detection`.\n",
    "\n",
    "This score is measured through below steps:\n",
    "1. Create a single, augmented table that has all the rows of real data and all the rows of synthetic data. Add an extra column to keep track of whether each original row is real or synthetic.\n",
    "2. Split the augmented data to create a training and validation sets.\n",
    "3. Choose a machine learning model. Train the model on the training split. The model will predict whether each row is real or synthetic (i.e. predict the extra column we created in step #1).\n",
    "4. Validate the model on the validation set.\n",
    "5. Repeat steps #2-4 multiple times.\n",
    "The final score is based on the average ROC-AUC score across all the cross validation splits,\n",
    "\n",
    "$score = 1 - (max($ <span style=\"text-decoration:overline\">ROC-AUC</span> $, 0.5) \\times 2 - 1)$\n",
    ".\n",
    "\n",
    "This score can range between $[0, 1]$ with $0$ being the lowest (meaning that the machine learning model can perfectly identify synthetic data apart from the real data), and $1$ being the highest (meaning that the machine learning model cannot identify the synthetic data apart from the real data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "compressed-executive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABSYN - Detection score: 0.9749056954732511\n"
     ]
    }
   ],
   "source": [
    "detection_score = eval_detection(TABSYN_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH, dataname, model=\"tabsyn\")\n",
    "print(\"TABSYN - Detection score:\", detection_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e5725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABDDPM - Detection score: 0.9630604403292181\n"
     ]
    }
   ],
   "source": [
    "detection_score = eval_detection(TABDDPM_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH, dataname, model=\"tabsyn\")\n",
    "print(\"TABDDPM - Detection score:\", detection_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09421b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE - Detection score: 1.0\n"
     ]
    }
   ],
   "source": [
    "detection_score = eval_detection(TRAIN_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH, dataname, model=\"tabsyn\")\n",
    "print(\"BASELINE - Detection score:\", detection_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92710a70",
   "metadata": {},
   "source": [
    "# Missing Value Imputation for the Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5685cc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scripts.impute' from '/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table_synthesis/scripts/impute.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "numeric-crystal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n",
      "{'loss_params': {'lambd': 0.7, 'max_beta': 0.01, 'min_beta': 1e-05},\n",
      " 'model_params': {'d_token': 4, 'factor': 32, 'n_head': 1, 'num_layers': 2},\n",
      " 'task_type': 'binclass',\n",
      " 'train': {'diffusion': {'batch_size': 4096,\n",
      "                         'num_dataset_workers': 4,\n",
      "                         'num_epochs': 9},\n",
      "           'optim': {'diffusion': {'factor': 0.9,\n",
      "                                   'lr': 0.001,\n",
      "                                   'patience': 20,\n",
      "                                   'weight_decay': 0},\n",
      "                     'vae': {'factor': 0.95,\n",
      "                             'lr': 0.001,\n",
      "                             'patience': 10,\n",
      "                             'weight_decay': 0}},\n",
      "           'vae': {'batch_size': 4096,\n",
      "                   'num_dataset_workers': 4,\n",
      "                   'num_epochs': 10}},\n",
      " 'transforms': {'cat_encoding': None,\n",
      "                'cat_min_frequency': None,\n",
      "                'cat_nan_policy': None,\n",
      "                'normalization': 'quantile',\n",
      "                'num_nan_policy': 'mean',\n",
      "                'y_policy': 'default'}}\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(3000, 10)\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(3000, 10)\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(3000, 10)\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(3000, 10)\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(3000, 10)\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(3000, 10)\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(3000, 10)\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(3000, 10)\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(3000, 10)\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(3000, 10)\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "(3000, 10)\n",
      "data_path data/tabular/processed_data/default\n",
      "No NaNs in numerical features, skipping\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3919/4142259032.py\", line 7, in <module>\n",
      "    impute.main(dataname=\"default\", device=device)\n",
      "  File \"/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table_synthesis/scripts/impute.py\", line 221, in main\n",
      "  File \"/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table_synthesis/scripts/impute.py\", line 49, in step\n",
      "    x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# impute missing data\n",
    "from scripts import impute\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "impute.main(dataname=\"default\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c6514ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'impute/tabsyn/default/11.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eval_impute\n\u001b[0;32m----> 2\u001b[0m \u001b[43meval_impute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table_synthesis/scripts/eval/eval_impute.py:24\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(dataname, col)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m     23\u001b[0m     syn_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpute/tabsyn/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m     syn_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msyn_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     target \u001b[38;5;241m=\u001b[39m syn_data[target_col]\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m     syn_y\u001b[38;5;241m.\u001b[39mappend(encoder\u001b[38;5;241m.\u001b[39mtransform(target)\u001b[38;5;241m.\u001b[39mtoarray())\n",
      "File \u001b[0;32m/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'impute/tabsyn/default/11.csv'"
     ]
    }
   ],
   "source": [
    "from scripts.eval import eval_impute\n",
    "eval_impute.main(dataname=\"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_boot_shared",
   "language": "python",
   "name": "diffusion_boot_shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
