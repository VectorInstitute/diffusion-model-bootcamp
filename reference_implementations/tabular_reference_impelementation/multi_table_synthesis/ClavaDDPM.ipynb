{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent tabular data synthesis research has focused on single tables, while real-world applications often involve complex, interconnected tables. Existing methods for multi-relational data synthesis struggle with scalability and long-range dependencies. This paper introduces Cluster Latent Variable guided Denoising Diffusion Probabilistic Models (ClavaDDPM), using clustering labels to model inter-table relationships, particularly foreign key constraints. ClavaDDPM efficiently propagates latent variables across tables, capturing long-range dependencies. Evaluations show ClavaDDPM outperforms existing methods on multi-table data and remains competitive for single-table data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will delve deeper into the implementation of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we import all necessary libraries and modules for setting up the environment. This includes libraries for logging, argument parsing, file path management, and configuration loading. We also import essential packages for data loading, model creation, and training, such as PyTorch and numpy, along with custom modules specific to the ClavaDDPM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "from complex_pipeline import clava_clustering, clava_training, clava_synthesizing, clava_eval, load_configs\n",
    "from pipeline_modules import load_multi_table\n",
    "from gen_single_report import gen_single_report\n",
    "from report_utils import get_multi_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we establish the setup for model training by loading the configuration file, which includes the necessary parameters and settings for the training process. The configuration file, stored in `json` format, is read and parsed into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "### DEBUG: use debug configs to obtain the results faster ###\n",
    "# config_path = 'configs/debug_movie_lens.json'\n",
    "### DEBUG: use debug configs to obtain the results faster ###\n",
    "\n",
    "config_path = 'configs/california_20_debug.json'\n",
    "configs, save_dir = load_configs(config_path)\n",
    "\n",
    "# Display config\n",
    "json_str = json.dumps(configs, indent=4)\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we load and preprocess the dataset based on the configuration settings. We demonstrate the dataset's metadata and parent-child relationships to provide a clearer understanding of its structure. Following this, we perform clustering to preprocess the data, facilitating the training process for the ClavaDDPM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multi-table dataset\n",
    "tables, relation_order, dataset_meta = load_multi_table(configs['general']['data_dir'])\n",
    "\n",
    "# Tables is a dictionary of the multi-table dataset\n",
    "print(\"{} We show the keys of the tables dictionary below {}\".format(\"=\"*20, \"=\"*20))\n",
    "print(tables.keys())\n",
    "\n",
    "# Relation order is the topological order of the multi-table dataset\n",
    "print(\"{} We show the relation order below {}\".format(\"=\"*20, \"=\"*20))\n",
    "print(relation_order)\n",
    "\n",
    "# Visualize the parent-child relationship within the multi-table dataset\n",
    "multi_meta = get_multi_metadata(tables, relation_order)\n",
    "multi_meta.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper introduces relation-aware clustering to model parent-child constraints and leverages diffusion models for controlled tabular data synthesis. Specifically, Gaussian Process Latent Variable Models (GPLVM) are used to discover low-dimensional manifolds in noisy, high-dimensional spaces. We run the clustering algorithm below to preprocess the data for training the ClavaDDPM model. Additionally, we empirically determine the distribution of table sizes in the dataset, which will be used in the later sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering on the multi-table dataset\n",
    "# updates the tables dictionary with augmented tables and computes group size distributions\n",
    "tables, all_group_lengths_prob_dicts = clava_clustering(tables, relation_order, save_dir, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/clavaDDPM.png\" alt=\"ClavaDDPM Model Pipeline\" width=\"960\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines the training process for the ClavaDDPM model. The diagram above, taken from the original paper, illustrates the main steps: (a) latent learning and table augmentation (steps 1-2), (b) training (steps 3-5), and (c) synthesis (steps 6-8). Specifically, the clustering process corresponds to latent learning and is used for table augmentation during training. The training code below trains the conditional diffusion models and the cluster classifier models. Subsequently, we implement the generation process, starting with sampling the table size and conducting conditional generation to satisfy the parent-child constraints (i.e., relation order).\n",
    "\n",
    "Additionally, various callbacks are configured to monitor and save the model during training. The trainer is implemented using a custom PyTorch function, specifying parameters such as the number of epochs and checkpoints. The training process is then initiated, logging progress and completing the model's training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation order is the topological order of the multi-table dataset\n",
    "print(\"{} We show the relation order again, each line indicates one conditional generative model {}\".format(\"=\"*20, \"=\"*20))\n",
    "print(relation_order)\n",
    "\n",
    "# Launch training or use the pre-trained models\n",
    "tables, models = clava_training(tables, relation_order, save_dir, configs)\n",
    "\n",
    "# if the training process takes too long, please run the following command to load pre-trained models and samples\n",
    "# !unzip -o clavaDDPM_workspace.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completion of the training, the model is evaluated using the test dataset. To assess the model's performance, we first use the `clava_synthesizing` function to generate synthetic samples and showcase the results qualitatively. We initiate the generation process by sampling the table size (i.e., the number of rows per table) and performing conditional generation to meet the parent-child constraints (i.e., relation order). Quantitative evaluations will be conducted in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tables, synthesizing_time_spent, matching_time_spent = clava_synthesizing(\n",
    "    tables, \n",
    "    relation_order, \n",
    "    save_dir, \n",
    "    all_group_lengths_prob_dicts, \n",
    "    models,\n",
    "    configs,\n",
    "    sample_scale=1 if not 'debug' in configs else configs['debug']['sample_scale']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we quantitatively evaluate the generated tabular data by computing metrics to determine the accuracy of the predictions, specifically assessing how closely the generated data matches the observed samples in the training set.\n",
    "\n",
    "In particular, the critical multi-table metrics are as follows:\n",
    "\n",
    "1. Pair-wise column correlation (k-hop): This metric measures the correlations between columns from tables at a distance k (e.g., 0-hop for columns within the same table, 1-hop for a column and a column from its parent or child table).\n",
    "\n",
    "2. Average 2-way: This metric computes the average of all k-hop column-pair correlations, taking into account both short-range (k = 0) and longer-range (k > 0) dependencies.\n",
    "\n",
    "Please refer to the `ts-diff` notebook for the other metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-table Evaluation\n",
    "report = clava_eval(tables, save_dir, configs, relation_order, cleaned_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the multi-table metrics\n",
    "for key, val in report.items():\n",
    "    if key in ['hop_relation', 'avg_scores', 'all_avg_score']:\n",
    "        if isinstance(val, dict):\n",
    "            print(\"{:20}\".format(key))\n",
    "            for k, v in val.items():\n",
    "                print(\"{:20}: {}\".format(k, v))\n",
    "        else:\n",
    "            print(\"{:20}: {}\".format(key, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the synthetic data for single-table metric evaluation\n",
    "shutil.copy(os.path.join(configs['general']['data_dir'], 'dataset_meta.json'), os.path.join(save_dir, 'dataset_meta.json'))\n",
    "\n",
    "for table_name in tables.keys():\n",
    "    shutil.copy(os.path.join(configs['general']['workspace_dir'], table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "    shutil.copy(os.path.join(configs['general']['data_dir'], f'{table_name}_domain.json'), os.path.join(save_dir, f'{table_name}_domain.json'))\n",
    "\n",
    "test_tables, _, _ = load_multi_table(save_dir)\n",
    "real_tables, _, _ = load_multi_table(configs['general']['data_dir'])\n",
    "\n",
    "# Single table metrics\n",
    "for table_name in tables.keys():\n",
    "    print(f'Generating report for {table_name}')\n",
    "    real_data = real_tables[table_name]['df']\n",
    "    syn_data = cleaned_tables[table_name]\n",
    "    domain_dict = real_tables[table_name]['domain']\n",
    "\n",
    "    if configs['general']['workspace_dir'] is not None:\n",
    "        test_data = test_tables[table_name]['df']\n",
    "    else:\n",
    "        test_data = None\n",
    "\n",
    "    gen_single_report(\n",
    "        real_data, \n",
    "        syn_data,\n",
    "        domain_dict,\n",
    "        table_name,\n",
    "        save_dir,\n",
    "        alpha_beta_sample_size=200_000,\n",
    "        test_data=test_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pang, Wei, et al.** \"ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models.\" *preprint* (2024).\n",
    "\n",
    "**GitHub Repository:** [ClavaDDPM](https://github.com/weipang142857/ClavaDDPM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabsyn_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
