{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models\n",
    "\n",
    "Recent tabular data synthesis research has focused on single tables, while real-world applications often involve complex, interconnected tables. Existing methods for multi-relational data synthesis struggle with scalability and long-range dependencies. This paper introduces Cluster Latent Variable guided Denoising Diffusion Probabilistic Models (ClavaDDPM), using clustering labels to model inter-table relationships, particularly foreign key constraints. ClavaDDPM efficiently propagates latent variables across tables, capturing long-range dependencies. Evaluations show ClavaDDPM outperforms existing methods on multi-table data and remains competitive for single-table data.\n",
    "\n",
    "In the following sections, we will delve deeper into the implementation of this method. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Load Configuration]()\n",
    "\n",
    "\n",
    "3. [Data Loading and Preprocessing]()\n",
    "    \n",
    "    \n",
    "4. [ClavaDDPM Algorithm]()\n",
    "\n",
    "    4.1. [Overview]()\n",
    "    \n",
    "    4.2. [Clustring]()\n",
    "    \n",
    "    4.3. [Model Training]()\n",
    "    \n",
    "    4.4. [Model Sampling]()\n",
    "    \n",
    "    \n",
    "    \n",
    "6. [Model Evaluation]()\n",
    "\n",
    "    6.1. [Multi-Table Metrics]()\n",
    "    \n",
    "    6.2. [Single-Table Metrics]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup\n",
    "\n",
    "In this section, we import all necessary libraries and modules for setting up the environment. This includes libraries for logging, argument parsing, file path management, and configuration loading. We also import essential packages for data loading, model creation, and training, such as PyTorch and numpy, along with custom modules specific to the ClavaDDPM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "from complex_pipeline import clava_clustering, clava_training, clava_load_pretrained, clava_synthesizing, clava_load_synthesized_data, clava_eval, load_configs\n",
    "from pipeline_modules import load_multi_table\n",
    "from gen_single_report import gen_single_report\n",
    "from report_utils import get_multi_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Configuration\n",
    "\n",
    "In this section, we establish the setup for model training by loading the configuration file, which includes the necessary parameters and settings for the training process. The configuration file, stored in `json` format, is read and parsed into a dictionary. We print out the entire configuration file in the code cell below and will explain the hyperparameters in more detail further down to clarify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"general\": {\n",
      "        \"data_dir\": \"/fs01/projects/aieng/diffusion_bootcamp/data/tabular/raw_data/california_20/train\",\n",
      "        \"exp_name\": \"california_20\",\n",
      "        \"workspace_dir\": \"clavaDDPM_workspace/california\",\n",
      "        \"sample_prefix\": \"\"\n",
      "    },\n",
      "    \"clustering\": {\n",
      "        \"parent_scale\": 1.0,\n",
      "        \"num_clusters\": {\n",
      "            \"individual\": 25\n",
      "        },\n",
      "        \"clustering_method\": \"both\"\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"d_layers\": [\n",
      "            512,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            512\n",
      "        ],\n",
      "        \"dropout\": 0.0,\n",
      "        \"num_timesteps\": 2000,\n",
      "        \"model_type\": \"mlp\",\n",
      "        \"iterations\": 10000,\n",
      "        \"batch_size\": 4096,\n",
      "        \"lr\": 0.0006,\n",
      "        \"gaussian_loss_type\": \"mse\",\n",
      "        \"weight_decay\": 1e-05,\n",
      "        \"scheduler\": \"cosine\"\n",
      "    },\n",
      "    \"classifier\": {\n",
      "        \"d_layers\": [\n",
      "            128,\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            512,\n",
      "            256,\n",
      "            128\n",
      "        ],\n",
      "        \"lr\": 0.0001,\n",
      "        \"dim_t\": 128,\n",
      "        \"batch_size\": 4096,\n",
      "        \"iterations\": 1000\n",
      "    },\n",
      "    \"sampling\": {\n",
      "        \"batch_size\": 20000,\n",
      "        \"classifier_scale\": 1.0\n",
      "    },\n",
      "    \"matching\": {\n",
      "        \"num_matching_clusters\": 1,\n",
      "        \"matching_batch_size\": 1000,\n",
      "        \"unique_matching\": true,\n",
      "        \"no_matching\": false\n",
      "    },\n",
      "    \"debug\": {\n",
      "        \"sample_scale\": 0.1\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config_path = 'configs/california_20_vector.json'\n",
    "configs, save_dir = load_configs(config_path)\n",
    "\n",
    "# Display config\n",
    "json_str = json.dumps(configs, indent=4)\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "In this section, we load and preprocess the dataset based on the configuration settings. We demonstrate the dataset's metadata and parent-child relationships to provide a clearer understanding of its structure. Following this, we perform clustering to preprocess the data, facilitating the training process for the ClavaDDPM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table name: household, Train dataframe shape: (110900, 10), Total dataframe shape: (110900, 10)\n",
      "Numerical (110900, 3)\n",
      "Categorical (110900, 7)\n",
      "Numerical data shape: (110900, 3)\n",
      "Categorical data shape: (110900, 7)\n",
      "Processing household table successfully!\n",
      "\n",
      "Table name: individual, Train dataframe shape: (304510, 15), Total dataframe shape: (304510, 15)\n",
      "Numerical (304510, 2)\n",
      "Categorical (304510, 13)\n",
      "Numerical data shape: (304510, 2)\n",
      "Categorical data shape: (304510, 13)\n",
      "Processing individual table successfully!\n",
      "\n",
      "==================== We show the keys of the tables dictionary below ====================\n",
      "dict_keys(['household', 'individual'])\n",
      "\n",
      "==================== We show the relation order below ====================\n",
      "[[None, 'household'], ['household', 'individual']]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Metadata Pages: 1 -->\n",
       "<svg width=\"298pt\" height=\"604pt\"\n",
       " viewBox=\"0.00 0.00 298.00 604.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 600)\">\n",
       "<title>Metadata</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-600 294,-600 294,4 -4,4\"/>\n",
       "<!-- household -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>household</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"#000000\" d=\"M45,-376.5C45,-376.5 185,-376.5 185,-376.5 191,-376.5 197,-382.5 197,-388.5 197,-388.5 197,-583.5 197,-583.5 197,-589.5 191,-595.5 185,-595.5 185,-595.5 45,-595.5 45,-595.5 39,-595.5 33,-589.5 33,-583.5 33,-583.5 33,-388.5 33,-388.5 33,-382.5 39,-376.5 45,-376.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"115\" y=\"-580.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">household</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"33,-572.5 197,-572.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-557.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">household_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-542.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">FARM : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-527.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">OWNERSHP : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-512.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ACREHOUS : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-497.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">TAXINCL : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-482.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PROPINSR : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-467.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">COSTELEC : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-452.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">VALUEH : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-437.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ROOMS : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-422.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PLUMBING : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-407.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PUMA : categorical</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"33,-399.5 197,-399.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-384.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Primary key: household_id</text>\n",
       "</g>\n",
       "<!-- individual -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>individual</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"#000000\" d=\"M12,-.5C12,-.5 218,-.5 218,-.5 224,-.5 230,-6.5 230,-12.5 230,-12.5 230,-312.5 230,-312.5 230,-318.5 224,-324.5 218,-324.5 218,-324.5 12,-324.5 12,-324.5 6,-324.5 0,-318.5 0,-312.5 0,-312.5 0,-12.5 0,-12.5 0,-6.5 6,-.5 12,-.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"115\" y=\"-309.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">individual</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"0,-301.5 230,-301.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-286.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">individual_id : id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-271.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RELATE : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-256.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">SEX : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-241.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">AGE : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-226.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">MARST : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-211.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RACE : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-196.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">CITIZEN : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-181.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">SPEAKENG : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-166.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">SCHOOL : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-151.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">EDUC : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-136.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">GRADEATT : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-121.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">SCHLTYPE : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-106.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">EMPSTAT : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">CLASSWKR : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-76.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">INCTOT : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-61.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">DISABWRK : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">household_id : id</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"0,-38.5 230,-38.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-23.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Primary key: individual_id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Foreign key (household): household_id</text>\n",
       "</g>\n",
       "<!-- household&#45;&gt;individual -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>household&#45;&gt;individual</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M115,-376.4777C115,-363.0552 115,-349.0952 115,-335.026\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"118.5,-324.7683 115.0001,-334.7683 111.5,-324.7683 118.5,-324.7683\"/>\n",
       "<text text-anchor=\"middle\" x=\"202.5\" y=\"-346.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\"> &#160;household_id → household_id</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f16f46effa0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load multi-table dataset\n",
    "# In this step, we load the multi-table dataset according to the 'dataset_meta.json' file located in the data_dir.\n",
    "# We organize the multi-table dataset as a dictionary of tables, a list of relation orders, and a dictionary of dataset metadata.\n",
    "tables, relation_order, dataset_meta = load_multi_table(configs['general']['data_dir'])\n",
    "\n",
    "# Tables is a dictionary of the multi-table dataset\n",
    "print(\"{} We show the keys of the tables dictionary below {}\".format(\"=\"*20, \"=\"*20))\n",
    "print(tables.keys())\n",
    "print(\"\")\n",
    "\n",
    "# Relation order is the topological order of the multi-table dataset\n",
    "print(\"{} We show the relation order below {}\".format(\"=\"*20, \"=\"*20))\n",
    "print(relation_order)\n",
    "print(\"\")\n",
    "\n",
    "# Visualize the parent-child relationship within the multi-table dataset\n",
    "multi_meta = get_multi_metadata(tables, relation_order)\n",
    "multi_meta.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClavaDDPM Algorithm\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "<img src=\"assets/clavaDDPM.png\" alt=\"ClavaDDPM Model Pipeline\" width=\"960\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines the training process for the ClavaDDPM model. The diagram above, taken from the original paper, illustrates the main steps: \n",
    "\n",
    "(a) **Latent learning and table augmentation (steps 1-2):** This step crossponds to clustering section, where we aim to augmente each table with associated clustering labels that used to capture inter-table relationships.\n",
    "\n",
    "(b) **Training (steps 3-5):** This step corresponds to the model training section, where we train separate conditional diffusion models and the cluster classifier models on each augmented table.\n",
    "\n",
    "(c) **Synthesis (steps 6-8):** This step corresponds to the model sampling section, where we sample the table size and generate data based on the parent-child constraints (i.e., relation order).\n",
    "\n",
    "We will implement and demonstrate each section below, step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "To get started, this paper first introduces relation-aware clustering to model parent-child constraints and leverages diffusion models for controlled tabular data synthesis. Specifically, [Gaussian Process Latent Variable Models (GPLVM)](https://pyro.ai/examples/gplvm.html) are used to discover low-dimensional manifolds in noisy, high-dimensional spaces. We run the clustering algorithm below to preprocess the data for training the ClavaDDPM model. Additionally, we empirically determine the distribution of table sizes in the dataset, which will be used in the later sampling process.\n",
    "\n",
    "Mathematically, let $x$ and $y$ denote the child and parent tables, respectively. We consider $k$ clusters and model the distribution of $h = (x; \\lambda y)$ with a Gaussian distribution around its corresponding centroid $c$, i.e.,\n",
    "$$\n",
    "P(h) = \\sum_{c=1}^{k} P(c) P(h \\mid c) = \\sum_{c=1}^{k} \\pi_c \\mathcal{N}(h; \\mu_c, \\Sigma_c),\n",
    "$$\n",
    "where the coefficient $\\lambda$ is the reweighting term called the parent scale. We opt for diagonal covariance, i.e., $\\Sigma_c = \\operatorname{diag}(\\ldots, \\sigma_l^2, \\ldots)$, which, when properly optimized, immediately satisfies our assumptions that the foreign key groups are conditionally independent of their parent rows given the cluster. \n",
    "\n",
    "A summary of the important parameters for the clustering step includes:\n",
    "- `parent_scale`: reweighting coefficient $\\lambda$ for the parent table. The default value is 1.0. It is not a sensitive factor. Recommended range for tuning: 1.0 to 2.\n",
    "- `num_clusters`: the number of clustering centers $k$ for child tables. The default is 20. Too few or too many clusters may compromise performance. Recommended range for tuning: 10 to 50.\n",
    "- `clustering_method`: ClavaDDPM provides two ways to initialize GMM-based clustering. `gmm` uses `kmeans` for initialization, and the default method `both` uses `k-means++`. It is recommended to use `both`.\n",
    "\n",
    "We expect the clava_cluster function to return following outputs:\n",
    "- `tables`: containing the updated relational tables with data augmentation, where the latent variable is attached to the parent tables.\n",
    "- `all_group_lengths_prob_dicts`: which is a dictionary that computes group size distributions for each table, used in the sampling stage to determine the size of the tables to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the clustering parameters below ====================\n",
      "parent_scale: 1.0\n",
      "num_clusters: {'individual': 25}\n",
      "clustering_method: both\n",
      "\n",
      "Clustering checkpoint found, loading...\n"
     ]
    }
   ],
   "source": [
    "# Display important clustering parameters\n",
    "params_clustering = configs['clustering']\n",
    "print(\"{} We show the clustering parameters below {}\".format(\"=\"*20, \"=\"*20))\n",
    "for key, val in params_clustering.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")\n",
    "\n",
    "# Clustering on the multi-table dataset\n",
    "tables, all_group_lengths_prob_dicts = clava_clustering(tables, relation_order, save_dir, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the clustering results set, we can proceed to launch the training in PyTorch. As the dataset contains multiple tables, we train separate conditional diffusion models and cluster classifier models on each augmented table. Important parameters for the training process include:\n",
    "\n",
    "- `d_layers`: the dimension of layers in the diffusion model. \n",
    "- `num_timesteps`: the number of diffusion steps for adding noise and denoising. \n",
    "- `iterations`: the number of training iterations. The default is 10000. Recommended range for tuning: 5000 to 20000.\n",
    "- `batch_size`: the batch size for training. The default is 4096. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs['diffusion']\n",
    "print(\"{} We show the important sampling parameters below {}\".format(\"=\"*20, \"=\"*20))\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Training from Scratch\n",
    "The training process is implemented using a custom PyTorch function, specifying parameters such as the number of epochs and checkpoints. Various callbacks are configured to monitor and save the model during training. The training process is then initiated, logging progress and completing the model's training. Finally, the trained models are saved to the specified directory and returned for further use. This process is happening in the `train_model` function, which gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the training process.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the trained models and logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation order is the topological order of the multi-table dataset\n",
    "print(\"{} We show the relation order again, each line indicates one conditional generative model {}\".format(\"=\"*20, \"=\"*20))\n",
    "print(relation_order)\n",
    "\n",
    "# Launch training from scratch\n",
    "models = clava_training(tables, relation_order, save_dir, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Models\n",
    "If the training process from scratch takes too long, please run the following command to load pre-trained models and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "california_20 clavaDDPM_workspace/california\n"
     ]
    }
   ],
   "source": [
    "print(configs['general']['exp_name'], configs['general']['workspace_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None -> household checkpoint found, loading...\n",
      "household -> individual checkpoint found, loading...\n"
     ]
    }
   ],
   "source": [
    "# Use the pre-trained models\n",
    "pretrained_dir = \"/fs01/projects/diffusion_bootcamp/models/tabular/clavaDDPM/california_20_pretrained\"\n",
    "models = clava_load_pretrained(relation_order, pretrained_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Sampling\n",
    "\n",
    "To assess the trained model's performance, we generate synthetic samples and showcase the results qualitatively. We initiate the generation process by sampling the table size (i.e., the number of rows per table) and performing conditional generation to meet the parent-child constraints (i.e., relation order). Quantitative evaluations will be conducted in the next section.\n",
    "\n",
    "Important parameters for the sampling process include:\n",
    "- `batch_size`: Mini-batch size for sampling.\n",
    "- `classifier_scale` ($\\eta$): Controls the magnitude of classifier gradients during guided sampling, balancing sample quality and conditional sampling accuracy. The default value is 1.0. When $\\eta = 0$ (disabling classifier conditioning), single column densities (1-way) may improve but fail to capture long-range correlations. When $\\eta = 2$, the increased conditioning weight significantly improves the modeling of multi-hop correlations compared to $\\eta = 0$. Recommended tuning range: 0 to 2.\n",
    "\n",
    "<!-- We also conduct a matching process to determine the parent-child table relationship of the generated data. This process uses an approximate nearest neighbor search-based matching technique, providing a universal solution to the multi-parent relational synthesis problem for a child table with multiple parents.\n",
    "\n",
    "Important parameters are as follows:\n",
    "- `num_matching_clusters`: Number of clusters used in the matching process.\n",
    "- `matching_batch_size`: Mini-batch size for table matching.\n",
    "- `unique_matching`: Boolean flag indicating if the matching result should be unique.\n",
    "- `no_matching`: Boolean flag to disable the matching process. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the important sampling parameters below ====================\n",
      "batch_size: 20000\n",
      "classifier_scale: 1.0\n",
      "\n",
      "==================== We show the important matching parameters below ====================\n",
      "num_matching_clusters: 1\n",
      "matching_batch_size: 1000\n",
      "unique_matching: True\n",
      "no_matching: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs['sampling']\n",
    "print(\"{} We show the important sampling parameters below {}\".format(\"=\"*20, \"=\"*20))\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data from Scratch\n",
    "To generate synthetic data from scratch, we run the following code cell. This `clava_synthesizing` function gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the synthetic data.\n",
    "- `all_group_lengths_prob_dicts`: a dictionary that computes group size distributions for each table, used in the sampling stage to determine the size of the tables to generate.\n",
    "- `models`: the trained diffusion models.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the sampling process.\n",
    "- `sample_scale`: the scale factor for the sampling process.\n",
    "\n",
    "The synthetic data will be saved in the specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating None -> household\n",
      "Sample size: 11090\n",
      "Sample timestep    0\n",
      "Generating household -> individual\n",
      "Sample size: 30451\n",
      "Sample timestep    0\n",
      "Sample timestep    0\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data from scratch\n",
    "cleaned_tables, synthesizing_time_spent, matching_time_spent = clava_synthesizing(\n",
    "    tables, \n",
    "    relation_order, \n",
    "    save_dir, \n",
    "    all_group_lengths_prob_dicts, \n",
    "    models,\n",
    "    configs,\n",
    "    sample_scale=1 if not 'debug' in configs else configs['debug']['sample_scale']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as some integer values are saved as strings during this process, we convert them back to integers for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast int values that saved as string to int for further evaluation\n",
    "for key in cleaned_tables.keys():\n",
    "    for col in cleaned_tables[key].columns:\n",
    "        if cleaned_tables[key][col].dtype == 'object':\n",
    "            try:\n",
    "                cleaned_tables[key][col] = cleaned_tables[key][col].astype(int)\n",
    "            except ValueError:\n",
    "                print(f\"Column {col} cannot be converted to int.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-Synthesized Data\n",
    "If the generation process takes too long, please run the following command to load pre-synthesized data. We provide the pre-synthesized data using the aforementioned pre-trained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic tables found, loading...\n",
      "Synethic tables loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load pre-synthesized data\n",
    "pretrained_dir = \"/fs01/projects/diffusion_bootcamp/models/tabular/clavaDDPM/california_20_pretrained\"\n",
    "cleaned_tables = clava_load_synthesized_data(tables.keys(), pretrained_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Table Metrics\n",
    "In this step, we quantitatively evaluate the generated tabular data by computing metrics to determine the accuracy of the predictions, specifically assessing how closely the generated data matches the observed samples in the reference dataset.\n",
    "\n",
    "In particular, the critical multi-table metrics are as follows:\n",
    "\n",
    "1. Pair-wise column correlation (k-hop): This metric measures the correlations between columns from tables at a distance k (e.g., 0-hop for columns within the same table, 1-hop for a column and a column from its parent or child table).\n",
    "\n",
    "2. Average 2-way: This metric computes the average of all k-hop column-pair correlations, taking into account both short-range (k = 0) and longer-range (k > 0) dependencies.\n",
    "\n",
    "We use the [SDV evaluation API](https://docs.sdv.dev/sdv/multi-table-data/evaluation/data-quality) to obtain these metrics. For more details about the computation process, refer to their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating multi-table report for clavaDDPM_workspace/california/california_20\n",
      "Generating report ...\n",
      "(1/4) Evaluating Column Shapes: : 100%|██████████| 28/28 [00:00<00:00, 29.46it/s]\n",
      "(2/4) Evaluating Column Pair Trends: : 100%|██████████| 191/191 [00:45<00:00,  4.21it/s]\n",
      "(3/4) Evaluating Cardinality: : 100%|██████████| 1/1 [00:00<00:00, 13.62it/s]\n",
      "(4/4) Evaluating Intertable Trends: : 100%|██████████| 187/187 [00:53<00:00,  3.50it/s]\n",
      "\n",
      "Overall Score: 95.05%\n",
      "\n",
      "Properties:\n",
      "- Column Shapes: 97.08%\n",
      "- Column Pair Trends: 94.33%\n",
      "- Cardinality: 96.23%\n",
      "- Intertable Trends: 92.55%\n"
     ]
    }
   ],
   "source": [
    "# Multi-table Evaluation\n",
    "report = clava_eval(tables, save_dir, configs, relation_order, cleaned_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hop_relation metrics:\n",
      "                   1-hop column correlation, format '(Parent Table, Child Table, Column 1, Column 2): Correlation score'\n",
      "                     ('household', 'individual', 'FARM', 'RELATE'): 0.9474703463939407\n",
      "                     ('household', 'individual', 'FARM', 'SEX'): 0.9959712406755361\n",
      "                     ('household', 'individual', 'FARM', 'AGE'): 0.9575335052828032\n",
      "                     ...... other rows are omitted ......\n",
      "\n",
      "                   0-hop column correlation, format '(Parent Table, Parent Table, Column 1, Column 2): Correlation score'\n",
      "                     ('household', 'household', 'FARM', 'OWNERSHP'): 0.9851758340847611\n",
      "                     ('household', 'household', 'FARM', 'ACREHOUS'): 0.9971055004508567\n",
      "                     ('household', 'household', 'FARM', 'TAXINCL'): 0.992741208295762\n",
      "                     ...... other rows are omitted ......\n",
      "\n",
      "avg_scores metrics:\n",
      "                   1-hop column correlation: 0.925540958105889\n",
      "                   0-hop column correlation: 0.9433169666170189\n",
      "all_avg_score       : 0.9344289623614539\n"
     ]
    }
   ],
   "source": [
    "# Print out the multi-table metrics\n",
    "n_rows = 3\n",
    "for key, val in report.items():\n",
    "    if key in ['hop_relation', 'avg_scores', 'all_avg_score']:\n",
    "        if key == 'hop_relation':\n",
    "            print(\"{} metrics:\".format(key))\n",
    "            for k, v in val.items():\n",
    "                if k > 0:\n",
    "                    print(\"{:20}-hop column correlation, format '(Parent Table, Child Table, Column 1, Column 2): Correlation score'\".format(k))\n",
    "                else:\n",
    "                    print(\"{:20}-hop column correlation, format '(Parent Table, Parent Table, Column 1, Column 2): Correlation score'\".format(k))\n",
    "                for i_row, (k2, v2) in enumerate(v.items()):\n",
    "                    if i_row < n_rows:\n",
    "                        print(\"{:20} {}: {}\".format('', k2, v2))\n",
    "                print(\"{:20} ...... other rows are omitted ......\\n\".format(''))\n",
    "        elif key == 'avg_scores':\n",
    "            print(\"{} metrics:\".format(key))\n",
    "            for k, v in val.items():\n",
    "                print(\"{:20}-hop column correlation: {}\".format(k, v))\n",
    "        elif key == 'all_avg_score':\n",
    "            print(\"{:20}: {}\".format(key, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Table Metrics\n",
    "While here we study multi-table metrics, we also provide a notebook to evaluate single table metrics for each synthesized table. Please refer to `single_table_synthesis/evalutate_synthetic_data.ipynb` for more details. \n",
    "\n",
    "<!-- The key metrics are as follows:\n",
    "To study single table metrics please refer to `single_table_synthesis/evalutate_synthetic_data.ipynb` notebook. We highlight some of the key metrics below:\n",
    "- Single Column Similarity Score: Compares the density distributions of individual columns in real and synthetic data to measure similarity.\n",
    "\n",
    "- Pair-wise Correlation Score: Evaluates the preservation of relationships between pairs of columns, using correlation or contingency similarity.\n",
    "\n",
    "- alpha-Precision and beta-Recall: Precision assesses the quality of synthetic data by how closely it matches real data points. Recall measures the diversity by how well synthetic data covers the variability of real data.\n",
    "\n",
    "- Privacy Protection: Distance to Closest Record (DCR): Ensures synthetic data points are sufficiently distant from real data points to prevent privacy leakage.\n",
    "\n",
    "- Detection: Classifier Two Sample Tests (C2ST): Measures the realism of synthetic data by evaluating if a machine learning model can distinguish it from real data. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Prepare the synthetic data and reference data for single-table metric evaluation\n",
    "shutil.copy(os.path.join(configs['general']['data_dir'], 'dataset_meta.json'), os.path.join(save_dir, 'dataset_meta.json'))\n",
    "for table_name in tables.keys():\n",
    "    shutil.copy(os.path.join(save_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "    # uncomment and run the following line if you want to use the pre-synthesized data\n",
    "    # shutil.copy(os.path.join(pretrained_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "\n",
    "    shutil.copy(os.path.join(configs['general']['data_dir'], f'{table_name}_domain.json'), os.path.join(save_dir, f'{table_name}_domain.json'))\n",
    "\n",
    "test_tables, _, _ = load_multi_table(save_dir, verbose=False)\n",
    "real_tables, _, _ = load_multi_table(configs['general']['data_dir'], verbose=False)\n",
    "\n",
    "# Single table metrics\n",
    "for table_name in tables.keys():\n",
    "    print(f'Generating report for {table_name}')\n",
    "    real_data = real_tables[table_name]['df']\n",
    "    syn_data = cleaned_tables[table_name]\n",
    "    domain_dict = real_tables[table_name]['domain']\n",
    "\n",
    "    if configs['general']['workspace_dir'] is not None:\n",
    "        test_data = test_tables[table_name]['df']\n",
    "    else:\n",
    "        test_data = None\n",
    "\n",
    "    gen_single_report(\n",
    "        real_data, \n",
    "        syn_data,\n",
    "        domain_dict,\n",
    "        table_name,\n",
    "        save_dir,\n",
    "        alpha_beta_sample_size=200_000,\n",
    "        test_data=test_data\n",
    "    ) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Pang, Wei, et al.** \"ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models.\" *preprint* (2024).\n",
    "\n",
    "**GitHub Repository:** [ClavaDDPM](https://github.com/weipang142857/ClavaDDPM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_models",
   "language": "python",
   "name": "diffusion_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
