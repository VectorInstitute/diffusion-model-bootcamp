{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSDI for Time Series Forecasting\n",
    "\n",
    "This notebook demonstrates the usage of the CSDI (Conditional Score-based Diffusion Models) model for time series forecasting tasks, specifically for electricity consumption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/54/70/e07c381e6488a77094f04c85c9caf1c8008cdc30778f7019bc52e5285ef0/gdown-5.2.0-py3-none-any.whl.metadata\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (3.12.4)\n",
      "Requirement already satisfied: requests[socks] in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports and Model Setup\n",
    "\n",
    "In this section, we import necessary libraries and modules required for the implementation of the CSDI model. This includes standard data handling libraries like `numpy` and `pandas`, deep learning libraries from `torch`, and specific components for building and training the CSDI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import gdown\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Add the parent directory to sys.path to import local modules\n",
    "sys.path.append(os.path.abspath(os.path.join('.')))\n",
    "\n",
    "from main_model import CSDI_base\n",
    "from dataset_forecasting import get_dataloader\n",
    "from utils import train, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "\n",
    "Run this cell to download the dataset if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists in data/electricity_nips. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "def download_data(folder_url, output_path='data/electricity_nips'):\n",
    "    \"\"\"\n",
    "    Download all files from a Google Drive folder and save them to the specified output path.\n",
    "    Checks if data already exists in the output directory before downloading.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Check if the directory is not empty, implying data might already be downloaded\n",
    "    if os.listdir(output_path):\n",
    "        print(f\"Data already exists in {output_path}. Skipping download.\")\n",
    "        return\n",
    "    \n",
    "    # Extract the folder ID from the URL\n",
    "    folder_id = folder_url.split('/')[-1]\n",
    "    \n",
    "    # List files in the folder\n",
    "    url = f\"https://drive.google.com/drive/folders/{folder_id}\"\n",
    "    temp_output_path = os.path.join(output_path, \"temp\")\n",
    "    os.makedirs(temp_output_path, exist_ok=True)\n",
    "    output = gdown.download_folder(url, output=temp_output_path, quiet=False, use_cookies=False)\n",
    "    \n",
    "    for root, dirs, files in os.walk(temp_output_path):\n",
    "        for file in files:\n",
    "            shutil.move(os.path.join(root, file), output_path)\n",
    "\n",
    "    shutil.rmtree(temp_output_path)\n",
    "    print(f\"Downloaded files to {output_path}\")\n",
    "\n",
    "# Example usage with a specified folder URL\n",
    "folder_url = \"https://drive.google.com/drive/folders/1krZQofLdeQrzunuKkLXy8L_kMzQrVFI_\"\n",
    "download_data(folder_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Configuration\n",
    "First, we determine the appropriate computation environment. If a GPU is available, the model will utilize it for faster computation; otherwise, it defaults to using the CPU. This ensures that the setup is optimized for performance regardless of the hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Argument Parsing\n",
    "The experiment's settings are loaded from a YAML configuration file, allowing easy adjustments to the model and training parameters. Modifications to these settings via command line arguments are directly reflected in the configuration, ensuring that each experiment can be finely tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"train\": {\n",
      "        \"epochs\": 100,\n",
      "        \"batch_size\": 8,\n",
      "        \"lr\": 0.001,\n",
      "        \"itr_per_epoch\": 100000000.0\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"layers\": 4,\n",
      "        \"channels\": 64,\n",
      "        \"nheads\": 8,\n",
      "        \"diffusion_embedding_dim\": 128,\n",
      "        \"beta_start\": 0.0001,\n",
      "        \"beta_end\": 0.5,\n",
      "        \"num_steps\": 50,\n",
      "        \"schedule\": \"quad\",\n",
      "        \"is_linear\": true\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"is_unconditional\": 0,\n",
      "        \"timeemb\": 128,\n",
      "        \"featureemb\": 16,\n",
      "        \"target_strategy\": \"test\",\n",
      "        \"num_sample_features\": 64\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"{\n",
    "    \"train\": {\n",
    "        \"epochs\": 100,  # Total number of training cycles through the entire dataset.\n",
    "        \"batch_size\": 8,  # Number of data samples processed before the model's internal parameters are updated.\n",
    "        \"lr\": 0.001,  # Learning rate, determines the step size at each iteration while moving toward a minimum of the loss function.\n",
    "        \"itr_per_epoch\": 100000000.0  # Presumably meant to be iterations per epoch.\n",
    "    },\n",
    "    \"diffusion\": {\n",
    "        \"layers\": 4,  # Number of layers in the diffusion model, affects depth and complexity.\n",
    "        \"channels\": 64,  # Number of channels in each layer, influences the model's capacity to process information.\n",
    "        \"nheads\": 8,  # Number of attention heads in transformer-based models, affects the model's ability to focus on different parts of the input sequence.\n",
    "        \"diffusion_embedding_dim\": 128,  # Dimension of the embeddings used in the diffusion process, impacts the representational power.\n",
    "        \"beta_start\": 0.0001,  # Initial value of the noise schedule, dictates how much noise starts the diffusion process.\n",
    "        \"beta_end\": 0.5,  # Final value in the noise schedule, controls how much noise is removed by the end of the diffusion process.\n",
    "        \"num_steps\": 50,  # Total number of steps in the diffusion process from start to completion.\n",
    "        \"schedule\": \"quad\",  # Type of scheduling for the beta values, 'quad' implies a quadratic progression.\n",
    "        \"is_linear\": True  # Indicates whether the scheduling progression is linear, set to true which conflicts with 'quad' indicating a possible oversight or specific implementation.\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"is_unconditional\": 0,  # Specifies whether the model is unconditional (0 indicates conditional).\n",
    "        \"timeemb\": 128,  # Dimension of the time embeddings, used in models that incorporate timing information in their predictions.\n",
    "        \"featureemb\": 16,  # Dimension of the feature embeddings, provides additional contextual information per feature.\n",
    "        \"target_strategy\": \"test\",  # Strategy for targeting in training/testing, 'test' might indicate a specific approach or mode used during evaluation.\n",
    "        \"num_sample_features\": 64  # Number of features to sample, relevant in scenarios like feature ablation or when working with high-dimensional data.\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def load_config(config_path='config/base_forecasting.yaml'):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "config = load_config()\n",
    "print(json.dumps(config, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "Data loaders are set up for the training, validation, and testing phases. These loaders are crucial for managing the data flow during model training and evaluation, ensuring efficient handling of data batches and the appropriate application of missing data simulations as specified in the experimental setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataloaders\n",
    "datatype = 'electricity'\n",
    "target_dim = 370  # for electricity dataset\n",
    "\n",
    "train_loader, valid_loader, test_loader, scaler, mean_scaler = get_dataloader(\n",
    "    datatype=datatype,\n",
    "    device=device,\n",
    "    batch_size=config['train']['batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSDI_Forecasting(CSDI_base):\n",
    "    \"\"\"\n",
    "     class CSDI_Forecasting Initializes the forecasting model derived from the CSDI_base class:\n",
    "       - Sets up the model with specific configurations and assigns it to a computational device.\n",
    "       - Initializes key attributes including the dimensionality of the target data and the number of sampling features based on the model configuration.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, device, target_dim):\n",
    "        super(CSDI_Forecasting, self).__init__(target_dim, config, device)\n",
    "        self.target_dim_base = target_dim\n",
    "        self.num_sample_features = config[\"model\"][\"num_sample_features\"]\n",
    "\n",
    "    \"\"\"\n",
    "    def process_data\n",
    "    - Processes the input batch for the model, formatting and preparing data for subsequent operations.\n",
    "    - `batch`: The input batch containing observed data, masks, and timepoints.\n",
    "    - Transfers all relevant data to the computation device and converts it to floating-point format.\n",
    "    - Reorders dimensions of observed data and masks to match the expected input format of the model.\n",
    "    - Initializes a tensor for cut lengths to dynamically manage sequence processing.\n",
    "    - Generates a tensor of feature identifiers for subsequent data handling processes.\n",
    "    \"\"\"\n",
    "    def process_data(self, batch):\n",
    "        observed_data = batch[\"observed_data\"].to(self.device).float()\n",
    "        observed_mask = batch[\"observed_mask\"].to(self.device).float()\n",
    "        observed_tp = batch[\"timepoints\"].to(self.device).float()\n",
    "        gt_mask = batch[\"gt_mask\"].to(self.device).float()\n",
    "\n",
    "        observed_data = observed_data.permute(0, 2, 1)\n",
    "        observed_mask = observed_mask.permute(0, 2, 1)\n",
    "        gt_mask = gt_mask.permute(0, 2, 1)\n",
    "\n",
    "        cut_length = torch.zeros(len(observed_data)).long().to(self.device)\n",
    "        for_pattern_mask = observed_mask\n",
    "\n",
    "        feature_id=torch.arange(self.target_dim_base).unsqueeze(0).expand(observed_data.shape[0],-1).to(self.device)\n",
    "\n",
    "        return (\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            observed_tp,\n",
    "            gt_mask,\n",
    "            for_pattern_mask,\n",
    "            cut_length,\n",
    "            feature_id, \n",
    "        )  \n",
    "          \n",
    "    \"\"\"\n",
    "    def sample_features\n",
    "    - Randomly samples a subset of features from the observed data for model processing.\n",
    "    - `observed_data`: Tensor containing the observed data points.\n",
    "    - `observed_mask`: Tensor indicating the presence of observed data points.\n",
    "    - `feature_id`: Tensor of feature identifiers corresponding to observed data.\n",
    "    - `gt_mask`: Tensor of ground truth masks indicating valid data points for evaluation.\n",
    "    - Samples a specified number of features (defined by `num_sample_features`) from each observation in the batch.\n",
    "    - Reorganizes the data, masks, and feature identifiers based on the sampled features to prepare for model input.\n",
    "    - Returns the newly formatted data, masks, feature identifiers, and ground truth masks.\n",
    "    \"\"\"\n",
    "    def sample_features(self,observed_data, observed_mask,feature_id,gt_mask):\n",
    "        size = self.num_sample_features\n",
    "        self.target_dim = size\n",
    "        extracted_data = []\n",
    "        extracted_mask = []\n",
    "        extracted_feature_id = []\n",
    "        extracted_gt_mask = []\n",
    "        \n",
    "        for k in range(len(observed_data)):\n",
    "            ind = np.arange(self.target_dim_base)\n",
    "            np.random.shuffle(ind)\n",
    "            extracted_data.append(observed_data[k,ind[:size]])\n",
    "            extracted_mask.append(observed_mask[k,ind[:size]])\n",
    "            extracted_feature_id.append(feature_id[k,ind[:size]])\n",
    "            extracted_gt_mask.append(gt_mask[k,ind[:size]])\n",
    "        extracted_data = torch.stack(extracted_data,0)\n",
    "        extracted_mask = torch.stack(extracted_mask,0)\n",
    "        extracted_feature_id = torch.stack(extracted_feature_id,0)\n",
    "        extracted_gt_mask = torch.stack(extracted_gt_mask,0)\n",
    "        return extracted_data, extracted_mask,extracted_feature_id, extracted_gt_mask\n",
    "\n",
    "    \"\"\"\n",
    "    def get_side_info\n",
    "    - Generates side information combining time embeddings and feature embeddings based on provided masks.\n",
    "    - `observed_tp`: Tensor containing the time points of observations.\n",
    "    - `cond_mask`: Conditional mask tensor that specifies which data points are used for conditioning.\n",
    "    - `feature_id`: Optional tensor of feature identifiers; used if model's target dimension differs from the base.\n",
    "    - Constructs time embeddings for each time point and expands these across the target dimensions.\n",
    "    - Depending on the model's configuration, either generates a static feature embedding for all features or specific embeddings based on the provided `feature_id`.\n",
    "    - Concatenates time and feature embeddings to form a comprehensive side information tensor.\n",
    "    - Reorders dimensions to match the expected input structure for further model processes.\n",
    "    - If the model conditions on the input (is not unconditional), includes the conditional mask in the side information.\n",
    "    - Returns the compiled side information for use in the model’s prediction or forecasting tasks.\n",
    "    \"\"\"\n",
    "    def get_side_info(self, observed_tp, cond_mask,feature_id=None):\n",
    "        B, K, L = cond_mask.shape\n",
    "\n",
    "        time_embed = self.time_embedding(observed_tp, self.emb_time_dim)  # (B,L,emb)\n",
    "        time_embed = time_embed.unsqueeze(2).expand(-1, -1, self.target_dim, -1)\n",
    "\n",
    "        if self.target_dim == self.target_dim_base:\n",
    "            feature_embed = self.embed_layer(\n",
    "                torch.arange(self.target_dim).to(self.device)\n",
    "            )  # (K,emb)\n",
    "            feature_embed = feature_embed.unsqueeze(0).unsqueeze(0).expand(B, L, -1, -1)\n",
    "        else:\n",
    "            feature_embed = self.embed_layer(feature_id).unsqueeze(1).expand(-1,L,-1,-1)\n",
    "        side_info = torch.cat([time_embed, feature_embed], dim=-1)  # (B,L,K,*)\n",
    "        side_info = side_info.permute(0, 3, 2, 1)  # (B,*,K,L)\n",
    "\n",
    "        if self.is_unconditional == False:\n",
    "            side_mask = cond_mask.unsqueeze(1)  # (B,1,K,L)\n",
    "            side_info = torch.cat([side_info, side_mask], dim=1)\n",
    "\n",
    "        return side_info\n",
    "\n",
    "    \"\"\"\n",
    "    def forward\n",
    "    - Defines the forward pass for the model, processing input data and executing training or evaluation steps.\n",
    "    - `batch`: Input batch containing observed data, masks, and additional information.\n",
    "    - `is_train`: Indicator of whether the model is in training mode (1) or evaluation mode (0).\n",
    "    - Processes the input batch to format and prepare data structures for model operations.\n",
    "    - Conditionally samples a subset of features from the observed data if in training mode and feature sampling is necessary due to dimensionality constraints.\n",
    "    - Sets the target dimensionality based on the model's base or sampled features.\n",
    "    - Determines the conditioning mask based on the mode of operation, using ground truth masks for evaluation or generating test patterns for training.\n",
    "    - Retrieves side information incorporating time and feature embeddings tailored to the conditioning context.\n",
    "    - Selects the appropriate loss calculation function based on the training or evaluation context.\n",
    "    - Computes and returns the loss by comparing model predictions with actual observations and side information, adjusted for the specified training or evaluation mode.\n",
    "    \"\"\"\n",
    "    def forward(self, batch, is_train=1):\n",
    "        (\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            observed_tp,\n",
    "            gt_mask,\n",
    "            _,\n",
    "            _,\n",
    "            feature_id, \n",
    "        ) = self.process_data(batch)\n",
    "        if is_train == 1 and (self.target_dim_base > self.num_sample_features):\n",
    "            observed_data, observed_mask,feature_id,gt_mask = \\\n",
    "                    self.sample_features(observed_data, observed_mask,feature_id,gt_mask)\n",
    "        else:\n",
    "            self.target_dim = self.target_dim_base\n",
    "            feature_id = None\n",
    "\n",
    "        if is_train == 0:\n",
    "            cond_mask = gt_mask\n",
    "        else: #test pattern\n",
    "            cond_mask = self.get_test_pattern_mask(\n",
    "                observed_mask, gt_mask\n",
    "            )\n",
    "\n",
    "        side_info = self.get_side_info(observed_tp, cond_mask, feature_id)\n",
    "\n",
    "        loss_func = self.calc_loss if is_train == 1 else self.calc_loss_valid\n",
    "\n",
    "        return loss_func(observed_data, cond_mask, observed_mask, side_info, is_train)\n",
    "\n",
    "    \"\"\"\n",
    "    def evaluate\n",
    "    - Conducts evaluation of the model by generating imputed samples and preparing metrics.\n",
    "    - `batch`: Input batch containing observed data and masks.\n",
    "    - `n_samples`: Number of imputation samples to generate for each point.\n",
    "    - Processes the batch to extract data and masks, including the length for each sequence to avoid redundancy.\n",
    "    - Sets up a non-gradient context for evaluation to prevent backpropagation and save computation.\n",
    "    - Uses ground truth masks to determine conditional and target masks.\n",
    "    - Retrieves side information based on observed time points and conditional masks.\n",
    "    - Generates multiple imputed data samples.\n",
    "    - Returns the generated samples along with the observed data and masks for further assessment.\n",
    "    \"\"\"\n",
    "    def evaluate(self, batch, n_samples):\n",
    "        (\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            observed_tp,\n",
    "            gt_mask,\n",
    "            _,\n",
    "            _,\n",
    "            feature_id, \n",
    "        ) = self.process_data(batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cond_mask = gt_mask\n",
    "            target_mask = observed_mask * (1-gt_mask)\n",
    "\n",
    "            side_info = self.get_side_info(observed_tp, cond_mask)\n",
    "\n",
    "            samples = self.impute(observed_data, cond_mask, side_info, n_samples)\n",
    "\n",
    "        return samples, observed_data, target_mask, observed_mask, observed_tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup and Execution\n",
    "\n",
    "This segment highlights the setup and execution of utilizing CSDI for time series forecasting of electricity data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization\n",
    "The model, `CSDI_Forecasting`, is initialized based on predefined configurations. This specialized model is designed to handle electricity time series data, inheriting robust functionalities from its base class to effectively manage the specific requirements of time series forecasting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model = CSDI_Forecasting(config, device, target_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Output Folder Setup\n",
    "Before training the model, an output directory is created to store training artifacts such as model checkpoints, logs, and output files. The directory name includes a timestamp to ensure uniqueness and to help track experiments based on the date and time they were performed.\n",
    "\n",
    "- **Directory Naming**: The folder is named using the current date and time, which helps in organizing and retrieving model training sessions based on when they were conducted.\n",
    "- **Creation**: The directory is created on the file system, ensuring it exists before any training outputs are written to it. This prevents errors related to file writing during model training.\n",
    "\n",
    "### Model Training Process\n",
    "The model is trained using the specified configurations, data loaders, and the path to the output directory. The training function is designed to handle both the training and validation phases within each epoch, allowing for a comprehensive assessment of model performance over time.\n",
    "\n",
    "- **Training Function**: Takes the model, training configurations, and data loaders as inputs. Additionally, it accepts the path to the output folder where the training results are stored.\n",
    "- **Validation Data**: Optionally, a validation loader can be passed to periodically evaluate model performance on a separate validation set during the training process.\n",
    "\n",
    "### Execution\n",
    "Upon execution, the training process iteratively updates the model weights based on the loss computed from the training data. It also evaluates the model on the validation set, if provided, to monitor its performance on unseen data. Results and model states are saved in the designated output directory, facilitating post-training evaluations and model deployment.\n",
    "\n",
    "### Loss Function\n",
    "The loss function employed in the CSDI model is designed to optimize the model's ability to denoise data:\n",
    "- **Denoising Loss**: During training, the model calculates the loss as the squared difference between the actual noise added to the data in the forward process and the noise predicted by the model during the reverse diffusion process. This loss function is key to training the model to accurately reverse the noise addition, effectively reconstructing the original data from its noisy version.\n",
    "\n",
    "### Metrics\n",
    "1. **Mean Absolute Error (MAE)**: This metric measures the average magnitude of errors in a set of predictions, without considering their direction. It's a linear score that averages the absolute differences between predicted and actual values, providing a straightforward interpretation of prediction accuracy.\n",
    "2. **Continuous Ranked Probability Score (CRPS)**: CRPS is used to assess the accuracy of probabilistic predictions. It measures the difference between the predicted cumulative distribution function and the empirical distribution function of the observed data. This score is particularly useful for evaluating the performance of models that generate probabilistic or distributional forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output folder\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "foldername = f\"./save/forecasting_{datatype}_{current_time}/\"\n",
    "os.makedirs(foldername, exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "with open(foldername + \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 691/691 [04:20<00:00,  2.65it/s, avg_epoch_loss=0.219, epoch=1]\n",
      "100%|██████████| 691/691 [04:22<00:00,  2.63it/s, avg_epoch_loss=0.204, epoch=2]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.196, epoch=3]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.186, epoch=4]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.177, epoch=5]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.181, epoch=6]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.65it/s, avg_epoch_loss=0.183, epoch=7]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.181, epoch=8]\n",
      "100%|██████████| 691/691 [04:20<00:00,  2.65it/s, avg_epoch_loss=0.175, epoch=9]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.17, epoch=10] \n",
      "100%|██████████| 691/691 [04:22<00:00,  2.63it/s, avg_epoch_loss=0.165, epoch=11]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.163, epoch=12]\n",
      "100%|██████████| 691/691 [04:22<00:00,  2.63it/s, avg_epoch_loss=0.167, epoch=13]\n",
      "100%|██████████| 691/691 [04:23<00:00,  2.63it/s, avg_epoch_loss=0.168, epoch=14]\n",
      "100%|██████████| 691/691 [04:23<00:00,  2.63it/s, avg_epoch_loss=0.167, epoch=15]\n",
      "100%|██████████| 691/691 [04:24<00:00,  2.61it/s, avg_epoch_loss=0.161, epoch=16]\n",
      "100%|██████████| 691/691 [04:24<00:00,  2.62it/s, avg_epoch_loss=0.158, epoch=17]\n",
      "100%|██████████| 691/691 [04:22<00:00,  2.63it/s, avg_epoch_loss=0.164, epoch=18]\n",
      "100%|██████████| 691/691 [04:22<00:00,  2.63it/s, avg_epoch_loss=0.16, epoch=19] \n",
      "100%|██████████| 1/1 [00:24<00:00, 24.28s/it, valid_avg_epoch_loss=0.161, epoch=19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss is updated to  0.16142742335796356 at 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 691/691 [04:21<00:00,  2.65it/s, avg_epoch_loss=0.156, epoch=20]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.164, epoch=21]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.156, epoch=22]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.163, epoch=23]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.162, epoch=24]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.65it/s, avg_epoch_loss=0.158, epoch=25]\n",
      "100%|██████████| 691/691 [04:20<00:00,  2.65it/s, avg_epoch_loss=0.16, epoch=26] \n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.158, epoch=27]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.156, epoch=28]\n",
      "100%|██████████| 691/691 [04:22<00:00,  2.64it/s, avg_epoch_loss=0.156, epoch=29]\n",
      "100%|██████████| 691/691 [04:21<00:00,  2.65it/s, avg_epoch_loss=0.16, epoch=30] \n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.16, epoch=31] \n",
      "100%|██████████| 691/691 [04:21<00:00,  2.64it/s, avg_epoch_loss=0.165, epoch=32]\n",
      " 67%|██████▋   | 462/691 [02:56<01:27,  2.62it/s, avg_epoch_loss=0.159, epoch=34]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(\n",
    "    model,\n",
    "    config['train'],\n",
    "    train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    foldername=foldername\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Pretrained Model\n",
    "\n",
    "### Function for Loading Model State\n",
    "To enhance or expedite the training process, or for evaluation purposes, you may start with a model that has already been trained. The function `load_pretrained_model` facilitates the loading of these pretrained weights into an existing model architecture.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `model`: The model instance into which the pretrained weights will be loaded.\n",
    "  - `modelfolder`: The subdirectory under `./save/` where the pretrained model is stored, defaulting to `pretrained`.\n",
    "  - `device`: The computing device (CPU or GPU) where the model will be loaded. This ensures that the model is compatible with the hardware used for subsequent operations.\n",
    "\n",
    "### Execution\n",
    "The function constructs the full path to the pretrained model's state dictionary file (`model.pth`) using the specified `modelfolder`. It then loads this state dictionary into the model, ensuring that all model parameters are updated accordingly.\n",
    "\n",
    "- **Model Compatibility**:\n",
    "  - It is crucial that the model architecture into which the weights are being loaded matches the architecture of the model when it was saved. Incompatibility in architectures will lead to errors during the loading process.\n",
    "\n",
    "### Usage\n",
    "To utilize a pretrained model, simply pass your initialized but untrained model to the `load_pretrained_model` function. This setup allows you to leverage previously learned patterns, potentially reducing training time and improving model robustness from the outset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(model, datatype='electricity', modelfolder='pretrained', device=device):\n",
    "    model_path = f\"./save/forecasting_{datatype}_{modelfolder}/model.pth\"\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Check if the state_dict keys start with 'module.'\n",
    "    if list(state_dict.keys())[0].startswith('module.'):\n",
    "        # Remove the 'module.' prefix\n",
    "        state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './save/forecasting_electricity_pretrained/model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load pre-trained model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m pretrained_model\u001b[38;5;241m.\u001b[39mtarget_dim \u001b[38;5;241m=\u001b[39m target_dim\n",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m, in \u001b[0;36mload_pretrained_model\u001b[0;34m(model, datatype, modelfolder, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_pretrained_model\u001b[39m(model, datatype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melectricity\u001b[39m\u001b[38;5;124m'\u001b[39m, modelfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[1;32m      2\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./save/forecasting_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatatype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Check if the state_dict keys start with 'module.'\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule.\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Remove the 'module.' prefix\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.8/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.8/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.8/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './save/forecasting_electricity_pretrained/model.pth'"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "pretrained_model = load_pretrained_model(model)\n",
    "pretrained_model.target_dim = target_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "### Evaluation Parameters Configuration\n",
    "Prior to evaluating the models, several key parameters are established:\n",
    "- **`test_loader`**: The data loader for the test dataset.\n",
    "- **`nsample`**: Specifies the number of samples to generate during model evaluation, set to 100 for comprehensive testing.\n",
    "- **`scaler`** and **`mean_scaler`**: Define scaling factors for the data. These parameters adjust the data normalization during the evaluation to match the conditions used during model training.\n",
    "\n",
    "### Data Loader Configuration\n",
    "- The test data loader is updated to include the new missing ratio, ensuring that the evaluation tests the model's ability to handle and impute missing data effectively.\n",
    "\n",
    "### Model Evaluation\n",
    "- **Current Model Evaluation**: The initially trained model is evaluated using the updated `test_loader`. This step is crucial for understanding the baseline performance of the model on the test set.\n",
    "- **Pre-trained Model Evaluation**: Additionally, a pre-trained model is evaluated under the same conditions. This is particularly useful for comparing the effectiveness of pre-training and fine-tuning strategies on model performance.\n",
    "\n",
    "### Execution\n",
    "Both the current and pre-trained models are evaluated with the specified number of samples and scaling parameters. The results are stored in a designated folder, facilitating subsequent analysis and comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m nsample \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# number of samples for evaluation\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnsample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmean_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfoldername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfoldername\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/gsharma/diffusion_model_bootcamp/reference_implementations/time_series_reference_impelementation/CSDI/utils.py:134\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, test_loader, nsample, scaler, mean_scaler, foldername)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(test_loader, mininterval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m, maxinterval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50.0\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m it:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_no, test_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(it, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 134\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m         samples, c_target, eval_points, observed_points, observed_time \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    137\u001b[0m         samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (B,nsample,L,K)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 185\u001b[0m, in \u001b[0;36mCSDI_Forecasting.evaluate\u001b[0;34m(self, batch, n_samples)\u001b[0m\n\u001b[1;32m    181\u001b[0m     target_mask \u001b[38;5;241m=\u001b[39m observed_mask \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mgt_mask)\n\u001b[1;32m    183\u001b[0m     side_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_side_info(observed_tp, cond_mask)\n\u001b[0;32m--> 185\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobserved_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples, observed_data, target_mask, observed_mask, observed_tp\n",
      "File \u001b[0;32m/fs01/home/gsharma/diffusion_model_bootcamp/reference_implementations/time_series_reference_impelementation/CSDI/main_model.py:173\u001b[0m, in \u001b[0;36mCSDI_base.impute\u001b[0;34m(self, observed_data, cond_mask, side_info, n_samples)\u001b[0m\n\u001b[1;32m    171\u001b[0m     noisy_target \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m cond_mask) \u001b[38;5;241m*\u001b[39m current_sample)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    172\u001b[0m     diff_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cond_obs, noisy_target], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,2,K,L)\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffmodel(diff_input, side_info, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    175\u001b[0m coeff1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_hat[t] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    176\u001b[0m coeff2 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_hat[t]) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha[t]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set evaluation parameters\n",
    "nsample = 100  # number of samples for evaluation\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(\n",
    "    model,\n",
    "    test_loader,\n",
    "    nsample=nsample,\n",
    "    scaler=scaler,\n",
    "    mean_scaler=mean_scaler,\n",
    "    foldername=foldername\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- Tashiro, Yusuke, et al. \"CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation.\" *Advances in Neural Information Processing Systems*. 2021. [GitHub Repository](https://github.com/ermongroup/CSDI)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
