{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSDI for Time Series Forecasting\n",
    "\n",
    "This notebook demonstrates the usage of the CSDI (Conditional Score-based Diffusion Models) model for time series forecasting tasks, specifically for electricity consumption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/54/70/e07c381e6488a77094f04c85c9caf1c8008cdc30778f7019bc52e5285ef0/gdown-5.2.0-py3-none-any.whl.metadata\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (3.12.4)\n",
      "Requirement already satisfied: requests[socks] in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports and Model Setup\n",
    "\n",
    "In this section, we import necessary libraries and modules required for the implementation of the CSDI model. This includes standard data handling libraries like `numpy` and `pandas`, deep learning libraries from `torch`, and specific components for building and training the CSDI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "# Add the parent directory to sys.path to import local modules\n",
    "sys.path.append(os.path.abspath(os.path.join('.')))\n",
    "\n",
    "#from main_model import CSDI_Forecasting\n",
    "from dataset_forecasting import get_dataloader\n",
    "from utils import train, evaluate\n",
    "from diff_models import diff_CSDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "\n",
    "Run this cell to download the dataset if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists in data. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "def download_data(folder_url, output_path='data/electricity_nips'):\n",
    "    \"\"\"\n",
    "    Download all files from a Google Drive folder and save them to the specified output path.\n",
    "    Checks if data already exists in the output directory before downloading.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Check if the directory is not empty, implying data might already be downloaded\n",
    "    if os.listdir(output_path):\n",
    "        print(f\"Data already exists in {output_path}. Skipping download.\")\n",
    "        return\n",
    "    \n",
    "    # Extract the folder ID from the URL\n",
    "    folder_id = folder_url.split('/')[-1]\n",
    "    \n",
    "    # List files in the folder\n",
    "    url = f\"https://drive.google.com/drive/folders/{folder_id}\"\n",
    "    output = gdown.download_folder(url, output=output_path, quiet=False, use_cookies=False)\n",
    "    \n",
    "    if output is None:\n",
    "        print(\"Failed to download files. Make sure the folder is publicly accessible.\")\n",
    "    else:\n",
    "        print(f\"Downloaded files to {output_path}\")\n",
    "\n",
    "# Example usage with a specified folder URL\n",
    "folder_url = \"https://drive.google.com/drive/folders/1krZQofLdeQrzunuKkLXy8L_kMzQrVFI_\"\n",
    "download_data(folder_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSDI_base(nn.Module):\n",
    "    \"\"\"\n",
    "    The `CSDI_base` class is a PyTorch module that facilitates Conditional Score-based Diffusion Modeling for \n",
    "    time series.\n",
    "\n",
    "    Key Components and Initializations:\n",
    "\n",
    "    - `__init__`: Constructor that initializes the model with:\n",
    "      - `target_dim`: The dimensionality of the target data, dictating the model's output features.\n",
    "      - `config`: Configuration settings for model parameters such as embeddings and the diffusion process.\n",
    "      - `device`: Computation device (CPU or GPU) for tensor operations, ensuring optimal performance.\n",
    "\n",
    "    Configuration and Setup:\n",
    "\n",
    "    1. Embedding Dimensions:\n",
    "       - `emb_time_dim` and `emb_feature_dim`: Define embedding sizes for time points and features, \n",
    "          transforming input data for the network.\n",
    "       - `is_unconditional`: Adjusts embedding dimensions based on whether the model is conditional, \n",
    "          integrating observed data when necessary.\n",
    "\n",
    "    2. Network Layers:\n",
    "       - `embed_layer`: A PyTorch embedding layer that maps features to dense vectors, enhancing data pattern recognition.\n",
    "\n",
    "    3. Diffusion Model Configuration:\n",
    "       - Extracts diffusion-related settings from `config`, like the number of diffusion steps and the beta schedule, \n",
    "         which dictate the noise addition process.\n",
    "\n",
    "    4. Diffusion Parameters:\n",
    "       - `beta`, `alpha_hat`, `alpha`: Arrays that manage the noise levels and data transformation during the diffusion process.\n",
    "       - `alpha_torch`: Converts the `alpha` array to a PyTorch tensor formatted for use in the model's operations and \n",
    "          placed on the designated device.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_dim, config, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.target_dim = target_dim\n",
    "\n",
    "        self.emb_time_dim = config[\"model\"][\"timeemb\"]\n",
    "        self.emb_feature_dim = config[\"model\"][\"featureemb\"]\n",
    "        self.is_unconditional = config[\"model\"][\"is_unconditional\"]\n",
    "        self.target_strategy = config[\"model\"][\"target_strategy\"]\n",
    "\n",
    "        self.emb_total_dim = self.emb_time_dim + self.emb_feature_dim\n",
    "        if self.is_unconditional == False:\n",
    "            self.emb_total_dim += 1  # for conditional mask\n",
    "        self.embed_layer = nn.Embedding(\n",
    "            num_embeddings=self.target_dim, embedding_dim=self.emb_feature_dim\n",
    "        )\n",
    "\n",
    "        config_diff = config[\"diffusion\"]\n",
    "        config_diff[\"side_dim\"] = self.emb_total_dim\n",
    "\n",
    "        input_dim = 1 if self.is_unconditional == True else 2\n",
    "        self.diffmodel = diff_CSDI(config_diff, input_dim)\n",
    "\n",
    "        # parameters for diffusion models\n",
    "        self.num_steps = config_diff[\"num_steps\"]\n",
    "        if config_diff[\"schedule\"] == \"quad\":\n",
    "            self.beta = np.linspace(\n",
    "                config_diff[\"beta_start\"] ** 0.5, config_diff[\"beta_end\"] ** 0.5, self.num_steps\n",
    "            ) ** 2\n",
    "        elif config_diff[\"schedule\"] == \"linear\":\n",
    "            self.beta = np.linspace(\n",
    "                config_diff[\"beta_start\"], config_diff[\"beta_end\"], self.num_steps\n",
    "            )\n",
    "\n",
    "        self.alpha_hat = 1 - self.beta\n",
    "        self.alpha = np.cumprod(self.alpha_hat)\n",
    "        self.alpha_torch = torch.tensor(self.alpha).float().to(self.device).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "    \"\"\"\n",
    "    def time_embedding\n",
    "       - Constructs a time-based embedding for input positions using sinusoidal functions, commonly used in models like transformers.\n",
    "       - `pos`: Tensor of positions for which embeddings are generated.\n",
    "       - `d_model`: Dimensionality of the embedding; defaults to 128.\n",
    "       - The function uses sine and cosine functions to embed time positions, which helps in capturing periodic patterns.\n",
    "    \"\"\"\n",
    "    def time_embedding(self, pos, d_model=128):\n",
    "        pe = torch.zeros(pos.shape[0], pos.shape[1], d_model).to(self.device)\n",
    "        position = pos.unsqueeze(2)\n",
    "        div_term = 1 / torch.pow(\n",
    "            10000.0, torch.arange(0, d_model, 2).to(self.device) / d_model\n",
    "        )\n",
    "        pe[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    \"\"\"\n",
    "    def get_randmask\n",
    "       - Generates a random mask for observed data points to simulate missingness during training, \n",
    "         enhancing the model's robustness.\n",
    "       - `observed_mask`: A tensor indicating which data points are observed.\n",
    "       - This function randomly masks observed data points based on their existing observed status, \n",
    "         useful in self-supervised setups where true missing patterns need to be approximated.\n",
    "    \"\"\"\n",
    "    def get_randmask(self, observed_mask):\n",
    "        rand_for_mask = torch.rand_like(observed_mask) * observed_mask\n",
    "        rand_for_mask = rand_for_mask.reshape(len(rand_for_mask), -1)\n",
    "        for i in range(len(observed_mask)):\n",
    "            sample_ratio = np.random.rand()  # missing ratio\n",
    "            num_observed = observed_mask[i].sum().item()\n",
    "            num_masked = round(num_observed * sample_ratio)\n",
    "            rand_for_mask[i][rand_for_mask[i].topk(num_masked).indices] = -1\n",
    "        cond_mask = (rand_for_mask > 0).reshape(observed_mask.shape).float()\n",
    "        return cond_mask\n",
    "\n",
    "    \"\"\"\n",
    "    def get_hist_mask\n",
    "       - Generates a historical mask based on the observed data and possibly an external pattern.\n",
    "       - `observed_mask`: A tensor indicating observed data points.\n",
    "       - `for_pattern_mask`: An optional tensor to provide historical patterns for masking, defaults to the observed_mask.\n",
    "       - Depending on the `target_strategy`, it combines random masking and historical patterns to simulate different training conditions.\n",
    "\n",
    "    \"\"\"\n",
    "    def get_hist_mask(self, observed_mask, for_pattern_mask=None):\n",
    "        if for_pattern_mask is None:\n",
    "            for_pattern_mask = observed_mask\n",
    "        if self.target_strategy == \"mix\":\n",
    "            rand_mask = self.get_randmask(observed_mask)\n",
    "\n",
    "        cond_mask = observed_mask.clone()\n",
    "        for i in range(len(cond_mask)):\n",
    "            mask_choice = np.random.rand()\n",
    "            if self.target_strategy == \"mix\" and mask_choice > 0.5:\n",
    "                cond_mask[i] = rand_mask[i]\n",
    "            else:  # draw another sample for histmask (i-1 corresponds to another sample)\n",
    "                cond_mask[i] = cond_mask[i] * for_pattern_mask[i - 1] \n",
    "        return cond_mask\n",
    "    \n",
    "    \"\"\"\n",
    "    def get_test_pattern_mask\n",
    "       - Produces a test pattern mask that is the element-wise product of observed data points and a specified test pattern.\n",
    "       - `observed_mask`: A tensor of observed data points.\n",
    "       - `test_pattern_mask`: A predefined tensor describing specific test conditions or missing data patterns.\n",
    "       - This function is useful for simulating specific scenarios during model evaluation or testing.\n",
    "    \"\"\"\n",
    "    def get_test_pattern_mask(self, observed_mask, test_pattern_mask):\n",
    "        return observed_mask * test_pattern_mask\n",
    "\n",
    "    \"\"\"\n",
    "    def get_side_info\n",
    "       - Constructs side information by combining time and feature embeddings with an optional conditional mask.\n",
    "       - `observed_tp`: Tensor of observed time points.\n",
    "       - `cond_mask`: A condition mask indicating available data points.\n",
    "       - This function enriches the model input with necessary contextual information, adjusting for the presence of observed data \n",
    "         and enhancing the model's ability to interpret and impute based on time and feature dependencies.\n",
    "    \"\"\"\n",
    "    def get_side_info(self, observed_tp, cond_mask):\n",
    "        B, K, L = cond_mask.shape\n",
    "\n",
    "        time_embed = self.time_embedding(observed_tp, self.emb_time_dim)  # (B,L,emb)\n",
    "        time_embed = time_embed.unsqueeze(2).expand(-1, -1, K, -1)\n",
    "        feature_embed = self.embed_layer(\n",
    "            torch.arange(self.target_dim).to(self.device)\n",
    "        )  # (K,emb)\n",
    "        feature_embed = feature_embed.unsqueeze(0).unsqueeze(0).expand(B, L, -1, -1)\n",
    "\n",
    "        side_info = torch.cat([time_embed, feature_embed], dim=-1)  # (B,L,K,*)\n",
    "        side_info = side_info.permute(0, 3, 2, 1)  # (B,*,K,L)\n",
    "\n",
    "        if self.is_unconditional == False:\n",
    "            side_mask = cond_mask.unsqueeze(1)  # (B,1,K,L)\n",
    "            side_info = torch.cat([side_info, side_mask], dim=1)\n",
    "\n",
    "        return side_info\n",
    "\n",
    "    \"\"\"\n",
    "    def calc_loss_valid\n",
    "       - Computes the validation loss by aggregating losses over all diffusion steps.\n",
    "       - `observed_data`: Tensor containing the observed data points.\n",
    "       - `cond_mask`, `observed_mask`: Masks indicating conditions and observed data points.\n",
    "       - `side_info`: Additional contextual information provided to the diffusion model.\n",
    "       - `is_train`: A flag indicating whether the function is being called in training mode.\n",
    "       - Iterates over all diffusion steps, accumulating loss calculated at each step and averages it.\n",
    "    \"\"\"\n",
    "    def calc_loss_valid(\n",
    "        self, observed_data, cond_mask, observed_mask, side_info, is_train\n",
    "    ):\n",
    "        loss_sum = 0\n",
    "        for t in range(self.num_steps):  # calculate loss for all t\n",
    "            loss = self.calc_loss(\n",
    "                observed_data, cond_mask, observed_mask, side_info, is_train, set_t=t\n",
    "            )\n",
    "            loss_sum += loss.detach()\n",
    "        return loss_sum / self.num_steps\n",
    "\n",
    "    \"\"\"\n",
    "    def calc_loss\n",
    "       - Calculates loss for a specific step in the diffusion process during training or validation.\n",
    "       - `is_train`: Flag to distinguish between training and validation modes.\n",
    "       - `set_t`: Specific time step at which to calculate the loss; if not in training mode, the step is predetermined.\n",
    "       - Determines the diffusion time step randomly during training or uses the provided step during validation.\n",
    "       - Applies the diffusion model to compute predictions for noisy data, compares these predictions against actual noise added, \n",
    "         and computes the loss based on differences weighted by the target mask (areas not covered by the condition mask).\n",
    "    \"\"\"\n",
    "    def calc_loss(\n",
    "        self, observed_data, cond_mask, observed_mask, side_info, is_train, set_t=-1\n",
    "    ):\n",
    "        B, K, L = observed_data.shape\n",
    "        if is_train != 1:  # for validation\n",
    "            t = (torch.ones(B) * set_t).long().to(self.device)\n",
    "        else:\n",
    "            t = torch.randint(0, self.num_steps, [B]).to(self.device)\n",
    "        current_alpha = self.alpha_torch[t]  # (B,1,1)\n",
    "        noise = torch.randn_like(observed_data)\n",
    "        noisy_data = (current_alpha ** 0.5) * observed_data + (1.0 - current_alpha) ** 0.5 * noise\n",
    "\n",
    "        total_input = self.set_input_to_diffmodel(noisy_data, observed_data, cond_mask)\n",
    "\n",
    "        predicted = self.diffmodel(total_input, side_info, t)  # (B,K,L)\n",
    "\n",
    "        target_mask = observed_mask - cond_mask\n",
    "        residual = (noise - predicted) * target_mask\n",
    "        num_eval = target_mask.sum()\n",
    "        loss = (residual ** 2).sum() / (num_eval if num_eval > 0 else 1)\n",
    "        return loss\n",
    "\n",
    "    \"\"\"\n",
    "    def set_input_to_diffmodel\n",
    "       - Prepares the input tensors for the diffusion model based on the model's conditionality.\n",
    "       - `noisy_data`: Tensor containing the data perturbed with noise.\n",
    "       - `observed_data`: Original data tensor containing observed values.\n",
    "       - `cond_mask`: A mask that distinguishes between observed and unobserved data points.\n",
    "       - If the model is unconditional, it uses only the noisy data as input.\n",
    "       - For conditional models, it creates separate channels for conditioned observed data and noisy targets, then concatenates them.\n",
    "    \"\"\"\n",
    "    def set_input_to_diffmodel(self, noisy_data, observed_data, cond_mask):\n",
    "        if self.is_unconditional == True:\n",
    "            total_input = noisy_data.unsqueeze(1)  # (B,1,K,L)\n",
    "        else:\n",
    "            cond_obs = (cond_mask * observed_data).unsqueeze(1)\n",
    "            noisy_target = ((1 - cond_mask) * noisy_data).unsqueeze(1)\n",
    "            total_input = torch.cat([cond_obs, noisy_target], dim=1)  # (B,2,K,L)\n",
    "\n",
    "        return total_input\n",
    "\n",
    "    \"\"\"\n",
    "    def impute\n",
    "       - Conducts the imputation of missing values based on observed data, condition masks, and side information.\n",
    "       - `observed_data`: The observed portion of the data.\n",
    "       - `cond_mask`: Mask indicating observed (1) and missing (0) parts of the data.\n",
    "       - `side_info`: Contextual information that aids the model in imputation.\n",
    "       - `n_samples`: Number of imputation samples to generate.\n",
    "       - Initializes storage for the imputed samples and iteratively generates them using a diffusion process.\n",
    "       - For unconditional models, creates noisy versions of the observed data for each diffusion step.\n",
    "       - For conditional models, combines observed data and generated samples in a structured input for the diffusion model.\n",
    "       - Utilizes backward diffusion (from the last step to the first) to progressively refine each imputed sample, adjusting with noise and model predictions.\n",
    "       - Returns a tensor of imputed samples across all specified instances.\n",
    "    \"\"\"\n",
    "    def impute(self, observed_data, cond_mask, side_info, n_samples):\n",
    "        B, K, L = observed_data.shape\n",
    "\n",
    "        imputed_samples = torch.zeros(B, n_samples, K, L).to(self.device)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            # generate noisy observation for unconditional model\n",
    "            if self.is_unconditional == True:\n",
    "                noisy_obs = observed_data\n",
    "                noisy_cond_history = []\n",
    "                for t in range(self.num_steps):\n",
    "                    noise = torch.randn_like(noisy_obs)\n",
    "                    noisy_obs = (self.alpha_hat[t] ** 0.5) * noisy_obs + self.beta[t] ** 0.5 * noise\n",
    "                    noisy_cond_history.append(noisy_obs * cond_mask)\n",
    "\n",
    "            current_sample = torch.randn_like(observed_data)\n",
    "\n",
    "            for t in range(self.num_steps - 1, -1, -1):\n",
    "                if self.is_unconditional == True:\n",
    "                    diff_input = cond_mask * noisy_cond_history[t] + (1.0 - cond_mask) * current_sample\n",
    "                    diff_input = diff_input.unsqueeze(1)  # (B,1,K,L)\n",
    "                else:\n",
    "                    cond_obs = (cond_mask * observed_data).unsqueeze(1)\n",
    "                    noisy_target = ((1 - cond_mask) * current_sample).unsqueeze(1)\n",
    "                    diff_input = torch.cat([cond_obs, noisy_target], dim=1)  # (B,2,K,L)\n",
    "                predicted = self.diffmodel(diff_input, side_info, torch.tensor([t]).to(self.device))\n",
    "\n",
    "                coeff1 = 1 / self.alpha_hat[t] ** 0.5\n",
    "                coeff2 = (1 - self.alpha_hat[t]) / (1 - self.alpha[t]) ** 0.5\n",
    "                current_sample = coeff1 * (current_sample - coeff2 * predicted)\n",
    "\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(current_sample)\n",
    "                    sigma = (\n",
    "                        (1.0 - self.alpha[t - 1]) / (1.0 - self.alpha[t]) * self.beta[t]\n",
    "                    ) ** 0.5\n",
    "                    current_sample += sigma * noise\n",
    "\n",
    "            imputed_samples[:, i] = current_sample.detach()\n",
    "        return imputed_samples\n",
    "\n",
    "    \"\"\"\n",
    "    def forward\n",
    "       - Defines the forward pass of the model, integrating preprocessing, conditioning, and loss calculation.\n",
    "       - `batch`: Input batch containing data and various masks.\n",
    "       - `is_train`: Flag to indicate if the model is in training (1) or validation (0) mode.\n",
    "       - Extracts observed data, masks, and time points from the batch.\n",
    "       - Depending on the training mode and target strategy, determines the conditional mask for the data.\n",
    "       - Computes side information based on observed time points and the conditional mask.\n",
    "       - Selects the appropriate loss function based on the training mode.\n",
    "       - Calculates and returns the loss, facilitating both training and validation processes.\n",
    "    \"\"\"\n",
    "    def forward(self, batch, is_train=1):\n",
    "        (\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            observed_tp,\n",
    "            gt_mask,\n",
    "            for_pattern_mask,\n",
    "            _,\n",
    "        ) = self.process_data(batch)\n",
    "        if is_train == 0:\n",
    "            cond_mask = gt_mask\n",
    "        elif self.target_strategy != \"random\":\n",
    "            cond_mask = self.get_hist_mask(\n",
    "                observed_mask, for_pattern_mask=for_pattern_mask\n",
    "            )\n",
    "        else:\n",
    "            cond_mask = self.get_randmask(observed_mask)\n",
    "\n",
    "        side_info = self.get_side_info(observed_tp, cond_mask)\n",
    "\n",
    "        loss_func = self.calc_loss if is_train == 1 else self.calc_loss_valid\n",
    "\n",
    "        return loss_func(observed_data, cond_mask, observed_mask, side_info, is_train)\n",
    "\n",
    "    \"\"\"\n",
    "    def evaluate\n",
    "       - Conducts evaluation of the model by generating imputed samples and preparing metrics.\n",
    "       - `batch`: Input batch containing observed data and masks.\n",
    "       - `n_samples`: Number of imputation samples to generate for each point.\n",
    "       - Processes the batch to extract data and masks, including the length for each sequence to avoid redundancy.\n",
    "       - Sets up a non-gradient context for evaluation to prevent backpropagation and save computation.\n",
    "       - Uses ground truth masks to determine conditional and target masks.\n",
    "       - Retrieves side information based on observed time points and conditional masks.\n",
    "       - Generates multiple imputed data samples.\n",
    "       - Adjusts target masks to avoid evaluation on predefined cut lengths in data.\n",
    "       - Returns the generated samples along with the observed data and masks for further assessment.\n",
    "    \"\"\"\n",
    "    def evaluate(self, batch, n_samples):\n",
    "        (\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            observed_tp,\n",
    "            gt_mask,\n",
    "            _,\n",
    "            cut_length,\n",
    "        ) = self.process_data(batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cond_mask = gt_mask\n",
    "            target_mask = observed_mask - cond_mask\n",
    "\n",
    "            side_info = self.get_side_info(observed_tp, cond_mask)\n",
    "\n",
    "            samples = self.impute(observed_data, cond_mask, side_info, n_samples)\n",
    "\n",
    "            for i in range(len(cut_length)):  # to avoid double evaluation\n",
    "                target_mask[i, ..., 0 : cut_length[i].item()] = 0\n",
    "        return samples, observed_data, target_mask, observed_mask, observed_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSDI_Forecasting(CSDI_base):\n",
    "    \"\"\"\n",
    "     class CSDI_Forecasting Initializes the forecasting model derived from the CSDI_base class:\n",
    "       - Sets up the model with specific configurations and assigns it to a computational device.\n",
    "       - Initializes key attributes including the dimensionality of the target data and the number of sampling features based on the model configuration.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, device, target_dim):\n",
    "        super(CSDI_Forecasting, self).__init__(target_dim, config, device)\n",
    "        self.target_dim_base = target_dim\n",
    "        self.num_sample_features = config[\"model\"][\"num_sample_features\"]\n",
    "\n",
    "    \"\"\"\n",
    "    def process_data\n",
    "    - Processes the input batch for the model, formatting and preparing data for subsequent operations.\n",
    "    - `batch`: The input batch containing observed data, masks, and timepoints.\n",
    "    - Transfers all relevant data to the computation device and converts it to floating-point format.\n",
    "    - Reorders dimensions of observed data and masks to match the expected input format of the model.\n",
    "    - Initializes a tensor for cut lengths to dynamically manage sequence processing.\n",
    "    - Generates a tensor of feature identifiers for subsequent data handling processes.\n",
    "    \"\"\"\n",
    "    def process_data(self, batch):\n",
    "        observed_data = batch[\"observed_data\"].to(self.device).float()\n",
    "        observed_mask = batch[\"observed_mask\"].to(self.device).float()\n",
    "        observed_tp = batch[\"timepoints\"].to(self.device).float()\n",
    "        gt_mask = batch[\"gt_mask\"].to(self.device).float()\n",
    "\n",
    "        observed_data = observed_data.permute(0, 2, 1)\n",
    "        observed_mask = observed_mask.permute(0, 2, 1)\n",
    "        gt_mask = gt_mask.permute(0, 2, 1)\n",
    "\n",
    "        cut_length = torch.zeros(len(observed_data)).long().to(self.device)\n",
    "        for_pattern_mask = observed_mask\n",
    "\n",
    "        feature_id=torch.arange(self.target_dim_base).unsqueeze(0).expand(observed_data.shape[0],-1).to(self.device)\n",
    "\n",
    "        return (\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            observed_tp,\n",
    "            gt_mask,\n",
    "            for_pattern_mask,\n",
    "            cut_length,\n",
    "            feature_id, \n",
    "        )  \n",
    "          \n",
    "    \"\"\"\n",
    "    def sample_features\n",
    "    - Randomly samples a subset of features from the observed data for model processing.\n",
    "    - `observed_data`: Tensor containing the observed data points.\n",
    "    - `observed_mask`: Tensor indicating the presence of observed data points.\n",
    "    - `feature_id`: Tensor of feature identifiers corresponding to observed data.\n",
    "    - `gt_mask`: Tensor of ground truth masks indicating valid data points for evaluation.\n",
    "    - Samples a specified number of features (defined by `num_sample_features`) from each observation in the batch.\n",
    "    - Reorganizes the data, masks, and feature identifiers based on the sampled features to prepare for model input.\n",
    "    - Returns the newly formatted data, masks, feature identifiers, and ground truth masks.\n",
    "    \"\"\"\n",
    "    def sample_features(self,observed_data, observed_mask,feature_id,gt_mask):\n",
    "        size = self.num_sample_features\n",
    "        self.target_dim = size\n",
    "        extracted_data = []\n",
    "        extracted_mask = []\n",
    "        extracted_feature_id = []\n",
    "        extracted_gt_mask = []\n",
    "        \n",
    "        for k in range(len(observed_data)):\n",
    "            ind = np.arange(self.target_dim_base)\n",
    "            np.random.shuffle(ind)\n",
    "            extracted_data.append(observed_data[k,ind[:size]])\n",
    "            extracted_mask.append(observed_mask[k,ind[:size]])\n",
    "            extracted_feature_id.append(feature_id[k,ind[:size]])\n",
    "            extracted_gt_mask.append(gt_mask[k,ind[:size]])\n",
    "        extracted_data = torch.stack(extracted_data,0)\n",
    "        extracted_mask = torch.stack(extracted_mask,0)\n",
    "        extracted_feature_id = torch.stack(extracted_feature_id,0)\n",
    "        extracted_gt_mask = torch.stack(extracted_gt_mask,0)\n",
    "        return extracted_data, extracted_mask,extracted_feature_id, extracted_gt_mask\n",
    "\n",
    "    \"\"\"\n",
    "    def get_side_info\n",
    "    - Generates side information combining time embeddings and feature embeddings based on provided masks.\n",
    "    - `observed_tp`: Tensor containing the time points of observations.\n",
    "    - `cond_mask`: Conditional mask tensor that specifies which data points are used for conditioning.\n",
    "    - `feature_id`: Optional tensor of feature identifiers; used if model's target dimension differs from the base.\n",
    "    - Constructs time embeddings for each time point and expands these across the target dimensions.\n",
    "    - Depending on the model's configuration, either generates a static feature embedding for all features or specific embeddings based on the provided `feature_id`.\n",
    "    - Concatenates time and feature embeddings to form a comprehensive side information tensor.\n",
    "    - Reorders dimensions to match the expected input structure for further model processes.\n",
    "    - If the model conditions on the input (is not unconditional), includes the conditional mask in the side information.\n",
    "    - Returns the compiled side information for use in the model’s prediction or forecasting tasks.\n",
    "    \"\"\"\n",
    "    def get_side_info(self, observed_tp, cond_mask,feature_id=None):\n",
    "        B, K, L = cond_mask.shape\n",
    "\n",
    "        time_embed = self.time_embedding(observed_tp, self.emb_time_dim)  # (B,L,emb)\n",
    "        time_embed = time_embed.unsqueeze(2).expand(-1, -1, self.target_dim, -1)\n",
    "\n",
    "        if self.target_dim == self.target_dim_base:\n",
    "            feature_embed = self.embed_layer(\n",
    "                torch.arange(self.target_dim).to(self.device)\n",
    "            )  # (K,emb)\n",
    "            feature_embed = feature_embed.unsqueeze(0).unsqueeze(0).expand(B, L, -1, -1)\n",
    "        else:\n",
    "            feature_embed = self.embed_layer(feature_id).unsqueeze(1).expand(-1,L,-1,-1)\n",
    "        side_info = torch.cat([time_embed, feature_embed], dim=-1)  # (B,L,K,*)\n",
    "        side_info = side_info.permute(0, 3, 2, 1)  # (B,*,K,L)\n",
    "\n",
    "        if self.is_unconditional == False:\n",
    "            side_mask = cond_mask.unsqueeze(1)  # (B,1,K,L)\n",
    "            side_info = torch.cat([side_info, side_mask], dim=1)\n",
    "\n",
    "        return side_info\n",
    "\n",
    "    \"\"\"\n",
    "    def forward\n",
    "    - Defines the forward pass for the model, processing input data and executing training or evaluation steps.\n",
    "    - `batch`: Input batch containing observed data, masks, and additional information.\n",
    "    - `is_train`: Indicator of whether the model is in training mode (1) or evaluation mode (0).\n",
    "    - Processes the input batch to format and prepare data structures for model operations.\n",
    "    - Conditionally samples a subset of features from the observed data if in training mode and feature sampling is necessary due to dimensionality constraints.\n",
    "    - Sets the target dimensionality based on the model's base or sampled features.\n",
    "    - Determines the conditioning mask based on the mode of operation, using ground truth masks for evaluation or generating test patterns for training.\n",
    "    - Retrieves side information incorporating time and feature embeddings tailored to the conditioning context.\n",
    "    - Selects the appropriate loss calculation function based on the training or evaluation context.\n",
    "    - Computes and returns the loss by comparing model predictions with actual observations and side information, adjusted for the specified training or evaluation mode.\n",
    "    \"\"\"\n",
    "    def forward(self, batch, is_train=1):\n",
    "        (\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            observed_tp,\n",
    "            gt_mask,\n",
    "            _,\n",
    "            _,\n",
    "            feature_id, \n",
    "        ) = self.process_data(batch)\n",
    "        if is_train == 1 and (self.target_dim_base > self.num_sample_features):\n",
    "            observed_data, observed_mask,feature_id,gt_mask = \\\n",
    "                    self.sample_features(observed_data, observed_mask,feature_id,gt_mask)\n",
    "        else:\n",
    "            self.target_dim = self.target_dim_base\n",
    "            feature_id = None\n",
    "\n",
    "        if is_train == 0:\n",
    "            cond_mask = gt_mask\n",
    "        else: #test pattern\n",
    "            cond_mask = self.get_test_pattern_mask(\n",
    "                observed_mask, gt_mask\n",
    "            )\n",
    "\n",
    "        side_info = self.get_side_info(observed_tp, cond_mask, feature_id)\n",
    "\n",
    "        loss_func = self.calc_loss if is_train == 1 else self.calc_loss_valid\n",
    "\n",
    "        return loss_func(observed_data, cond_mask, observed_mask, side_info, is_train)\n",
    "\n",
    "    \"\"\"\n",
    "    def evaluate\n",
    "    - Conducts evaluation of the model by generating imputed samples and preparing metrics.\n",
    "    - `batch`: Input batch containing observed data and masks.\n",
    "    - `n_samples`: Number of imputation samples to generate for each point.\n",
    "    - Processes the batch to extract data and masks, including the length for each sequence to avoid redundancy.\n",
    "    - Sets up a non-gradient context for evaluation to prevent backpropagation and save computation.\n",
    "    - Uses ground truth masks to determine conditional and target masks.\n",
    "    - Retrieves side information based on observed time points and conditional masks.\n",
    "    - Generates multiple imputed data samples.\n",
    "    - Returns the generated samples along with the observed data and masks for further assessment.\n",
    "    \"\"\"\n",
    "    def evaluate(self, batch, n_samples):\n",
    "        (\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            observed_tp,\n",
    "            gt_mask,\n",
    "            _,\n",
    "            _,\n",
    "            feature_id, \n",
    "        ) = self.process_data(batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cond_mask = gt_mask\n",
    "            target_mask = observed_mask * (1-gt_mask)\n",
    "\n",
    "            side_info = self.get_side_info(observed_tp, cond_mask)\n",
    "\n",
    "            samples = self.impute(observed_data, cond_mask, side_info, n_samples)\n",
    "\n",
    "        return samples, observed_data, target_mask, observed_mask, observed_tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup and Execution\n",
    "\n",
    "This segment highlights the setup and execution of utilizing CSDI for time series forecasting of electricity data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Argument Parsing\n",
    "The experiment's settings are loaded from a YAML configuration file, allowing easy adjustments to the model and training parameters. Modifications to these settings via command line arguments are directly reflected in the configuration, ensuring that each experiment can be finely tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"train\": {\n",
      "        \"epochs\": 100,\n",
      "        \"batch_size\": 8,\n",
      "        \"lr\": 0.001,\n",
      "        \"itr_per_epoch\": 100000000.0\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"layers\": 4,\n",
      "        \"channels\": 64,\n",
      "        \"nheads\": 8,\n",
      "        \"diffusion_embedding_dim\": 128,\n",
      "        \"beta_start\": 0.0001,\n",
      "        \"beta_end\": 0.5,\n",
      "        \"num_steps\": 50,\n",
      "        \"schedule\": \"quad\",\n",
      "        \"is_linear\": true\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"is_unconditional\": 0,\n",
      "        \"timeemb\": 128,\n",
      "        \"featureemb\": 16,\n",
      "        \"target_strategy\": \"test\",\n",
      "        \"num_sample_features\": 64\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"{\n",
    "    \"train\": {\n",
    "        \"epochs\": 100,  # Total number of training cycles through the entire dataset.\n",
    "        \"batch_size\": 8,  # Number of data samples processed before the model's internal parameters are updated.\n",
    "        \"lr\": 0.001,  # Learning rate, determines the step size at each iteration while moving toward a minimum of the loss function.\n",
    "        \"itr_per_epoch\": 100000000.0  # Presumably meant to be iterations per epoch.\n",
    "    },\n",
    "    \"diffusion\": {\n",
    "        \"layers\": 4,  # Number of layers in the diffusion model, affects depth and complexity.\n",
    "        \"channels\": 64,  # Number of channels in each layer, influences the model's capacity to process information.\n",
    "        \"nheads\": 8,  # Number of attention heads in transformer-based models, affects the model's ability to focus on different parts of the input sequence.\n",
    "        \"diffusion_embedding_dim\": 128,  # Dimension of the embeddings used in the diffusion process, impacts the representational power.\n",
    "        \"beta_start\": 0.0001,  # Initial value of the noise schedule, dictates how much noise starts the diffusion process.\n",
    "        \"beta_end\": 0.5,  # Final value in the noise schedule, controls how much noise is removed by the end of the diffusion process.\n",
    "        \"num_steps\": 50,  # Total number of steps in the diffusion process from start to completion.\n",
    "        \"schedule\": \"quad\",  # Type of scheduling for the beta values, 'quad' implies a quadratic progression.\n",
    "        \"is_linear\": True  # Indicates whether the scheduling progression is linear, set to true which conflicts with 'quad' indicating a possible oversight or specific implementation.\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"is_unconditional\": 0,  # Specifies whether the model is unconditional (0 indicates conditional).\n",
    "        \"timeemb\": 128,  # Dimension of the time embeddings, used in models that incorporate timing information in their predictions.\n",
    "        \"featureemb\": 16,  # Dimension of the feature embeddings, provides additional contextual information per feature.\n",
    "        \"target_strategy\": \"test\",  # Strategy for targeting in training/testing, 'test' might indicate a specific approach or mode used during evaluation.\n",
    "        \"num_sample_features\": 64  # Number of features to sample, relevant in scenarios like feature ablation or when working with high-dimensional data.\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def load_config(config_path='config/base_forecasting.yaml'):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "config = load_config()\n",
    "print(json.dumps(config, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Configuration\n",
    "First, we determine the appropriate computation environment. If a GPU is available, the model will utilize it for faster computation; otherwise, it defaults to using the CPU. This ensures that the setup is optimized for performance regardless of the hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "Data loaders are set up for the training, validation, and testing phases. These loaders are crucial for managing the data flow during model training and evaluation, ensuring efficient handling of data batches and the appropriate application of missing data simulations as specified in the experimental setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/electricity_nips/data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m datatype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melectricity\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m target_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m370\u001b[39m  \u001b[38;5;66;03m# for electricity dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_loader, valid_loader, test_loader, scaler, mean_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/gsharma/diffusion_model_bootcamp/reference_implementations/time_series_reference_impelementation/CSDI/dataset_forecasting.py:75\u001b[0m, in \u001b[0;36mget_dataloader\u001b[0;34m(datatype, device, batch_size)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataloader\u001b[39m(datatype, device, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m---> 75\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mForecasting_Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     77\u001b[0m     valid_dataset \u001b[38;5;241m=\u001b[39m Forecasting_Dataset(datatype, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/fs01/home/gsharma/diffusion_model_bootcamp/reference_implementations/time_series_reference_impelementation/CSDI/dataset_forecasting.py:22\u001b[0m, in \u001b[0;36mForecasting_Dataset.__init__\u001b[0;34m(self, datatype, mode)\u001b[0m\n\u001b[1;32m     19\u001b[0m paths \u001b[38;5;241m=\u001b[39m datafolder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# shape: (T x N)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# mask_data is usually filled by 1\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     24\u001b[0m paths \u001b[38;5;241m=\u001b[39m datafolder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/meanstd.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/electricity_nips/data.pkl'"
     ]
    }
   ],
   "source": [
    "# Set up dataloaders\n",
    "datatype = 'electricity'\n",
    "target_dim = 370  # for electricity dataset\n",
    "\n",
    "train_loader, valid_loader, test_loader, scaler, mean_scaler = get_dataloader(\n",
    "    datatype=datatype,\n",
    "    device=device,\n",
    "    batch_size=config['train']['batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization\n",
    "The model, `CSDI_Forecasting`, is initialized based on predefined configurations. This specialized model is designed to handle electricity time series data, inheriting robust functionalities from its base class to effectively manage the specific requirements of time series forecasting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model = CSDI_Forecasting(config, device, target_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Output Folder Setup\n",
    "Before training the model, an output directory is created to store training artifacts such as model checkpoints, logs, and output files. The directory name includes a timestamp to ensure uniqueness and to help track experiments based on the date and time they were performed.\n",
    "\n",
    "- **Directory Naming**: The folder is named using the current date and time, which helps in organizing and retrieving model training sessions based on when they were conducted.\n",
    "- **Creation**: The directory is created on the file system, ensuring it exists before any training outputs are written to it. This prevents errors related to file writing during model training.\n",
    "\n",
    "### Model Training Process\n",
    "The model is trained using the specified configurations, data loaders, and the path to the output directory. The training function is designed to handle both the training and validation phases within each epoch, allowing for a comprehensive assessment of model performance over time.\n",
    "\n",
    "- **Training Function**: Takes the model, training configurations, and data loaders as inputs. Additionally, it accepts the path to the output folder where the training results are stored.\n",
    "- **Validation Data**: Optionally, a validation loader can be passed to periodically evaluate model performance on a separate validation set during the training process.\n",
    "\n",
    "### Execution\n",
    "Upon execution, the training process iteratively updates the model weights based on the loss computed from the training data. It also evaluates the model on the validation set, if provided, to monitor its performance on unseen data. Results and model states are saved in the designated output directory, facilitating post-training evaluations and model deployment.\n",
    "\n",
    "### Loss Function\n",
    "The loss function employed in the CSDI model is designed to optimize the model's ability to denoise data:\n",
    "- **Denoising Loss**: During training, the model calculates the loss as the squared difference between the actual noise added to the data in the forward process and the noise predicted by the model during the reverse diffusion process. This loss function is key to training the model to accurately reverse the noise addition, effectively reconstructing the original data from its noisy version.\n",
    "\n",
    "### Metrics\n",
    "1. **Mean Absolute Error (MAE)**: This metric measures the average magnitude of errors in a set of predictions, without considering their direction. It's a linear score that averages the absolute differences between predicted and actual values, providing a straightforward interpretation of prediction accuracy.\n",
    "2. **Continuous Ranked Probability Score (CRPS)**: CRPS is used to assess the accuracy of probabilistic predictions. It measures the difference between the predicted cumulative distribution function and the empirical distribution function of the observed data. This score is particularly useful for evaluating the performance of models that generate probabilistic or distributional forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output folder\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "foldername = f\"./save/forecasting_{datatype}_{current_time}/\"\n",
    "os.makedirs(foldername, exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "with open(foldername + \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▌                                                          | 18/691 [00:55<34:45,  3.10s/it, avg_epoch_loss=0.658, epoch=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfoldername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfoldername\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/gsharma/diffusion_model_bootcamp/reference_implementations/time_series_reference_impelementation/CSDI/utils.py:35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, config, train_loader, valid_loader, valid_epoch_interval, foldername)\u001b[0m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(train_batch)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(\n",
    "    model,\n",
    "    config['train'],\n",
    "    train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    foldername=foldername\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Pretrained Model\n",
    "\n",
    "### Function for Loading Model State\n",
    "To enhance or expedite the training process, or for evaluation purposes, you may start with a model that has already been trained. The function `load_pretrained_model` facilitates the loading of these pretrained weights into an existing model architecture.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `model`: The model instance into which the pretrained weights will be loaded.\n",
    "  - `modelfolder`: The subdirectory under `./save/` where the pretrained model is stored, defaulting to `pretrained`.\n",
    "  - `device`: The computing device (CPU or GPU) where the model will be loaded. This ensures that the model is compatible with the hardware used for subsequent operations.\n",
    "\n",
    "### Execution\n",
    "The function constructs the full path to the pretrained model's state dictionary file (`model.pth`) using the specified `modelfolder`. It then loads this state dictionary into the model, ensuring that all model parameters are updated accordingly.\n",
    "\n",
    "- **Model Compatibility**:\n",
    "  - It is crucial that the model architecture into which the weights are being loaded matches the architecture of the model when it was saved. Incompatibility in architectures will lead to errors during the loading process.\n",
    "\n",
    "### Usage\n",
    "To utilize a pretrained model, simply pass your initialized but untrained model to the `load_pretrained_model` function. This setup allows you to leverage previously learned patterns, potentially reducing training time and improving model robustness from the outset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CSDI_Forecasting:\n\tMissing key(s) in state_dict: \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.norm.bias\". \n\tUnexpected key(s) in state_dict: \"diffmodel.residual_layers.0.time_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.0.time_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.0.time_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.0.time_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.0.time_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.0.time_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.norm2.bias\". \n\tsize mismatch for embed_layer.weight: copying a param with shape torch.Size([35, 16]) from checkpoint, the shape in current model is torch.Size([370, 16]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load pre-trained model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m pretrained_model\u001b[38;5;241m.\u001b[39mtarget_dim \u001b[38;5;241m=\u001b[39m target_dim\n",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m, in \u001b[0;36mload_pretrained_model\u001b[0;34m(model, modelfolder, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_pretrained_model\u001b[39m(model, modelfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[1;32m      2\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./save/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.8/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CSDI_Forecasting:\n\tMissing key(s) in state_dict: \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.0.time_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.1.time_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.2.time_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.3.time_layer.layers.layers.0.1.norm.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.fn.to_q.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.fn.to_k.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.fn.to_v.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.fn.to_out.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.fn.to_out.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.norm.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.0.norm.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.fn.fn.w1.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.fn.fn.w1.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.fn.fn.w2.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.fn.fn.w2.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.norm.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.layers.0.1.norm.bias\". \n\tUnexpected key(s) in state_dict: \"diffmodel.residual_layers.0.time_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.0.time_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.0.time_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.0.time_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.0.time_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.0.time_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.0.time_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.0.feature_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.0.feature_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.1.time_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.1.time_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.1.feature_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.1.feature_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.2.time_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.2.time_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.2.feature_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.2.feature_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.3.time_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.3.time_layer.layers.0.norm2.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.self_attn.in_proj_weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.self_attn.in_proj_bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.self_attn.out_proj.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.self_attn.out_proj.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.linear1.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.linear1.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.linear2.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.linear2.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.norm1.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.norm1.bias\", \"diffmodel.residual_layers.3.feature_layer.layers.0.norm2.weight\", \"diffmodel.residual_layers.3.feature_layer.layers.0.norm2.bias\". \n\tsize mismatch for embed_layer.weight: copying a param with shape torch.Size([35, 16]) from checkpoint, the shape in current model is torch.Size([370, 16])."
     ]
    }
   ],
   "source": [
    "def load_pretrained_model(model, modelfolder='pretrained', device=device):\n",
    "    model_path = f\"./save/{modelfolder}/model.pth\"\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    return model\n",
    "\n",
    "# Load pre-trained model\n",
    "pretrained_model = load_pretrained_model(model)\n",
    "pretrained_model.target_dim = target_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "### Evaluation Parameters Configuration\n",
    "Prior to evaluating the models, several key parameters are established:\n",
    "- **`test_loader`**: The data loader for the test dataset.\n",
    "- **`nsample`**: Specifies the number of samples to generate during model evaluation, set to 100 for comprehensive testing.\n",
    "- **`scaler`** and **`mean_scaler`**: Define scaling factors for the data. These parameters adjust the data normalization during the evaluation to match the conditions used during model training.\n",
    "\n",
    "### Data Loader Configuration\n",
    "- The test data loader is updated to include the new missing ratio, ensuring that the evaluation tests the model's ability to handle and impute missing data effectively.\n",
    "\n",
    "### Model Evaluation\n",
    "- **Current Model Evaluation**: The initially trained model is evaluated using the updated `test_loader`. This step is crucial for understanding the baseline performance of the model on the test set.\n",
    "- **Pre-trained Model Evaluation**: Additionally, a pre-trained model is evaluated under the same conditions. This is particularly useful for comparing the effectiveness of pre-training and fine-tuning strategies on model performance.\n",
    "\n",
    "### Execution\n",
    "Both the current and pre-trained models are evaluated with the specified number of samples and scaling parameters. The results are stored in a designated folder, facilitating subsequent analysis and comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set evaluation parameters\n",
    "nsample = 100  # number of samples for evaluation\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(\n",
    "    model,\n",
    "    test_loader,\n",
    "    nsample=nsample,\n",
    "    scaler=scaler,\n",
    "    mean_scaler=mean_scaler,\n",
    "    foldername=foldername\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- Tashiro, Yusuke, et al. \"CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation.\" *Advances in Neural Information Processing Systems*. 2021. [GitHub Repository](https://github.com/ermongroup/CSDI)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
