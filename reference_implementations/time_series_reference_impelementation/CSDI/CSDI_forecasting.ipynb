{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSDI for Time Series Forecasting\n",
    "\n",
    "This notebook demonstrates the usage of the CSDI (Conditional Score-based Diffusion Models for Imputation) model for time series forecasting tasks, specifically for electricity consumption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/54/70/e07c381e6488a77094f04c85c9caf1c8008cdc30778f7019bc52e5285ef0/gdown-5.2.0-py3-none-any.whl.metadata\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (3.12.4)\n",
      "Requirement already satisfied: requests[socks] in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/abhaypuri/Desktop/pytorch_test/env/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports and Model Setup\n",
    "\n",
    "In this section, we import necessary libraries and modules required for the implementation of the CSDI model. This includes standard data handling libraries like `numpy` and `pandas`, deep learning libraries from `torch`, and specific components for building and training the CSDI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "# Add the parent directory to sys.path to import local modules\n",
    "sys.path.append(os.path.abspath(os.path.join('.')))\n",
    "\n",
    "from main_model import CSDI_Forecasting\n",
    "from dataset_forecasting import get_dataloader\n",
    "from utils import train, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download and Module Import\n",
    "\n",
    "The provided code snippet performs several tasks essential for setting up a data processing and model training environment:\n",
    "\n",
    "1. **Add Parent Directory to `sys.path`**:\n",
    "   - This allows the script to import local modules from the parent directory, ensuring all necessary components are accessible.\n",
    "\n",
    "2. **Import Required Modules**:\n",
    "   - It imports various functions and classes from local modules (`main_model`, `dataset_forecasting`, `utils`) needed for model training and evaluation.\n",
    "\n",
    "3. **Download Data Function**:\n",
    "   - A function named `download_data` is defined to download all files from a specified Google Drive folder and save them to a local directory.\n",
    "   - The function:\n",
    "     - Creates the output directory if it doesn't exist.\n",
    "     - Extracts the folder ID from the provided Google Drive URL.\n",
    "     - Downloads the files from the Google Drive folder to the specified local path.\n",
    "     - Prints a message indicating whether the download was successful or if it failed.\n",
    "\n",
    "4. **Calling the `download_data` Function**:\n",
    "   - The function is called with a specific Google Drive folder URL to initiate the download process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder 156s2fZlxFjkE1C6ndckCZ9Yoc6ZFK-h- electricity_nips\n",
      "Processing file 198PkjDgXjFKg4J8Q73ZUuH0gXF5_fFNi data.pkl\n",
      "Processing file 1F2XK0Z_4IczbU1iwE5qySCEhwIZmuBMv meanstd.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=198PkjDgXjFKg4J8Q73ZUuH0gXF5_fFNi\n",
      "To: /fs01/home/gsharma/diffusion_model_bootcamp/reference_implementations/time_series_reference_impelementation/CSDI/data/electricity_nips/electricity_nips/data.pkl\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 35.5M/35.5M [00:00<00:00, 72.4MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1F2XK0Z_4IczbU1iwE5qySCEhwIZmuBMv\n",
      "To: /fs01/home/gsharma/diffusion_model_bootcamp/reference_implementations/time_series_reference_impelementation/CSDI/data/electricity_nips/electricity_nips/meanstd.pkl\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 6.11k/6.11k [00:00<00:00, 11.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded files to data/electricity_nips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "def download_data(folder_url, output_path='data/electricity_nips'):\n",
    "    \"\"\"\n",
    "    Download all files from a Google Drive folder and save them to the specified output path.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Extract the folder ID from the URL\n",
    "    folder_id = folder_url.split('/')[-1]\n",
    "    \n",
    "    # List files in the folder\n",
    "    url = f\"https://drive.google.com/drive/folders/{folder_id}\"\n",
    "    output = gdown.download_folder(url, output=output_path, quiet=False, use_cookies=False)\n",
    "    \n",
    "    if output is None:\n",
    "        print(\"Failed to download files. Make sure the folder is publicly accessible.\")\n",
    "    else:\n",
    "        print(f\"Downloaded files to {output_path}\")\n",
    "\n",
    "folder_url = \"https://drive.google.com/drive/folders/1krZQofLdeQrzunuKkLXy8L_kMzQrVFI_\"\n",
    "download_data(folder_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration File\n",
    "\n",
    "The provided code snippet demonstrates how to load a configuration file in YAML format and convert it to a dictionary for use in a Python script:\n",
    "\n",
    "1. **Load Configuration Function**:\n",
    "   - A function named `load_config` is defined to read and parse a YAML configuration file.\n",
    "   - The function:\n",
    "     - Opens the specified YAML file (`config_path`), with a default path of `config/base_forecasting.yaml`.\n",
    "     - Uses `yaml.safe_load` to parse the file content into a Python dictionary.\n",
    "     - Returns the parsed configuration dictionary.\n",
    "\n",
    "2. **Load and Print Configuration**:\n",
    "   - The `load_config` function is called to load the configuration from the specified YAML file.\n",
    "   - The resulting configuration dictionary is printed in a readable JSON format using `json.dumps` with indentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"train\": {\n",
      "        \"epochs\": 100,\n",
      "        \"batch_size\": 8,\n",
      "        \"lr\": 0.001,\n",
      "        \"itr_per_epoch\": 100000000.0\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"layers\": 4,\n",
      "        \"channels\": 64,\n",
      "        \"nheads\": 8,\n",
      "        \"diffusion_embedding_dim\": 128,\n",
      "        \"beta_start\": 0.0001,\n",
      "        \"beta_end\": 0.5,\n",
      "        \"num_steps\": 50,\n",
      "        \"schedule\": \"quad\",\n",
      "        \"is_linear\": true\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"is_unconditional\": 0,\n",
      "        \"timeemb\": 128,\n",
      "        \"featureemb\": 16,\n",
      "        \"target_strategy\": \"test\",\n",
      "        \"num_sample_features\": 64\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def load_config(config_path='config/base_forecasting.yaml'):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "config = load_config()\n",
    "print(json.dumps(config, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup and Initialization\n",
    "\n",
    "The provided code snippet outlines the steps to set up and initialize the model, data loaders, and configuration settings for a forecasting task:\n",
    "\n",
    "1. **Set Device**:\n",
    "   - Determines whether a GPU (`cuda`) or CPU is available and sets the device accordingly.\n",
    "\n",
    "2. **Set Up Data Loaders**:\n",
    "   - Specifies the datatype as 'electricity' and the target dimension as 370 for the electricity dataset.\n",
    "   - Calls the `get_dataloader` function to set up training, validation, and test data loaders, along with scalers. These loaders are essential for batching and normalizing data during model training and evaluation.\n",
    "\n",
    "3. **Set Up Model**:\n",
    "   - Initializes the `CSDI_Forecasting` model with the loaded configuration, device, and target dimension.\n",
    "   - Transfers the model to the specified device (GPU or CPU).\n",
    "\n",
    "4. **Set Up Output Folder**:\n",
    "   - Creates a unique folder for saving model outputs and configurations, named based on the current date and time.\n",
    "\n",
    "5. **Save Configuration**:\n",
    "   - Saves the loaded configuration to a JSON file within the output folder, ensuring that the exact settings used for the model are recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set up dataloaders\n",
    "datatype = 'electricity'\n",
    "target_dim = 370  # for electricity dataset\n",
    "\n",
    "train_loader, valid_loader, test_loader, scaler, mean_scaler = get_dataloader(\n",
    "    datatype=datatype,\n",
    "    device=device,\n",
    "    batch_size=config['train']['batch_size']\n",
    ")\n",
    "\n",
    "# Set up model\n",
    "model = CSDI_Forecasting(config, device, target_dim).to(device)\n",
    "\n",
    "# Set up output folder\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "foldername = f\"./save/forecasting_{datatype}_{current_time}/\"\n",
    "os.makedirs(foldername, exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "with open(foldername + \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The provided code snippet demonstrates how to initiate the training process for the model:\n",
    "\n",
    "1. **Train the Model**:\n",
    "   - Calls the `train` function to start the training process.\n",
    "   - Passes the following parameters to the `train` function:\n",
    "     - `model`: The initialized `CSDI_Forecasting` model.\n",
    "     - `config['train']`: The training configuration settings.\n",
    "     - `train_loader`: The data loader for the training dataset.\n",
    "     - `valid_loader`: The data loader for the validation dataset (optional).\n",
    "     - `foldername`: The directory where training outputs and logs will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▌                                                          | 18/691 [00:55<34:45,  3.10s/it, avg_epoch_loss=0.658, epoch=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfoldername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfoldername\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/gsharma/diffusion_model_bootcamp/reference_implementations/time_series_reference_impelementation/CSDI/utils.py:35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, config, train_loader, valid_loader, valid_epoch_interval, foldername)\u001b[0m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(train_batch)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(\n",
    "    model,\n",
    "    config['train'],\n",
    "    train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    foldername=foldername\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "The provided code snippet outlines the steps to evaluate the trained model:\n",
    "\n",
    "1. **Set Evaluation Parameters**:\n",
    "   - Specifies the number of samples (`nsample`) to be used for evaluation, set to 100 in this case.\n",
    "\n",
    "2. **Evaluate the Model**:\n",
    "   - Calls the `evaluate` function to assess the performance of the trained model.\n",
    "   - Passes the following parameters to the `evaluate` function:\n",
    "     - `model`: The trained `CSDI_Forecasting` model.\n",
    "     - `test_loader`: The data loader for the test dataset.\n",
    "     - `nsample`: The number of samples for evaluation.\n",
    "     - `scaler`: The scaler used for normalizing the data during training.\n",
    "     - `mean_scaler`: The mean scaler used for normalization.\n",
    "     - `foldername`: The directory where evaluation results and logs will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set evaluation parameters\n",
    "nsample = 100  # number of samples for evaluation\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(\n",
    "    model,\n",
    "    test_loader,\n",
    "    nsample=nsample,\n",
    "    scaler=scaler,\n",
    "    mean_scaler=mean_scaler,\n",
    "    foldername=foldername\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(model, modelfolder='pretrained', device=device):\n",
    "    model_path = f\"./save/{modelfolder}/model.pth\"\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    return model\n",
    "\n",
    "# Load pre-trained model\n",
    "pretrained_model = load_pretrained_model(model)\n",
    "pretrained_model.target_dim = target_dim\n",
    "\n",
    "# Evaluate the pre-trained model\n",
    "evaluate(\n",
    "    pretrained_model,\n",
    "    test_loader,\n",
    "    nsample=nsample,\n",
    "    scaler=scaler,\n",
    "    mean_scaler=mean_scaler,\n",
    "    foldername=foldername\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
