{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Score-based Diffusion Models for Time Series Imputation\n",
    "\n",
    "This notebook investigates the application of Conditional Score-based Diffusion Models (CSDI) to time series imputation. Developed by researchers at Stanford University, CSDI adapts diffusion model principles, primarily used in image and audio synthesis, to effectively handle missing data in time series.\n",
    "\n",
    "## Background\n",
    "Time series data, especially in healthcare and finance, often contain gaps resulting from sensor failures or incomplete data capture. Traditional imputation methods typically overlook the complex temporal dependencies inherent in time series data. CSDI addresses these challenges by using a conditional diffusion process to impute missing values, leveraging observed data to guide the imputation process.\n",
    "\n",
    "## Objectives\n",
    "- Introduce the theoretical framework and operation of CSDI.\n",
    "- Implement the CSDI model using PyTorch to showcase its application on real-world datasets.\n",
    "- Assess CSDI's performance in imputing missing values compared to conventional imputation techniques.\n",
    "\n",
    "Through the exploration of CSDI's implementation and its imputation efficacy, this notebook aims to highlight its potential in improving the accuracy and reliability of time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports and Model Setup\n",
    "\n",
    "In this section, we import necessary libraries and modules required for the implementation of the CSDI model. This includes standard data handling libraries like `numpy` and `pandas`, deep learning libraries from `torch`, and specific components for building and training the CSDI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # To load or save Python objects\n",
    "import os  # To interact with the operating system\n",
    "import re  # To use regular expressions\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset  # To handle datasets in PyTorch\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torch  # Main PyTorch library\n",
    "import torch.nn as nn  # Neural network module\n",
    "import torch.nn.functional as F  # Functional interface\n",
    "import math  # Provides access to mathematical functions\n",
    "from linear_attention_transformer import (\n",
    "    LinearAttentionTransformer,\n",
    ")  # For attention mechanisms\n",
    "\n",
    "import argparse  # For command-line option and argument parsing\n",
    "import datetime  # For handling date and time\n",
    "import json  # To work with JSON data\n",
    "import yaml  # To handle YAML files\n",
    "\n",
    "from main_model import CSDI_Physio  # The main model for CSDI on PhysioNet data\n",
    "from dataset_physio import get_dataloader  # Helper to get data loader for the dataset\n",
    "from utils import train, evaluate  # Utility functions for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup and Execution for CSDI Model \n",
    "\n",
    "This segment highlights the setup and execution of utilizing CSDI for time series imputation of physiological data. Below is an overview of its components and their functionalities:\n",
    "\n",
    "### **Imports and Module Loading**\n",
    "- **Libraries and Modules**: The script begins by importing necessary Python libraries for handling configurations, data manipulations, neural network operations, and file system interactions. Custom modules specific to the CSDI model such as data loaders, training, and evaluation functions are also loaded.\n",
    "\n",
    "### **Configuration and Argument Parsing**\n",
    "- **Command Line Arguments**: An `argparse.ArgumentParser` is set up to facilitate input of various experiment parameters through the command line, enhancing the script's flexibility and usability across different experimental conditions without altering the codebase.\n",
    "- **Configuration File**: The experiment's settings are loaded from a YAML configuration file, allowing easy adjustments to the model and training parameters. Modifications to these settings via command line arguments are directly reflected in the configuration, ensuring that each experiment can be finely tuned.\n",
    "\n",
    "### **Output Directory Setup**\n",
    "- **Directory Creation**: The script automatically generates a directory based on the current timestamp and fold number for cross-validation. This structured approach to output management keeps results organized and separated based on experimental conditions, crucial for analysis and comparison of results.\n",
    "- **Configuration Saving**: The final configuration used for the experiment is saved in the created directory, promoting reproducibility and detailed documentation of experimental conditions.\n",
    "\n",
    "### **Data Preparation**\n",
    "- **Data Loaders**: Structured data loaders are prepared for the training, validation, and testing phases, considering factors like batch size and the ratio of missing data, ensuring consistent and correct data handling throughout the experiment.\n",
    "\n",
    "### **Model Handling**\n",
    "- **Model Initialization**: The CSDI model is initialized and configured based on the loaded settings and is prepared for deployment on the specified compute device.\n",
    "- **Conditional Training**: The script provides options to either train the model from scratch or load a pre-trained model, offering flexibility for continued training or fine-tuning of previously trained models.\n",
    "\n",
    "### **Model Training and Evaluation**\n",
    "- **Training**: The model undergoes training using the specified settings, with progress and outputs managed through the structured data loaders.\n",
    "- **Evaluation**: Independently of training, the model is evaluated to assess its performance, particularly focusing on its ability to impute missing values in physiological time series data, with results documented and stored in the designated output directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"CSDI\")\n",
    "parser.add_argument(\"--config\", type=str, default=\"base.yaml\")\n",
    "parser.add_argument(\"--device\", default=\"cuda:0\", help=\"Device for Attack\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1)\n",
    "parser.add_argument(\"--testmissingratio\", type=float, default=0.1)\n",
    "parser.add_argument(\n",
    "    \"--nfold\", type=int, default=0, help=\"for 5fold test (valid value:[0-4])\"\n",
    ")\n",
    "parser.add_argument(\"--unconditional\", action=\"store_true\")\n",
    "parser.add_argument(\"--modelfolder\", type=str, default=\"\")\n",
    "parser.add_argument(\"--nsample\", type=int, default=100)\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "path = \"config/\" + args.config\n",
    "with open(path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config[\"model\"][\"is_unconditional\"] = args.unconditional\n",
    "config[\"model\"][\"test_missing_ratio\"] = args.testmissingratio\n",
    "\n",
    "print(json.dumps(config, indent=4))\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "foldername = \"./save/physio_fold\" + str(args.nfold) + \"_\" + current_time + \"/\"\n",
    "print(\"model folder:\", foldername)\n",
    "os.makedirs(foldername, exist_ok=True)\n",
    "with open(foldername + \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_dataloader(\n",
    "    seed=args.seed,\n",
    "    nfold=args.nfold,\n",
    "    batch_size=config[\"train\"][\"batch_size\"],\n",
    "    missing_ratio=config[\"model\"][\"test_missing_ratio\"],\n",
    ")\n",
    "\n",
    "model = CSDI_Physio(config, args.device).to(args.device)\n",
    "\n",
    "if args.modelfolder == \"\":\n",
    "    train(\n",
    "        model,\n",
    "        config[\"train\"],\n",
    "        train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        foldername=foldername,\n",
    "    )\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"./save/\" + args.modelfolder + \"/model.pth\"))\n",
    "\n",
    "evaluate(model, test_loader, nsample=args.nsample, scaler=1, foldername=foldername)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Data Setup\n",
    "\n",
    "Next, we set up the data loaders and initialize the model. The CSDI model, in this notebook has been customized for physiological data from the PhysioNet dataset. It will be prepared along with necessary configurations for training and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35 attributes which contains enough non-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of attributes with significant non-values to consider for imputation\n",
    "attributes = [\n",
    "    \"DiasABP\",\n",
    "    \"HR\",\n",
    "    \"Na\",\n",
    "    \"Lactate\",\n",
    "    \"NIDiasABP\",\n",
    "    \"PaO2\",\n",
    "    \"WBC\",\n",
    "    \"pH\",\n",
    "    \"Albumin\",\n",
    "    \"ALT\",\n",
    "    \"Glucose\",\n",
    "    \"SaO2\",\n",
    "    \"Temp\",\n",
    "    \"AST\",\n",
    "    \"Bilirubin\",\n",
    "    \"HCO3\",\n",
    "    \"BUN\",\n",
    "    \"RespRate\",\n",
    "    \"Mg\",\n",
    "    \"HCT\",\n",
    "    \"SysABP\",\n",
    "    \"FiO2\",\n",
    "    \"K\",\n",
    "    \"GCS\",\n",
    "    \"Cholesterol\",\n",
    "    \"NISysABP\",\n",
    "    \"TroponinT\",\n",
    "    \"MAP\",\n",
    "    \"TroponinI\",\n",
    "    \"PaCO2\",\n",
    "    \"Platelets\",\n",
    "    \"Urine\",\n",
    "    \"NIMAP\",\n",
    "    \"Creatinine\",\n",
    "    \"ALP\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling and Preprocessing \n",
    "\n",
    "This section outlines the procedures and components involved in processing physiological time series data for the purpose of time series imputation:\n",
    "\n",
    "- **extract_hour**: Converts timestamp strings from the dataset into numerical hours of the day. This numerical representation is crucial for aligning and analyzing time series data across multiple days.\n",
    "\n",
    "- **parse_data**: Processes individual data frames by extracting the last recorded values for a predefined list of attributes. This function fills missing entries with `NaN`, facilitating the creation of a comprehensive feature matrix for each patient over the designated timeframe.\n",
    "\n",
    "- **parse_id**: Manages data for individual patients by generating arrays of observed values and corresponding masks that indicate actual data points. It introduces controlled randomness in data availability by applying a missing data ratio, which mimics real-world scenarios of incomplete data, enhancing the model's ability to handle such occurrences effectively.\n",
    "\n",
    "- **get_idlist**: Retrieves a list of patient IDs by scanning a specified directory for data files, ensuring all available data is accounted for and prepared for further processing.\n",
    "\n",
    "- **Physio_Dataset**: Implements a custom dataset class for physiological data, which handles tasks such as loading, normalizing, and batching the data efficiently. It allows for the dynamic creation of datasets from raw data or loading from preprocessed files, supporting robust data handling within the PyTorch framework.\n",
    "\n",
    "- **get_dataloader**: Sets up DataLoaders for different phases of model training and evaluation—training, validation, and testing. This function facilitates the division of data into subsets for cross-validation, ensuring each subset is properly shuffled and batched according to the model's requirements.\n",
    "\n",
    "These components work together to ensure that the data is accurately prepared and readily available for implementing and training the imputation model, optimizing the workflow from raw data handling to model application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hour(x):\n",
    "    # Extracts the hour from a timestamp\n",
    "    h, _ = map(int, x.split(\":\"))\n",
    "    return h\n",
    "\n",
    "\n",
    "def parse_data(x):\n",
    "    # Extract the last recorded value for each attribute\n",
    "    x = x.set_index(\"Parameter\").to_dict()[\"Value\"]\n",
    "    values = []\n",
    "    for attr in attributes:\n",
    "        if x.__contains__(attr):\n",
    "            values.append(x[attr])\n",
    "        else:\n",
    "            values.append(np.nan)  # Use dict.get to simplify missing value handling\n",
    "    return values\n",
    "\n",
    "\n",
    "def parse_id(id_, missing_ratio=0.1):\n",
    "    # Parses the data for a single patient ID, handling missing data by the specified ratio\n",
    "    data = pd.read_csv(\"./data/physio/set-a/{}.txt\".format(id_))\n",
    "    # set hour\n",
    "    data[\"Time\"] = data[\"Time\"].apply(lambda x: extract_hour(x))\n",
    "\n",
    "    # Create a matrix for 48 hours x 35 attributes\n",
    "    observed_values = []\n",
    "    for h in range(48):\n",
    "        observed_values.append(parse_data(data[data[\"Time\"] == h]))\n",
    "    observed_values = np.array(observed_values)\n",
    "    observed_masks = ~np.isnan(observed_values)\n",
    "\n",
    "    # Randomly set some entries as missing based on the missing_ratio\n",
    "    masks = observed_masks.reshape(-1).copy()\n",
    "    obs_indices = np.where(masks)[0].tolist()\n",
    "    miss_indices = np.random.choice(\n",
    "        obs_indices, (int)(len(obs_indices) * missing_ratio), replace=False\n",
    "    )\n",
    "    masks[miss_indices] = False\n",
    "    gt_masks = masks.reshape(observed_masks.shape)\n",
    "    observed_values = np.nan_to_num(observed_values)\n",
    "    observed_masks = observed_masks.astype(\"float32\")\n",
    "    gt_masks = gt_masks.astype(\"float32\")\n",
    "    return observed_values, observed_masks, gt_masks\n",
    "\n",
    "\n",
    "def get_idlist():\n",
    "    # Retrieves a list of patient IDs from filenames in a directory\n",
    "    patient_id = []\n",
    "    for filename in os.listdir(\"./data/physio/set-a\"):\n",
    "        match = re.search(\"\\d{6}\", filename)\n",
    "        if match:\n",
    "            patient_id.append(match.group())\n",
    "    patient_id = np.sort(patient_id)\n",
    "    return patient_id\n",
    "\n",
    "\n",
    "class Physio_Dataset(Dataset):\n",
    "    # A custom PyTorch Dataset for handling physiological data\n",
    "    def __init__(self, eval_length=48, use_index_list=None, missing_ratio=0.0, seed=0):\n",
    "        self.eval_length = eval_length\n",
    "        np.random.seed(seed)  # Set seed for reproducibility in missing data simulation\n",
    "        self.observed_values = []\n",
    "        self.observed_masks = []\n",
    "        self.gt_masks = []\n",
    "        # Attempt to load preprocessed data from a pickle file or generate if not available\n",
    "        path = (\n",
    "            \"./data/physio_missing\" + str(missing_ratio) + \"_seed\" + str(seed) + \".pk\"\n",
    "        )\n",
    "        if os.path.isfile(path):  # if datasetfile is none, create\n",
    "            idlist = get_idlist()\n",
    "            for id_ in idlist:\n",
    "                try:\n",
    "                    observed_values, observed_masks, gt_masks = parse_id(\n",
    "                        id_, missing_ratio\n",
    "                    )\n",
    "                    self.observed_values.append(observed_values)\n",
    "                    self.observed_masks.append(observed_masks)\n",
    "                    self.gt_masks.append(gt_masks)\n",
    "                except Exception as e:\n",
    "                    print(id_, e)\n",
    "                    continue\n",
    "            self.observed_values = np.array(self.observed_values)\n",
    "            self.observed_masks = np.array(self.observed_masks)\n",
    "            self.gt_masks = np.array(self.gt_masks)\n",
    "\n",
    "            # calc mean and std and normalize values\n",
    "            # (it is the same normalization as Cao et al. (2018) (https://github.com/caow13/BRITS))\n",
    "            tmp_values = self.observed_values.reshape(-1, 35)\n",
    "            tmp_masks = self.observed_masks.reshape(-1, 35)\n",
    "            mean = np.zeros(35)\n",
    "            std = np.zeros(35)\n",
    "            for k in range(35):\n",
    "                c_data = tmp_values[:, k][tmp_masks[:, k] == 1]\n",
    "                mean[k] = c_data.mean()\n",
    "                std[k] = c_data.std()\n",
    "            self.observed_values = (\n",
    "                (self.observed_values - mean) / std * self.observed_masks\n",
    "            )\n",
    "            # Save processed data\n",
    "            with open(path, \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    [self.observed_values, self.observed_masks, self.gt_masks], f\n",
    "                )\n",
    "        else:  # load datasetfile\n",
    "            with open(path, \"rb\") as f:\n",
    "                self.observed_values, self.observed_masks, self.gt_masks = pickle.load(\n",
    "                    f\n",
    "                )\n",
    "        if use_index_list is None:\n",
    "            self.use_index_list = np.arange(len(self.observed_values))\n",
    "        else:\n",
    "            self.use_index_list = use_index_list\n",
    "\n",
    "    def __getitem__(self, org_index):\n",
    "        index = self.use_index_list[org_index]\n",
    "        s = {\n",
    "            \"observed_data\": self.observed_values[index],\n",
    "            \"observed_mask\": self.observed_masks[index],\n",
    "            \"gt_mask\": self.gt_masks[index],\n",
    "            \"timepoints\": np.arange(self.eval_length),\n",
    "        }\n",
    "        return s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_index_list)\n",
    "\n",
    "\n",
    "def get_dataloader(seed=1, nfold=None, batch_size=16, missing_ratio=0.1):\n",
    "    # only to obtain total length of dataset\n",
    "    dataset = Physio_Dataset(missing_ratio=missing_ratio, seed=seed)\n",
    "    indlist = np.arange(len(dataset))\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indlist)\n",
    "\n",
    "    # 5-fold test\n",
    "    start = (int)(nfold * 0.2 * len(dataset))\n",
    "    end = (int)((nfold + 1) * 0.2 * len(dataset))\n",
    "    test_index = indlist[start:end]\n",
    "    remain_index = np.delete(indlist, np.arange(start, end))\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(remain_index)\n",
    "    num_train = (int)(len(dataset) * 0.7)\n",
    "    train_index = remain_index[:num_train]\n",
    "    valid_index = remain_index[num_train:]\n",
    "    dataset = Physio_Dataset(\n",
    "        use_index_list=train_index, missing_ratio=missing_ratio, seed=seed\n",
    "    )\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=1)\n",
    "    valid_dataset = Physio_Dataset(\n",
    "        use_index_list=valid_index, missing_ratio=missing_ratio, seed=seed\n",
    "    )\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=0)\n",
    "    test_dataset = Physio_Dataset(\n",
    "        use_index_list=test_index, missing_ratio=missing_ratio, seed=seed\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=0)\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Diffusion Model\n",
    "\n",
    "This section outlines the implementation details of the neural network components that form the backbone of the CSDI for time series imputation i.e. diffusion model. \n",
    "\n",
    "### Transformer Layers for Long-range Dependencies\n",
    "The `get_torch_trans` and `get_linear_trans` functions are designed to construct transformer layers, which are important for CSDI's ability to capture long-range dependencies within data sequences. `get_torch_trans` creates a standard transformer encoder that utilizes multiple attention heads to process various segments of the input data concurrently. Conversely, `get_linear_trans` sets up a transformer using linear attention mechanisms to manage longer sequences efficiently by reducing computational overhead, which is beneficial for scalability and performance in large datasets.\n",
    "\n",
    "### Customized Convolutional Layer Initialization\n",
    "The initialization of convolutional layers through `Conv1d_with_init` uses the Kaiming normalization method to ensure consistent variance of activations, promoting stable learning dynamics across the model's layers. These convolutional layers are essential for extracting localized features from data, aiding the transformer layers in detailed pattern recognition.\n",
    "\n",
    "### Embedding Temporal Dynamics with Diffusion Embedding\n",
    "The `DiffusionEmbedding` class provides a method to embed the diffusion steps used in CSDI into a high-dimensional space using sinusoidal functions. This embedding is integral to the model, as it enables the precise modeling of how data evolves through the diffusion process, which is central to the score-based generative approach of CSDI.\n",
    "\n",
    "### Model Architecture\n",
    "The `diff_CSDI` class encapsulates the complete model structure, integrating diffusion embeddings with transformer and convolutional layers. This class demonstrates the application of advanced neural network techniques to build a robust architecture for time series imputation. It is specifically tailored to exploit both the temporal and feature-wise dependencies within data, leveraging the unique properties of diffusion models for effective imputation.\n",
    "\n",
    "### Residual Blocks for Enhanced Learning\n",
    "Within `diff_CSDI`, the `ResidualBlock` utilizes a dual-path approach to process information across both time and feature dimensions independently, allowing the model to capture complex interactions in the data. This design enhances the model's predictive accuracy and is a key component of the CSDI architecture, reflecting its capability to adapt and learn from multi-dimensional time series data effectively.\n",
    "\n",
    "These components collectively form a sophisticated framework designed for the CSDI model, showcasing how theoretical advancements in machine learning can be practically applied to solve challenges in time series imputation with high efficiency and adaptability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_trans(heads=8, layers=1, channels=64):\n",
    "    encoder_layer = nn.TransformerEncoderLayer(\n",
    "        d_model=channels, nhead=heads, dim_feedforward=64, activation=\"gelu\"\n",
    "    )\n",
    "    return nn.TransformerEncoder(encoder_layer, num_layers=layers)\n",
    "\n",
    "\n",
    "def get_linear_trans(heads=8, layers=1, channels=64, localheads=0, localwindow=0):\n",
    "    return LinearAttentionTransformer(\n",
    "        dim=channels,\n",
    "        depth=layers,\n",
    "        heads=heads,\n",
    "        max_seq_len=256,\n",
    "        n_local_attn_heads=0,\n",
    "        local_attn_window_size=0,\n",
    "    )\n",
    "\n",
    "\n",
    "def Conv1d_with_init(in_channels, out_channels, kernel_size):\n",
    "    layer = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "    nn.init.kaiming_normal_(layer.weight)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class DiffusionEmbedding(nn.Module):\n",
    "    def __init__(self, num_steps, embedding_dim=128, projection_dim=None):\n",
    "        super().__init__()\n",
    "        if projection_dim is None:\n",
    "            projection_dim = embedding_dim\n",
    "        self.register_buffer(\n",
    "            \"embedding\",\n",
    "            self._build_embedding(num_steps, embedding_dim / 2),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.projection1 = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.projection2 = nn.Linear(projection_dim, projection_dim)\n",
    "\n",
    "    def forward(self, diffusion_step):\n",
    "        x = self.embedding[diffusion_step]\n",
    "        x = self.projection1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.projection2(x)\n",
    "        x = F.silu(x)\n",
    "        return x\n",
    "\n",
    "    def _build_embedding(self, num_steps, dim=64):\n",
    "        steps = torch.arange(num_steps).unsqueeze(1)  # (T,1)\n",
    "        frequencies = 10.0 ** (torch.arange(dim) / (dim - 1) * 4.0).unsqueeze(\n",
    "            0\n",
    "        )  # (1,dim)\n",
    "        table = steps * frequencies  # (T,dim)\n",
    "        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)  # (T,dim*2)\n",
    "        return table\n",
    "\n",
    "\n",
    "# class diff_CSDI(nn.Module):\n",
    "#     def __init__(self, config, inputdim=2):\n",
    "#         super().__init__()\n",
    "#         self.channels = config[\"channels\"]\n",
    "\n",
    "#         self.diffusion_embedding = DiffusionEmbedding(\n",
    "#             num_steps=config[\"num_steps\"],\n",
    "#             embedding_dim=config[\"diffusion_embedding_dim\"],\n",
    "#         )\n",
    "\n",
    "#         self.input_projection = Conv1d_with_init(inputdim, self.channels, 1)\n",
    "#         self.output_projection1 = Conv1d_with_init(self.channels, self.channels, 1)\n",
    "#         self.output_projection2 = Conv1d_with_init(self.channels, 1, 1)\n",
    "#         nn.init.zeros_(self.output_projection2.weight)\n",
    "\n",
    "#         self.residual_layers = nn.ModuleList(\n",
    "#             [\n",
    "#                 ResidualBlock(\n",
    "#                     side_dim=config[\"side_dim\"],\n",
    "#                     channels=self.channels,\n",
    "#                     diffusion_embedding_dim=config[\"diffusion_embedding_dim\"],\n",
    "#                     nheads=config[\"nheads\"],\n",
    "#                     is_linear=config[\"is_linear\"],\n",
    "#                 )\n",
    "#                 for _ in range(config[\"layers\"])\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "# def forward(self, x, cond_info, diffusion_step):\n",
    "#     B, inputdim, K, L = x.shape\n",
    "\n",
    "#     x = x.reshape(B, inputdim, K * L)\n",
    "#     x = self.input_projection(x)\n",
    "#     x = F.relu(x)\n",
    "#     x = x.reshape(B, self.channels, K, L)\n",
    "\n",
    "#     diffusion_emb = self.diffusion_embedding(diffusion_step)\n",
    "\n",
    "#     skip = []\n",
    "#     for layer in self.residual_layers:\n",
    "#         x, skip_connection = layer(x, cond_info, diffusion_emb)\n",
    "#         skip.append(skip_connection)\n",
    "\n",
    "#     x = torch.sum(torch.stack(skip), dim=0) / math.sqrt(len(self.residual_layers))\n",
    "#     x = x.reshape(B, self.channels, K * L)\n",
    "#     x = self.output_projection1(x)  # (B,channel,K*L)\n",
    "#     x = F.relu(x)\n",
    "#     x = self.output_projection2(x)  # (B,1,K*L)\n",
    "#     x = x.reshape(B, K, L)\n",
    "#     return x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, side_dim, channels, diffusion_embedding_dim, nheads, is_linear=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.diffusion_projection = nn.Linear(diffusion_embedding_dim, channels)\n",
    "        self.cond_projection = Conv1d_with_init(side_dim, 2 * channels, 1)\n",
    "        self.mid_projection = Conv1d_with_init(channels, 2 * channels, 1)\n",
    "        self.output_projection = Conv1d_with_init(channels, 2 * channels, 1)\n",
    "\n",
    "        self.is_linear = is_linear\n",
    "        if is_linear:\n",
    "            self.time_layer = get_linear_trans(\n",
    "                heads=nheads, layers=1, channels=channels\n",
    "            )\n",
    "            self.feature_layer = get_linear_trans(\n",
    "                heads=nheads, layers=1, channels=channels\n",
    "            )\n",
    "        else:\n",
    "            self.time_layer = get_torch_trans(heads=nheads, layers=1, channels=channels)\n",
    "            self.feature_layer = get_torch_trans(\n",
    "                heads=nheads, layers=1, channels=channels\n",
    "            )\n",
    "\n",
    "    def forward_time(self, y, base_shape):\n",
    "        B, channel, K, L = base_shape\n",
    "        if L == 1:\n",
    "            return y\n",
    "        y = y.reshape(B, channel, K, L).permute(0, 2, 1, 3).reshape(B * K, channel, L)\n",
    "\n",
    "        if self.is_linear:\n",
    "            y = self.time_layer(y.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        else:\n",
    "            y = self.time_layer(y.permute(2, 0, 1)).permute(1, 2, 0)\n",
    "        y = y.reshape(B, K, channel, L).permute(0, 2, 1, 3).reshape(B, channel, K * L)\n",
    "        return y\n",
    "\n",
    "    def forward_feature(self, y, base_shape):\n",
    "        B, channel, K, L = base_shape\n",
    "        if K == 1:\n",
    "            return y\n",
    "        y = y.reshape(B, channel, K, L).permute(0, 3, 1, 2).reshape(B * L, channel, K)\n",
    "        if self.is_linear:\n",
    "            y = self.feature_layer(y.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        else:\n",
    "            y = self.feature_layer(y.permute(2, 0, 1)).permute(1, 2, 0)\n",
    "        y = y.reshape(B, L, channel, K).permute(0, 2, 3, 1).reshape(B, channel, K * L)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, cond_info, diffusion_emb):\n",
    "        B, channel, K, L = x.shape\n",
    "        base_shape = x.shape\n",
    "        x = x.reshape(B, channel, K * L)\n",
    "\n",
    "        diffusion_emb = self.diffusion_projection(diffusion_emb).unsqueeze(\n",
    "            -1\n",
    "        )  # (B,channel,1)\n",
    "        y = x + diffusion_emb\n",
    "\n",
    "        y = self.forward_time(y, base_shape)\n",
    "        y = self.forward_feature(y, base_shape)  # (B,channel,K*L)\n",
    "        y = self.mid_projection(y)  # (B,2*channel,K*L)\n",
    "\n",
    "        _, cond_dim, _, _ = cond_info.shape\n",
    "        cond_info = cond_info.reshape(B, cond_dim, K * L)\n",
    "        cond_info = self.cond_projection(cond_info)  # (B,2*channel,K*L)\n",
    "        y = y + cond_info\n",
    "\n",
    "        gate, filter = torch.chunk(y, 2, dim=1)\n",
    "        y = torch.sigmoid(gate) * torch.tanh(filter)  # (B,channel,K*L)\n",
    "        y = self.output_projection(y)\n",
    "\n",
    "        residual, skip = torch.chunk(y, 2, dim=1)\n",
    "        x = x.reshape(base_shape)\n",
    "        residual = residual.reshape(base_shape)\n",
    "        skip = skip.reshape(base_shape)\n",
    "        return (x + residual) / math.sqrt(2.0), skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the CSDI Model\n",
    "\n",
    "This section describes the training process of the Conditional Score-based Diffusion Model (CSDI) and the evaluation metrics used to measure its performance in time series imputation tasks.\n",
    "\n",
    "### Model Training\n",
    "The `train` function orchestrates the model training over multiple epochs, handling both the training and optional validation phases to optimize and evaluate the model's performance iteratively:\n",
    "\n",
    "- **Optimizer and Scheduler**: An Adam optimizer is initialized with specific learning rate and weight decay parameters from the configuration. A learning rate scheduler adjusts the learning rate at predefined milestones to fine-tune the training process as it progresses, typically reducing the learning rate to stabilize training as it nears completion.\n",
    "- **Training Loop**: The model undergoes training over a specified number of epochs, processing batches of data loaded through `train_loader`. For each batch, the model performs a forward pass to compute the loss, followed by a backward pass to update the model weights. Progress and average loss for each epoch are displayed using the tqdm progress bar.\n",
    "- **Validation**: If a `valid_loader` is provided, the model periodically evaluates its performance on the validation set after specified intervals. This phase involves computing the loss over the validation data without backpropagation or weight updates, providing an estimate of the model's performance on unseen data.\n",
    "- **Model Saving**: If improved validation loss is observed, the model's parameters are saved to the specified directory, ensuring that the best-performing model is retained.\n",
    "\n",
    "### Quantile Loss and CRPS Calculation\n",
    "- **Quantile Loss**: This custom loss function measures the accuracy of predicted quantiles against actual data. It is used particularly for evaluating forecasts that involve probabilistic predictions. The loss function emphasizes the asymmetry in overestimation versus underestimation, weighted by the quantile level.\n",
    "- **Continuous Ranked Probability Score (CRPS)**: CRPS is calculated to assess the quality of probabilistic forecasts. It is computed as the average quantile loss across multiple quantiles, providing a single score that summarises the model's accuracy across the entire probability distribution of outcomes.\n",
    "- **CRPS Sum**: In scenarios involving aggregated or cumulative data predictions, CRPS sum is calculated to evaluate the model's performance on summed predictions. This is particularly useful in applications like financial forecasting where cumulative figures are more relevant than individual predictions.\n",
    "\n",
    "These components ensure the model is not only trained effectively but also evaluated using metrics that provide deep insights into its probabilistic forecasting abilities. The use of CRPS and quantile-based evaluations aligns well with the needs of applications requiring reliable uncertainty estimates in their predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    config,\n",
    "    train_loader,\n",
    "    valid_loader=None,\n",
    "    valid_epoch_interval=20,\n",
    "    foldername=\"\",\n",
    "):\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"lr\"], weight_decay=1e-6)\n",
    "    if foldername != \"\":\n",
    "        output_path = foldername + \"/model.pth\"\n",
    "\n",
    "    p1 = int(0.75 * config[\"epochs\"])\n",
    "    p2 = int(0.9 * config[\"epochs\"])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[p1, p2], gamma=0.1\n",
    "    )\n",
    "\n",
    "    best_valid_loss = 1e10\n",
    "    for epoch_no in range(config[\"epochs\"]):\n",
    "        avg_loss = 0\n",
    "        model.train()\n",
    "        with tqdm(train_loader, mininterval=5.0, maxinterval=50.0) as it:\n",
    "            for batch_no, train_batch in enumerate(it, start=1):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss = model(train_batch)\n",
    "                loss.backward()\n",
    "                avg_loss += loss.item()\n",
    "                optimizer.step()\n",
    "                it.set_postfix(\n",
    "                    ordered_dict={\n",
    "                        \"avg_epoch_loss\": avg_loss / batch_no,\n",
    "                        \"epoch\": epoch_no,\n",
    "                    },\n",
    "                    refresh=False,\n",
    "                )\n",
    "                if batch_no >= config[\"itr_per_epoch\"]:\n",
    "                    break\n",
    "\n",
    "            lr_scheduler.step()\n",
    "        if valid_loader is not None and (epoch_no + 1) % valid_epoch_interval == 0:\n",
    "            model.eval()\n",
    "            avg_loss_valid = 0\n",
    "            with torch.no_grad():\n",
    "                with tqdm(valid_loader, mininterval=5.0, maxinterval=50.0) as it:\n",
    "                    for batch_no, valid_batch in enumerate(it, start=1):\n",
    "                        loss = model(valid_batch, is_train=0)\n",
    "                        avg_loss_valid += loss.item()\n",
    "                        it.set_postfix(\n",
    "                            ordered_dict={\n",
    "                                \"valid_avg_epoch_loss\": avg_loss_valid / batch_no,\n",
    "                                \"epoch\": epoch_no,\n",
    "                            },\n",
    "                            refresh=False,\n",
    "                        )\n",
    "            if best_valid_loss > avg_loss_valid:\n",
    "                best_valid_loss = avg_loss_valid\n",
    "                print(\n",
    "                    \"\\n best loss is updated to \",\n",
    "                    avg_loss_valid / batch_no,\n",
    "                    \"at\",\n",
    "                    epoch_no,\n",
    "                )\n",
    "\n",
    "    if foldername != \"\":\n",
    "        torch.save(model.state_dict(), output_path)\n",
    "\n",
    "\n",
    "def quantile_loss(target, forecast, q: float, eval_points) -> float:\n",
    "    return 2 * torch.sum(\n",
    "        torch.abs((forecast - target) * eval_points * ((target <= forecast) * 1.0 - q))\n",
    "    )\n",
    "\n",
    "\n",
    "def calc_denominator(target, eval_points):\n",
    "    return torch.sum(torch.abs(target * eval_points))\n",
    "\n",
    "\n",
    "def calc_quantile_CRPS(target, forecast, eval_points, mean_scaler, scaler):\n",
    "    target = target * scaler + mean_scaler\n",
    "    forecast = forecast * scaler + mean_scaler\n",
    "\n",
    "    quantiles = np.arange(0.05, 1.0, 0.05)\n",
    "    denom = calc_denominator(target, eval_points)\n",
    "    CRPS = 0\n",
    "    for i in range(len(quantiles)):\n",
    "        q_pred = []\n",
    "        for j in range(len(forecast)):\n",
    "            q_pred.append(torch.quantile(forecast[j : j + 1], quantiles[i], dim=1))\n",
    "        q_pred = torch.cat(q_pred, 0)\n",
    "        q_loss = quantile_loss(target, q_pred, quantiles[i], eval_points)\n",
    "        CRPS += q_loss / denom\n",
    "    return CRPS.item() / len(quantiles)\n",
    "\n",
    "\n",
    "def calc_quantile_CRPS_sum(target, forecast, eval_points, mean_scaler, scaler):\n",
    "    eval_points = eval_points.mean(-1)\n",
    "    target = target * scaler + mean_scaler\n",
    "    target = target.sum(-1)\n",
    "    forecast = forecast * scaler + mean_scaler\n",
    "\n",
    "    quantiles = np.arange(0.05, 1.0, 0.05)\n",
    "    denom = calc_denominator(target, eval_points)\n",
    "    CRPS = 0\n",
    "    for i in range(len(quantiles)):\n",
    "        q_pred = torch.quantile(forecast.sum(-1), quantiles[i], dim=1)\n",
    "        q_loss = quantile_loss(target, q_pred, quantiles[i], eval_points)\n",
    "        CRPS += q_loss / denom\n",
    "    return CRPS.item() / len(quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for CSDI Model \n",
    "\n",
    "The `evaluate` function is designed to assess the CSDI model's performance by processing test data loaded through `test_loader`. This function focuses on generating and evaluating a number of sample predictions per test input, facilitating an in-depth analysis of the model's accuracy and reliability.\n",
    "\n",
    "### Evaluation Procedure\n",
    "- **Model State and Setup**: The model is set to evaluation mode to disable operations specific to training phases like dropout. The function calculates multiple metrics including mean squared error (MSE), mean absolute error (MAE), and Continuous Ranked Probability Score (CRPS) to provide a holistic view of model performance.\n",
    "\n",
    "- **Batch Processing**: Test data is processed batch-by-batch. For each batch:\n",
    "  - The model generates multiple samples for each input sequence to capture the distribution of possible outcomes, which is crucial for evaluating probabilistic models like CSDI.\n",
    "  - Each sample output is compared to the actual target values using metrics adapted for evaluating forecasts:\n",
    "    - **Median Prediction**: The median of the generated samples is computed and used to calculate MSE and MAE. This approach highlights the model's ability to predict the central tendency of the distribution accurately.\n",
    "    - **Quantile Calculation**: Samples are also used to calculate the CRPS, which quantifies the model's accuracy across the entire distribution of forecasts, not just at the median or mean.\n",
    "\n",
    "### Aggregating Results\n",
    "- **Data Aggregation**: All metric calculations are accumulated over the entire test set to ensure comprehensive evaluation. This includes aggregating the targets, evaluation points, and generated samples.\n",
    "- **Persistence**: Results, including raw generated samples and calculated metrics, are saved to files for further analysis and verification. This is important for detailed post-evaluation analysis and for ensuring reproducibility.\n",
    "\n",
    "### Metrics Computation\n",
    "- **CRPS and Sum**: The function also computes CRPS and its sum variant, which are particularly informative for aggregated data predictions, such as in financial forecasting or total rainfall estimation. These metrics provide insight into the accuracy of the model's probabilistic forecasts.\n",
    "- **Output**: The computed root mean squared error (RMSE), MAE, CRPS, and CRPS sum are printed and saved, providing a quantified summary of model performance across different aspects of forecasting accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, nsample=100, scaler=1, mean_scaler=0, foldername=\"\"):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        mse_total = 0\n",
    "        mae_total = 0\n",
    "        evalpoints_total = 0\n",
    "\n",
    "        all_target = []\n",
    "        all_observed_point = []\n",
    "        all_observed_time = []\n",
    "        all_evalpoint = []\n",
    "        all_generated_samples = []\n",
    "        with tqdm(test_loader, mininterval=5.0, maxinterval=50.0) as it:\n",
    "            for batch_no, test_batch in enumerate(it, start=1):\n",
    "                output = model.evaluate(test_batch, nsample)\n",
    "\n",
    "                samples, c_target, eval_points, observed_points, observed_time = output\n",
    "                samples = samples.permute(0, 1, 3, 2)  # (B,nsample,L,K)\n",
    "                c_target = c_target.permute(0, 2, 1)  # (B,L,K)\n",
    "                eval_points = eval_points.permute(0, 2, 1)\n",
    "                observed_points = observed_points.permute(0, 2, 1)\n",
    "\n",
    "                samples_median = samples.median(dim=1)\n",
    "                all_target.append(c_target)\n",
    "                all_evalpoint.append(eval_points)\n",
    "                all_observed_point.append(observed_points)\n",
    "                all_observed_time.append(observed_time)\n",
    "                all_generated_samples.append(samples)\n",
    "\n",
    "                mse_current = (\n",
    "                    ((samples_median.values - c_target) * eval_points) ** 2\n",
    "                ) * (scaler**2)\n",
    "                mae_current = (\n",
    "                    torch.abs((samples_median.values - c_target) * eval_points)\n",
    "                ) * scaler\n",
    "\n",
    "                mse_total += mse_current.sum().item()\n",
    "                mae_total += mae_current.sum().item()\n",
    "                evalpoints_total += eval_points.sum().item()\n",
    "\n",
    "                it.set_postfix(\n",
    "                    ordered_dict={\n",
    "                        \"rmse_total\": np.sqrt(mse_total / evalpoints_total),\n",
    "                        \"mae_total\": mae_total / evalpoints_total,\n",
    "                        \"batch_no\": batch_no,\n",
    "                    },\n",
    "                    refresh=True,\n",
    "                )\n",
    "\n",
    "            with open(\n",
    "                foldername + \"/generated_outputs_nsample\" + str(nsample) + \".pk\", \"wb\"\n",
    "            ) as f:\n",
    "                all_target = torch.cat(all_target, dim=0)\n",
    "                all_evalpoint = torch.cat(all_evalpoint, dim=0)\n",
    "                all_observed_point = torch.cat(all_observed_point, dim=0)\n",
    "                all_observed_time = torch.cat(all_observed_time, dim=0)\n",
    "                all_generated_samples = torch.cat(all_generated_samples, dim=0)\n",
    "\n",
    "                pickle.dump(\n",
    "                    [\n",
    "                        all_generated_samples,\n",
    "                        all_target,\n",
    "                        all_evalpoint,\n",
    "                        all_observed_point,\n",
    "                        all_observed_time,\n",
    "                        scaler,\n",
    "                        mean_scaler,\n",
    "                    ],\n",
    "                    f,\n",
    "                )\n",
    "\n",
    "            CRPS = calc_quantile_CRPS(\n",
    "                all_target, all_generated_samples, all_evalpoint, mean_scaler, scaler\n",
    "            )\n",
    "            CRPS_sum = calc_quantile_CRPS_sum(\n",
    "                all_target, all_generated_samples, all_evalpoint, mean_scaler, scaler\n",
    "            )\n",
    "\n",
    "            with open(foldername + \"/result_nsample\" + str(nsample) + \".pk\", \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    [\n",
    "                        np.sqrt(mse_total / evalpoints_total),\n",
    "                        mae_total / evalpoints_total,\n",
    "                        CRPS,\n",
    "                    ],\n",
    "                    f,\n",
    "                )\n",
    "                print(\"RMSE:\", np.sqrt(mse_total / evalpoints_total))\n",
    "                print(\"MAE:\", mae_total / evalpoints_total)\n",
    "                print(\"CRPS:\", CRPS)\n",
    "                print(\"CRPS_sum:\", CRPS_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- Tashiro, Yusuke, et al. \"CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation.\" *Advances in Neural Information Processing Systems*. 2021. [GitHub Repository](https://github.com/ermongroup/CSDI)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
