# TSDiff: An Unconditional Diffusion Model for Time Series

[![Paper](https://img.shields.io/static/v1?label=arXiv&message=2307.11494&color=B31B1B)](https://arxiv.org/abs/2307.11494)

This codebase contains the implementation of the NeurIPS 2023 paper [*Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting*](https://arxiv.org/abs/2307.11494). In this paper, they propose *TSDiff*, an unconditional diffusion model for time series. This proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. Furthermore, the refinement scheme leverages the implicit density learned by the diffusion model to iteratively refine the predictions of base forecasters. Finally, they demonstrate the high quality of the synthetic time series by training downstrain models solely on generated data and introducing the *Linear Predictive Score (LPS)*.

# TSDiff: An Unconditional Diffusion Model for Time Series

This repository contains the implementation of the NeurIPS 2023 paper [*Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting*]. In this paper, *TSDiff*, an unconditional diffusion model, is designed for time series data. 

## Overview

The primary contributions of this work include:

1. **Self-Guidance Mechanism**: This novel mechanism allows TSDiff to be conditioned for various downstream tasks during inference, without the need for auxiliary networks or changes to the training procedure.
2. **Refinement Scheme**: This scheme takes advantage of the implicit density learned by the diffusion model to iteratively improve the predictions made by base forecasters.
3. **Synthetic Data Generation**: They demonstrate the high quality of synthetic time series data generated by TSDiff. This is validated by training downstream models exclusively on the generated data and evaluating their performance on real datasets.
   
## Notebooks

The repository includes two key Jupyter notebooks that demonstrate the use of TSDiff:

1. **`forecasting.ipynb`**: This notebook provides a comprehensive demo for training the TSDiff model and using it for the forecasting task. It walks you through the entire process, from data preparation to model training and evaluation of forecasting performance.

2. **`train_on_synthetic_test_on_real.ipynb`**: This notebook demonstrates how to use TSDiff for generating synthetic datasets. It includes a detailed example of training various forecasters on the synthetic data generated by TSDiff and then testing these models on real datasets to assess their performance.
