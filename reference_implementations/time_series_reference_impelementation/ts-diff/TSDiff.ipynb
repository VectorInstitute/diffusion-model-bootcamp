{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ea1925-b528-4c96-875c-ba73f97cbd8b",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "## Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a777f9b5-46cd-43f0-8da7-21657f565c46",
   "metadata": {},
   "source": [
    "The paper proposes TSDiff, an unconditionally-trained diffusion model for time series. TSDiff utilizes a self-guidance mechanism that allows it to conditionally generate forecasts, refine predictions, and produce synthetic data without requiring auxiliary networks or altering the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ebc70d-1c01-4e9f-8db1-93ef50d6b2f6",
   "metadata": {},
   "source": [
    "Time series forecasting is crucial for making informed decisions in various fields such as finance, energy, and healthcare. Traditional deep learning models approach this problem through conditional generative modeling. The paper introduces TSDiff, an unconditional diffusion model for time series, which can handle multiple downstream tasks. The self-guidance mechanism allows TSDiff to perform predictive tasks during inference without conditional training. The model's generative capabilities are also leveraged to improve the accuracy of base forecasters and generate high-quality synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0c8ea-11ed-4b7f-905f-841a07cbf00f",
   "metadata": {},
   "source": [
    "In the following, we will take a deeper dive to the implementation of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17887c36-2c62-4f8a-8ab6-d7cccdade296",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a08fdbe-afa8-4be4-b2b7-72c94d5eeca4",
   "metadata": {},
   "source": [
    "In this section, we import all necessary libraries and modules required for setting up the environment. This includes libraries for logging, parsing arguments, handling file paths, and loading configurations. Additionally, we import essential packages for data loading, model creation, and training such as PyTorch, PyTorch Lightning, and GluonTS. Custom modules specific to the time series diffusion model (TSDiff) are also imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e1b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back on slow Cauchy kernel. Install at least one of pykeops or the CUDA extension for efficiency.\n",
      "Falling back on slow Vandermonde kernel. Install pykeops for improved memory efficiency.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar\n",
    "\n",
    "from gluonts.dataset.loader import TrainDataLoader\n",
    "from gluonts.dataset.split import OffsetSplitter\n",
    "from gluonts.itertools import Cached\n",
    "from gluonts.torch.batchify import batchify\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "\n",
    "import uncond_ts_diff.configs as diffusion_configs\n",
    "from uncond_ts_diff.dataset import get_gts_dataset\n",
    "from uncond_ts_diff.model.callback import EvaluateCallback\n",
    "from uncond_ts_diff.model import TSDiff\n",
    "from uncond_ts_diff.sampler import DDPMGuidance, DDIMGuidance\n",
    "from uncond_ts_diff.utils import (\n",
    "    create_transforms,\n",
    "    create_splitter,\n",
    "    add_config_to_argparser,\n",
    "    filter_metrics,\n",
    "    MaskInput,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb1aff-c084-42ee-8484-33a71a5f9521",
   "metadata": {},
   "source": [
    "# Load Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c09deb-8bd4-4eae-906c-3b905cac2b43",
   "metadata": {},
   "source": [
    "Here, we set up the configuration for the model training. This involves loading the configuration file which contains parameters and settings needed for the training process. The configuration is read from a YAML file and parsed into a dictionary format. Logging is also configured in this section to record the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "486282cb-bb69-41a6-8ca9-71222afd9af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_map = {\"ddpm\": DDPMGuidance, \"ddim\": DDIMGuidance}\n",
    "\n",
    "# Setup Logger\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Setup config\n",
    "config_path = \"configs/train_tsdiff/train_uber_tlc.yaml\"\n",
    "log_dir = \"./\"\n",
    "\n",
    "with open(config_path, \"r\") as fp:\n",
    "    config = yaml.safe_load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964c4e-7bc6-4835-8226-2913aa4b5e8a",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b022b-80ae-48b2-9619-ca4c971d24be",
   "metadata": {},
   "source": [
    "**Denoising Diffusion Probabilistic Models (DDPMs):**\n",
    "DDPMs model data generation as a discrete-time diffusion process with Gaussian transitions. The forward process gradually adds noise to the data, while the reverse process, learned by the model, removes this noise to generate data samples. The model is trained to approximate this reverse process by minimizing a simplified objective function.\n",
    "\n",
    "**TSDiff Architecture:**\n",
    "TSDiff is designed for univariate time series and uses S4 layers for temporal modeling. The architecture incorporates historical information by appending lagged time series along the channel dimension, allowing it to handle noisy inputs. The model's output dimensions match its input dimensions, making it suitable for unconditional generative tasks.\n",
    "\n",
    "**Observation Self-Guidance:**\n",
    "This mechanism enables TSDiff to perform conditional forecasting during inference. By leveraging the learned probability density, TSDiff can guide its predictions based on observed data points. Two variants are proposed: mean square self-guidance, which uses Gaussian distribution, and quantile self-guidance, which uses asymmetric Laplace distribution for better quantile-based evaluation.\n",
    "\n",
    "**Prediction Refinement:**\n",
    "TSDiff can iteratively refine the predictions of base forecasters by interpreting the implicit probability density as a prior. This refinement is done directly in the data space, providing a computationally efficient way to improve forecast accuracy without modifying the core forecasting model. Two approaches are presented: energy-based sampling using Langevin Monte Carlo and maximum likelihood optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f902a2-39d4-4ac9-b0eb-1cf2cba32187",
   "metadata": {},
   "source": [
    "The following cells focuses on creating the TSDiff model based on the loaded configuration. A function create_model is defined which initializes the TSDiff model with parameters such as frequency, feature usage, normalization, context length, prediction length, and learning rate. The model is then moved to the specified device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f00df3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    model = TSDiff(\n",
    "        **getattr(diffusion_configs, config[\"diffusion_config\"]),\n",
    "        freq=config[\"freq\"],\n",
    "        use_features=config[\"use_features\"],\n",
    "        use_lags=config[\"use_lags\"],\n",
    "        normalization=config[\"normalization\"],\n",
    "        context_length=config[\"context_length\"],\n",
    "        prediction_length=config[\"prediction_length\"],\n",
    "        lr=config[\"lr\"],\n",
    "        init_skip=config[\"init_skip\"],\n",
    "    )\n",
    "    model.to(config[\"device\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "120544cf-81bc-4f2f-b671-9ef969f341fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b70597-18a7-4859-821b-29e1c7e2b593",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a01a06-4949-4182-b6c1-3a041245de3e",
   "metadata": {},
   "source": [
    "In this section, the dataset is loaded and preprocessed based on the configuration settings. The dataset's metadata is validated to ensure consistency with the expected frequency and prediction length. Depending on the setup (forecasting or missing values), the appropriate data split is performed. Transformations and data loaders are also set up to facilitate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1672dc3-ea0d-4982-a2fd-23dfaaee7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "dataset_name = config[\"dataset\"]\n",
    "freq = config[\"freq\"]\n",
    "context_length = config[\"context_length\"]\n",
    "prediction_length = config[\"prediction_length\"]\n",
    "total_length = context_length + prediction_length\n",
    "\n",
    "\n",
    "\n",
    "# Setup dataset and data loading\n",
    "dataset = get_gts_dataset(dataset_name)\n",
    "assert dataset.metadata.freq == freq\n",
    "assert dataset.metadata.prediction_length == prediction_length\n",
    "\n",
    "if config[\"setup\"] == \"forecasting\":\n",
    "    training_data = dataset.train\n",
    "elif config[\"setup\"] == \"missing_values\":\n",
    "    missing_values_splitter = OffsetSplitter(offset=-total_length)\n",
    "    training_data, _ = missing_values_splitter.split(dataset.train)\n",
    "\n",
    "num_rolling_evals = int(len(dataset.test) / len(dataset.train))\n",
    "\n",
    "transformation = create_transforms(\n",
    "    num_feat_dynamic_real=0,\n",
    "    num_feat_static_cat=0,\n",
    "    num_feat_static_real=0,\n",
    "    time_features=model.time_features,\n",
    "    prediction_length=config[\"prediction_length\"],\n",
    ")\n",
    "\n",
    "training_splitter = create_splitter(\n",
    "    past_length=config[\"context_length\"] + max(model.lags_seq),\n",
    "    future_length=config[\"prediction_length\"],\n",
    "    mode=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0b804-82f4-474e-9fa5-53a299af7ee3",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd0ffe-6fe8-41ab-94cc-9c6b1b521420",
   "metadata": {},
   "source": [
    "This section sets up the training process for the TSDiff model. Various callbacks are configured to monitor and save the model during training. The trainer is then defined using PyTorch Lightning, specifying parameters such as the number of epochs, devices, and callbacks. The training process is started, logging the progress and completing the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08eb6eb2-917a-42a1-a0e5-6196a894d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "callbacks = []\n",
    "if config[\"use_validation_set\"]:\n",
    "    transformed_data = transformation.apply(training_data, is_train=True)\n",
    "    train_val_splitter = OffsetSplitter(\n",
    "        offset=-config[\"prediction_length\"] * num_rolling_evals\n",
    "    )\n",
    "    _, val_gen = train_val_splitter.split(training_data)\n",
    "    val_data = val_gen.generate_instances(\n",
    "        config[\"prediction_length\"], num_rolling_evals\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        EvaluateCallback(\n",
    "            context_length=config[\"context_length\"],\n",
    "            prediction_length=config[\"prediction_length\"],\n",
    "            sampler=config[\"sampler\"],\n",
    "            sampler_kwargs=config[\"sampler_params\"],\n",
    "            num_samples=config[\"num_samples\"],\n",
    "            model=model,\n",
    "            transformation=transformation,\n",
    "            test_dataset=dataset.test,\n",
    "            val_dataset=val_data,\n",
    "            eval_every=config[\"eval_every\"],\n",
    "        )\n",
    "    ]\n",
    "else:\n",
    "    transformed_data = transformation.apply(training_data, is_train=True)\n",
    "\n",
    "log_monitor = \"train_loss\"\n",
    "filename = dataset_name + \"-{epoch:03d}-{train_loss:.3f}\"\n",
    "\n",
    "data_loader = TrainDataLoader(\n",
    "    Cached(transformed_data),\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    stack_fn=batchify,\n",
    "    transform=training_splitter,\n",
    "    num_batches_per_epoch=config[\"num_batches_per_epoch\"],\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=3,\n",
    "    monitor=f\"{log_monitor}\",\n",
    "    mode=\"min\",\n",
    "    filename=filename,\n",
    "    save_last=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "callbacks.append(checkpoint_callback)\n",
    "callbacks.append(RichProgressBar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d980d3a3-29c7-41a5-ad6c-a2728dcf47c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/vkhazaie/anaconda3/envs/tsdiff/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/vkhazaie/anaconda3/envs/tsdiff/lib/python3.8/site ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/h/vkhazaie/anaconda3/envs/tsdiff/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else None,\n",
    "    devices=[int(config[\"device\"].split(\":\")[-1])],\n",
    "    max_epochs=config[\"max_epochs\"],\n",
    "    enable_progress_bar=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=callbacks,\n",
    "    default_root_dir=log_dir,\n",
    "    gradient_clip_val=config.get(\"gradient_clip_val\", None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7c175-60e6-40bf-b034-82d7319a998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 09:40:01,848 - logger - INFO - Logging to ./lightning_logs/version_7\n",
      "/h/vkhazaie/anaconda3/envs/tsdiff/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type            </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ scaler   │ MeanScaler      │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ embedder │ FeatureEmbedder │      1 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ backbone │ BackboneModel   │  193 K │\n",
       "└───┴──────────┴─────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType           \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ scaler   │ MeanScaler      │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ embedder │ FeatureEmbedder │      1 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ backbone │ BackboneModel   │  193 K │\n",
       "└───┴──────────┴─────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 193 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 193 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 193 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 193 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236537da8c974862a9b179e77299e934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(f\"Logging to {trainer.logger.log_dir}\")\n",
    "trainer.fit(model, train_dataloaders=data_loader)\n",
    "logger.info(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b1cb0-55a5-43e4-82fa-60253c27ba5f",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba9522-b8a4-41c1-a43e-7d9095119954",
   "metadata": {},
   "source": [
    "After the training is completed, the model is evaluated on the test dataset. A function evaluate_guidance is defined to assess the model's performance using different sampling techniques. This involves generating forecasts, applying transformations, and calculating metrics to evaluate the accuracy of the predictions. The best model checkpoint is loaded, and the evaluation results are saved for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad201d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate guidance\n",
    "def evaluate_guidance(\n",
    "    config, model, test_dataset, transformation, num_samples=100\n",
    "):\n",
    "    logger.info(f\"Evaluating with {num_samples} samples.\")\n",
    "    results = []\n",
    "    if config[\"setup\"] == \"forecasting\":\n",
    "        missing_data_kwargs_list = [\n",
    "            {\n",
    "                \"missing_scenario\": \"none\",\n",
    "                \"missing_values\": 0,\n",
    "            }\n",
    "        ]\n",
    "        config[\"missing_data_configs\"] = missing_data_kwargs_list\n",
    "    elif config[\"setup\"] == \"missing_values\":\n",
    "        missing_data_kwargs_list = config[\"missing_data_configs\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown setup {config['setup']}\")\n",
    "\n",
    "    Guidance = guidance_map[config[\"sampler\"]]\n",
    "    sampler_kwargs = config[\"sampler_params\"]\n",
    "    for missing_data_kwargs in missing_data_kwargs_list:\n",
    "        logger.info(\n",
    "            f\"Evaluating scenario '{missing_data_kwargs['missing_scenario']}' \"\n",
    "            f\"with {missing_data_kwargs['missing_values']:.1f} missing_values.\"\n",
    "        )\n",
    "        sampler = Guidance(\n",
    "            model=model,\n",
    "            prediction_length=config[\"prediction_length\"],\n",
    "            num_samples=num_samples,\n",
    "            **missing_data_kwargs,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "\n",
    "        transformed_testdata = transformation.apply(\n",
    "            test_dataset, is_train=False\n",
    "        )\n",
    "        test_splitter = create_splitter(\n",
    "            past_length=config[\"context_length\"] + max(model.lags_seq),\n",
    "            future_length=config[\"prediction_length\"],\n",
    "            mode=\"test\",\n",
    "        )\n",
    "\n",
    "        masking_transform = MaskInput(\n",
    "            FieldName.TARGET,\n",
    "            FieldName.OBSERVED_VALUES,\n",
    "            config[\"context_length\"],\n",
    "            missing_data_kwargs[\"missing_scenario\"],\n",
    "            missing_data_kwargs[\"missing_values\"],\n",
    "        )\n",
    "        test_transform = test_splitter + masking_transform\n",
    "\n",
    "        predictor = sampler.get_predictor(\n",
    "            test_transform,\n",
    "            batch_size=1280 // num_samples,\n",
    "            device=config[\"device\"],\n",
    "        )\n",
    "        forecast_it, ts_it = make_evaluation_predictions(\n",
    "            dataset=transformed_testdata,\n",
    "            predictor=predictor,\n",
    "            num_samples=num_samples,\n",
    "        )\n",
    "        forecasts = list(tqdm(forecast_it, total=len(transformed_testdata)))\n",
    "        tss = list(ts_it)\n",
    "        evaluator = Evaluator()\n",
    "        metrics, _ = evaluator(tss, forecasts)\n",
    "        metrics = filter_metrics(metrics)\n",
    "        results.append(dict(**missing_data_kwargs, **metrics))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and save results\n",
    "best_ckpt_path = Path(trainer.logger.log_dir) / \"best_checkpoint.ckpt\"\n",
    "\n",
    "if not best_ckpt_path.exists():\n",
    "    torch.save(\n",
    "        torch.load(checkpoint_callback.best_model_path)[\"state_dict\"],\n",
    "        best_ckpt_path,\n",
    "    )\n",
    "logger.info(f\"Loading {best_ckpt_path}.\")\n",
    "best_state_dict = torch.load(best_ckpt_path)\n",
    "model.load_state_dict(best_state_dict, strict=True)\n",
    "\n",
    "metrics = (\n",
    "    evaluate_guidance(config, model, dataset.test, transformation)\n",
    "    if config.get(\"do_final_eval\", True)\n",
    "    else \"Final eval not performed\"\n",
    ")\n",
    "\n",
    "with open(Path(trainer.logger.log_dir) / \"results.yaml\", \"w\") as fp:\n",
    "    yaml.dump(\n",
    "        {\n",
    "            \"config\": config,\n",
    "            \"version\": trainer.logger.version,\n",
    "            \"metrics\": metrics,\n",
    "        },\n",
    "        fp,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b448f-0292-482a-b58f-8f9fa4ed375b",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77be91-f336-4dde-8e86-98733cc0ec6b",
   "metadata": {},
   "source": [
    "**Kollovieh, Marcel, et al.** \"Predict, refine, synthesize: Self-guiding diffusion models for probabilistic time series forecasting.\" *Advances in Neural Information Processing Systems* 36 (2024).\n",
    "\n",
    "**GitHub Repository:** [Amazon Science - Unconditional Time Series Diffusion](https://github.com/amazon-science/unconditional-time-series-diffusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsdiff",
   "language": "python",
   "name": "tsdiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
